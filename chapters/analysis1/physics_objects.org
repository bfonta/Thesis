:PROPERTIES:
:CUSTOM_ID: sec:physics_objects
:END:

* Electrons
* Muons
* Taus
:PROPERTIES:
:CUSTOM_ID: sec:hadronic_taus
:END:

The tau lepton, despite theoretically similar to the two other leptons, has fundamentally different properties.
For one, and contrary to the electron, which is stable, and to the muon, which can travel large distances before decaying, the very short $(290.3\pm0.5)\times10^{-15}\,\si{\second}$ tau lifetime [[cite:&PDG]] implies that it cannot currently be observed directly, as a $\tau$ lepton will travel on average just a few millimiters before decaying.
Secondly, its mass makes it the only hadronically-decaying lepton.
In fact, it does so approximately 2/3 of the time, while the remaining decays proceed leptonically, to electrons or muons.
The decay modes of the $\tau$ are listed in [[tab:tau_decays]], and can be categorized in terms of the number of charged particles, or /prongs/, the final state includes.
Due to charge conservation, \taus{} can only decay to an odd number of prongs, and usually decays with five or more prongs are neglected due to their extremely low \acp{BR}.
The decay itself can happen via a meson resonance, or directly to the hadrons, which consist of pions on \SI{\sim 98}{\percent} of occurrences, with the rest being kaons [[cite:&PDG]].

#+NAME: tab:tau_decays
#+CAPTION: $\tau$ lepton branching fractions, where $\text{h}^{\pm/\mp}$ symbolizes any charged hadron [[cite:&PDG]].
#+ATTR_LATEX: :placement [!h] :center t :align l|c|c
| Decay mode                                         | Meson resonance             | $\mathcal{B}$ [%] |
|----------------------------------------------------+-----------------------------+-------------------|
| $e\nu_{e}\nu_{\tau}$                                      |                             |              17.8 |
| $\mu\nu_{\mu}\nu_{\tau}$                                      |                             |              17.4 |
| *all leptonic decays*                                |                             |              35.2 |
|----------------------------------------------------+-----------------------------+-------------------|
| $\text{h}^{\pm}\nu_{\tau}$                                |                             |              11.5 |
| $\text{h}^{\pm}\pi^{0}\nu_{\tau}$                           | $\rho(770)$                    |              26.0 |
| $\text{h}^{\pm}\pi^{0}\pi^{0}\nu_{\tau}$                      | $\text{a}_{\text{1}}(1260)$ |               9.5 |
| $\text{h}^{\pm}\text{h}^{\mp}\text{h}^{\pm}\nu_{\tau}$      | $\text{a}_{\text{1}}(1260)$ |               9.8 |
| $\text{h}^{\pm}\text{h}^{\mp}\text{h}^{\pm}\pi^{0}\nu_{\tau}$ |                             |               4.8 |
| other hadronic decays                              |                             |               3.2 |
| *all hadronic decays*                                |                             |              64.8 |

In general, the hadronic decays can be differentiated from quark or gluon jets by the isolation of the decay products, their collimation and their multiplicity [[cite:&particle_flow]].
Tau leptons can also be mimicked as electrons and muons by decays with one charged track, and by decays with one prong plus a few photons, which can interpreted as an electron plus its bremstrahlung radiation.
The individual particles reconstructed by \ac{PF} are fed into the \ac{CMS}-specific \ac{HPS} algorithm [[cite:&HPS1;&HPS2;&HPS3]], which is responsible for hadronic $\tau$ reconstruction.
The task is rather complex, since a plethora of decay particles exists.
Neutral hadrons, on one hand, decay to photons ($\pi^{0}\rightarrow\gamma\gamma$) which quickly convert to electron-positron pairs in the material of the tracker.
Charged hadrons, instead, amount to kaons and pions, and can be produced with different multiplicities.
The algorithm starts by reconstructing neutral hadrons, which consists of collecting energy deposits in /strips/, \ie{} dynamically-sized regions along (\ac{eta}, \ac{phi}) which are created by the magnetic bending of electrons and positrons.
The momenta of the strips amounts to the vectorial sum of all its components.
Next, charged hadrons satisfying $\pt>0.5\,\si{\GeV}$ and coming from the \ac{PV} are reconstructed.
They can fit into multiple topologies, as shown in [[tab:tau_decays]], and when appropriate, the combination of charged hadrons with strips is required to be compatible with the masses of the $\rho$ or $a_{1}$ resonances.
Candidates with particles outside the so-called /signal cone/ are also rejected, where the cone is defined as $3\,\si{\GeV}/\pt(\tau_{\text{h}})$, capturing the dependence with the system's boost.
The cone size is bounded between 0.1 at low \ac{pt} and 0.05 at high \ac{pt}.
An /isolation cone/ with a process-dependent radius of 0.3 or 0.5 is also defined, in order to reduce the misidentification probability of \tauhs{} as jets.
A series of \ac{BDT} classifiers is also exploited for further discrimination.
Finally, in case multiple \tauh{} candidates satisfy the requirements, only the one with the highest \ac{pt} is kept, such that only one candidate is defined per jet.

The perfomances of \ac{HPS} during \run{2} are very dependent on the process and on the kinematics, and are presented in detail in Ref. [[cite:&HPS1]].
One is generally concerned with the misidentification probabilities of jets, electrons and muons, and with the identification efficiencies of hadronically-decaying \taus{}.
In a nutshell, the misidentification probabilities for leptons are extremely small, often at subpercent level given appropriate \ac{WP} choices.
Jet misidentification tends to be a bit higher, but still usually below 1%, and never above 2%.
Concerning efficiencies, they are \ac{WP}-dependent too, lying always above 90% for \ac{DY} events and above 86% for $\ttbar{}$ events.
Finally, the \ac{HPS} algorithm can recover events with two prongs, where one extra track was "lost".
This happens for 19% of the 3-prong decays and for 13% of the 3-prong+$\pi^{0}$ decays.
We finalize by noting that 2-prong recovered events are not considered in the analysis, since for most analyses the background increase brought by the extra events outweighs the increase in efficiency.
We thus follow the general \ac{CMS} recommendations, and consider only 1-prong and 3-prong events, with or without an additional neutral pion.

In the context of the \xhhbbtt{} analysis, hadronic $\tau$ decays are the most important $\tau$ decays, given their large \ac{BR}.
This is also the case for many other analyses.
There is thus a strong interest in improving the $\tau$ performance reconstruction as much as possible.
The reconstruction of hadronically-decaying \taus{} is thus complemented by dedicated \ac{DNN} algorithms.
In \run{2}, =DeepTau= [[cite:&deeptau]] has demonstrated good performances, and is used in our analysis.
Its goal is to perform the identification of \taus{}, disentangling those objects from electrons, muons, or quark- and gluon-initiated jets.
It uses information from all \ac{CMS} subdetectors, including variables used by \ac{HPS} or previous $\tau$ identification algorithms to reconstruct hadronic \taus{}.
It also considers information on candidates reconstructed within the \ac{HPS} tau signal and isolation cones, such as track and cluster properties and kinematics.
A multi-layered \ac{CNN}-based architecture is employed, with a loss function which gives precedence to the classification performance of \tauhs{}.
In total, the algorithm is trained with 140 million of \tauh{} candidates, and validation with 10 million.
The final discriminators $D$ against electrons, muons are jets are based on a softmax activation function, and are given by:

#+NAME: eq:deeptau
\begin{equation}
y_{\alpha} = \exp(x_{\alpha}) / \sum_{\beta}\exp{x_{\beta}} \:\:\: \, \:\:\: D_{\alpha} = \frac{y_{\tau}}{y_{\tau} + y_{\alpha}}
\end{equation}

\noindent with $\alpha \in {\text{jet}, \mu, e}$.
The discrimnators are also referred as =DeepTauVSjet= / =DeepTauVSe= / =DeepTauVSmu= for $D_{\text{jet}}$ / $D_{mu}$ / $D_{e}$, respectively.
The expected \tauh{} identification efficiencies are obtained with the validation samples, and are defined as the efficiencies for genuine \tauhs{} in a \htt{} sample that are reconstructed as \tauhs{} with $30 < \pt < 170 \,\si{\GeV}$ to pass a particular discriminator \ac{WP}.
The efficiencies range from 40 to 98% for jets, from 60 to 99.5% for electrons, and from 99.5 to 99.95% for muons, depending on the chosen \ac{WP}.
The $\text{jet} \rightarrow \tau$ misidentification rate depends on the \ac{pt} and quark flavor of the jet, and in simulated W production events with jets it has been estimated to be 0.43% for a genuine $\tau$ identification efficiency of 70%.
The same rate for electrons and muons is 2.60 (0.03)% for a genuine \tauh{} identification efficiency of 80 ($>99\%$).
Significant updates are being put in place, mostly for \run{3} analyses, including using more recent and extended data for training, improved training techniques, and optimised hyperparameter tuning [[cite:&deeptau_run3]].

* Jets
:PROPERTIES:
:CUSTOM_ID: sec:jets
:END:

The \ac{CMS} \ac{PF} algorithm creates a list of particle candidates which account for all inner-tracker and muon tracks, and for all energy deposits in the calorimeters above a certain threshold. 
These particles are assembled into jets using the anti-$\ktalgo$ clustering algorithm, described in [[#sec:offline_jet_object]], using distance parameters of 0.4 for AK4 jets or 0.8 for AK8 jets.
AK4 jets are required to satisfy $\pt > 20\,\si{\\GeV}$ and to not overlap with the two leptons from the \htt{} decay ($\Delta R(\text{jet},\tau) < 0.5$).
Since tracking information is only available in the central region of the CMS detector and the b-tagging process heavily relies on tracking information,
all b-jet candidates are required to have $|\eta| < 2.5$ for the 2017 and 2018 datasets ($|\eta| < 2.4$ for 2016).
The difference in \ac{eta} coverage between different years stems from the new \ac{CMS} pixel detector installed during the \phase{1} upgrade [[cite:&pixel_detector_eta_coverage]].
A more detailed description of jets coming from b quarks and identified as b-jets follows in [[#sec:bjets_id]].
The recommended set of jet energy corrections are applied to both AK4 and AK8 jets in Data and MC as described in [[#sec:jets_corrections]].

Some jets must ocasionally be vetoed due to their low reconstruction quality, or because they originate from electronic noise.
A \ac{PF} jet identification criterion is available to \ac{CMS} users, and all AK4 jets in our analysis are required to pass its =Tight= \ac{WP}.
The criterion is based on many jet observables, including the multiplicity of charged hadrons, the energy fraction deposited in \ac{ECAL} by hadrons, and the fraction of hadrons clustered within the jet.
The efficiency is around 98/99% or more for all \ac{eta} values, with a background rejection above 98% at $|\eta|<2.7$.

It can also happen for jets to have a \ac{PU} origin, and be unrelated to the \ac{PV}.
These jets often result from the overlap of many low-energy jets, being thus broader than \ac{PV} jets.
To avoid all such background jets, AK4 jets satisfying $\pt < 50\,\si{\GeV}$ are required to pass the =Loose= \ac{WP} of the discriminant.
The discriminant uses a \ac{BDT} to find an optimized decision boundary using information related to jet shape, object multiplicity and compatibility with the \ac{PV}.

Jets from b-quarks originating from the decay of high \ac{pt} Higgs bosons are often so close to be merged into a single large radius jet by the anti-$\ktalgo$ algorithm, forming an AK8 jet.
In our analysis, the \ac{GNN}-based \ac{PNet} algorithm [[cite:&particle_net]] is used to discriminate \hbb{} decays from the multijet background, as detailed in [[#sec:bjets_id]].
We require AK8 jets to satisfy $\pt > 250\,\si{\GeV}$, to not overlap with the two analysis leptons ($\Delta R(\text{jet},\tau) < 0.8$).
They must also have a =SoftDrop= mass above \SI{30}{\GeV}, where =SoftDrop= [[cite:&softdrop]] is a boosted jet grooming algorithm which removes soft and wide-angle radiation, aiming at mitigating the effects from contamination of \ac{ISR}, \ac{UE} and \ac{PU}.

** Identification of b-jets
:PROPERTIES:
:CUSTOM_ID: sec:bjets_id
:END:

During \run{1} in \ac{CMS}, the existing algorithms reconstructed b-jets by manually building discriminative variables.
The most advanced, the Combined Secondary Vertex (CSV) algorithm, used the secondary vertex mass and the number of tracks in a jet, among many others.
Deep learning techniques first appeared in \run{2}, starting with =deepCSV= [[cite:&deep_csv]], and later with =deepJet= [[cite:&deepjet;&deepjet_performance]], which is based on \acp{CNN} and \acp{RNN}, and is used in this work's analysis for the resolved categories.
Further improvements, particularly the widespread adoption of \acp{GNN}, have lead to \ac{PNet} [[cite:&particle_net]], used for the boosted category, and finally to \ac{ParT} [[cite:&transformer]], which additionally exploits the state-of-the-art transformer technology [[cite:&transformers]].

In our analysis, AK4 jets originating from b quarks are identified using the =DeepJet= algorithm [[cite:&deepjet]].
In order to separate b-jets from other jets, =DeepJet= combines secondary vertex properties, track-based variables and \ac{PF} jet constituents (neutral and charged candidates) in a \ac{DNN}.
It then classifies jets into six different categories.
Three of those are summed together to define a single discriminator used to tag b-jets in physics analyses:
+ at least two b hadrons;
+ exactly one b hadron decayin hadronically;
+ exactly one b hadron decayin leptonically.
The thresholds on the DeepFlavour discriminator value corresponding to =Loose=, =Medium= and =Tight= \acp{WP} are shown in [[tab:bTagWPs]].

#+NAME: tab:bTagWPs
#+CAPTION: DeepFlavour scores defining the \ac{UL} b-tagging \acp{WP}.
\begin{table}[htbp]
    \centering
    \setlength{\tabcolsep}{10pt}
    \begin{tabular}{lll}
	\hline \\[-1em]
	Year & Final state & $r$ factor \\ \hline \\[-1em]
	\multirow{3}{*}{2016}    & \texttt{Loose}  & 0.0408 \\
			         & \texttt{Medium} & 0.2489 \\
			         & \texttt{Tight}  & 0.8819 \\[+0.3em] \hline \\[-1em]

	\multirow{3}{*}{2016APV} & \texttt{Loose}  & 0.0508 \\
			         & \texttt{Medium} & 0.2598 \\
			         & \texttt{Tight}  & 0.8819 \\[+0.3em] \hline \\[-1em]

	\multirow{3}{*}{2017}    & \texttt{Loose}  & 0.0532 \\
			         & \texttt{Medium} & 0.3040 \\
			         & \texttt{Tight}  & 0.7476 \\[+0.3em] \hline \\[-1em]

	\multirow{3}{*}{2018}    & \texttt{Loose}  & 0.0490 \\
			         & \texttt{Medium} & 0.2783 \\
			         & \texttt{Tight}  & 0.7100 \\[+0.3em] \hline \\[-1em]
    \end{tabular}
\end{table}

As mentioned before, AK8 jets originating from decays merged \hbb{} decays are tagged using the \ac{PNet} algorithm.
This \ac{GNN}-based algorithm is able to identify hadronic decays of highly Lorentz-boosted top quarks and W, Z, and Higgs bosons, and classify different decay modes, such as $\bbbar$, $\ccbar$ or $\qqbar$ pairs.
We use its mass-decorrelated version.
The tagger is trained with \xbb{}, \xcc{} and \xqq{} signal jets, and with \ac{QCD} multijet background samples, where X is a \spin{0} scalar, and accordingly outputs four scores, each representing the probability for one of the four processes: $P(X\rightarrow \text{b}\bar{\text{b}})$, $P(X\rightarrow \text{c}\bar{\text{c}})$, $P(X\rightarrow \text{q}\bar{\text{q}})$ and $P(\text{QCD})$.
The algorithm achieves mass decorrelation by reweighting the training samples into uniform jet \ac{pt} and jet =SoftDrop= mass distributions.
The \xbb{} discriminant is defined as:

#+NAME: tab:pnet
\begin{equation}
  \frac{P(X\rightarrow b\bar{b})}{P(X\rightarrow b\bar{b}) + P(QCD)}.
\end{equation} 

\noindent Three \acp{WP} are defined with \hbb{} signal jets at efficiencies of 40%, 60%, and 80%: \ac{LP}, \ac{MP}, and \ac{HP}, respectively.
The final exclusion limits are run once per \ac{WP}, and the LP \ac{WP} is found to lead to the most stringent results.
It corresponds to a discriminator value of 0.9088 for 2016APV, 0.9137 for 2016, 0.9105 for 2017, and 0.9172 for 2018.
Finally, data/\ac{MC} discrepancies require the application of dedicated \acp{SF} to all jets passing the \ac{PNet} \acp{WP}.
AK8 analysis jets must thus be corrected, in a procedure described in [[#sec:pnet_sfs]].

** Jet scale and resolution corrections
:PROPERTIES:
:CUSTOM_ID: sec:jets_corrections
:END:

The measured jet energy can significantly differ from the underlying true hadron energy it represents.
Differences can arise due to detector noise, \ac{PU} or a non-linear calorimetric response.
The precise understanding of jet energy scales and momentum resolutions is of crucial importance for multiple analyses, also entering as an important component of their systematic uncertainties.
The energy of jets must therefore be corrected with appropriate corrections, in order to match the true particle-level deposited energy [[cite:&jet_corr1;&jet_corr2]].
In [[fig:jerc]] we show an illustration for the approach adopted by \ac{CMS} in \run{2}.
It consists on a sequential series of steps, where each step is responsible to independently correct a different effect.
Each data-taking period has its own set of corrections.
The first step addresses the spurious energy deposits from \ac{PU} interactions.
The correction chain begins with a PU correction, which accounts for the spurious energy contribution from PU interactions.
For each type of \ac{PF} candidate an offset energy is subtracted from the jet energy.
Next, detector response corrections are applied, in order to fix its non-uniformity across the jet \ac{pt} and \ac{eta}.
In the following step, still remaining data/MC differences are further corrected; they originate from \ac{PU} effects that also depend on the jet \ac{pt} and \ac{eta}.
Finally, optional flavour dependet corrections can be applied.
For all jet types, the energy scale uncertainties are smaller than 3% for $\pt > 50\,\si{\GeV}$ in the $|η| < 3.0$ region, increasing to 5% for $3.0 < |η| < 5.0$.

#+NAME: fig:jerc
#+CAPTION:  Flow of jet energy corrections to sequentially apply to obtain a calibrated jet, as done for \run{2} within \ac{CMS}. Taken from [[cite:&jet_corr2]].
#+BEGIN_figure
#+ATTR_LATEX: :width 1.\textwidth :center
[[~/org/PhD/Thesis/figures/analysis1/Run2-JERC.pdf]]
#+END_figure

Since measurements show that the jet energy resolution in data is worse than in the simulation, resolution corrections must be applied to \ac{MC} jets.
The latter are smeared to describe the data.
The smearing procedure applied using a ``hybrid'' approach recommended within \ac{CMS}, and composed of two methods.
If a matched generator-level jet exists, then the four-momentum of the corresponding reconstructed jet is rescaled with the following factor, dependent on jet the \ac{pt}:

#+NAME: fig:hybrid1
\begin{equation}
	c_{\text{JER}} = 1+(s_{\text{JER}}-1)\,\frac{\pt-\pt^{\text{Gen.}}}{\pt}
\end{equation}

\noindent where $s_{\text{JER}}$ is the data-to-simulation core resolution scale factor.
If the jet was not matched (and $\pt^{\text{Gen.}}$ is not available), then a stochastic smearing is applied, performing the four-momentum rescaling using a different factor:

#+NAME: fig:hybrid2
\begin{equation}
	c_{\text{JER}} = 1+\mathcal{N}(0, \sigma_{\text{JER}})\sqrt{\max(s^2_{\text{JER}}-1, 0)}
\end{equation}

\noindent where $\sigma_{\text{JER}}$ is the relative \ac{pt} resolution in simulation, and $\mathcal{N}(0, \sigma)$ denotes a random number sampled from a normal distribution with zero mean and standard deviation $\sigma$.
The resolution corrections are computed after applying the above jet energy corrections.
The data/MC \acp{SF} usually vary between 1 and 1.2, but are larger in the transition region between the endcaps and the forward detectors.
No significant dependences on the \ac{pt} and \ac{eta} of the jets are observed, except for the transition region [[cite:&jec_jer_performance]].


* Missing transverse energy
+ The transverse momentum actually carried by invisible particles is systematically different from the true, corrected MET, due to the non-compensating nature of the calorimeters and to detector misalignment.
