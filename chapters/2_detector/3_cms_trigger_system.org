:PROPERTIES:
:CUSTOM_ID: sec:cms_trigger_system
:END:

With proton beams crossing every \SI{25}{\nano\second}, and considering the average \ac{PU} measured during \phase{1} ([[fig:lhc_lumi_results1]], right), around \num{e9} \ac{pp} interactions are expected every second at the \ac{CMS} \ac{IP}.
Given \SI{\sim 1}{\mega\bit} zero-suppressed events, a \SI{\sim 40}{\tera\bit\per\second} throughput follows.
Meanwhile, the maximum rate which can be realistically archived in real time by the \ac{CMS} online computer farm sits at \SI{\sim 5}{\kilo\hertz}, which already represents a 50-fold improvement with respect to \run{1}.
Collision and data rates are thus much higher than the rate at which data can be written to mass storage.
This implies a throughput reduction by a factor of \num{e4}.

#+NAME: fig:cms_xsec_summary
#+CAPTION: Summary of selected \ac{CMS} cross section measurements of high-energy processes. Measurements performed at different \ac{LHC} \ac{pp} collision energies are marked by unique symbols and the coloured bands indicate the combined statistical and systematic uncertainty of the measurement. Grey bands indicate the uncertainty of the corresponding \ac{SM} theory predictions. Shaded hashed bars indicate the excluded cross section region for a production process the measured 95% \ac{CL} upper limit on the process indicated by the solid line of the same colour. Values span around 14 orders of magnitude. Taken from [[cite:&trigger_stairway]].
#+BEGIN_figure
\centering
#+ATTR_LATEX: :width 1.\textwidth :center
[[~/org/PhD/Thesis/figures/detector/CMSCrossSectionsSummary2.pdf]]
#+END_figure

However, among all events produced in \ac{CMS}, only a small fraction carries physical significance.
Most events are related to well known physical interactions which are not meant to be covered by the \ac{LHC} programme.
As perfectly illustrated in [[fig:cms_xsec_summary]], the processes studied by \ac{CMS} span at least \num{14} orders of magnitude.
The dominant inclusive proton-proton interactions tower at \SI{e14}{\femto\barn}, followed closely by their inelastic counterparts at \SI{\sim 6e13}{\femto\barn} [[cite:&inelastic_pp_13tev;&inelastic_pp_7tev]].
On the other end, extremely rare triboson, \ac{VBF}, \ac{VBS}, top and Higgs processes, including the HH process studied in [[ref:sec:analysis1_intro,sec:analysis2_intro]], sit at the edge of what is currently achievable, with cross sections of the order of some \si{\femto\barn}, or even less.
These huge differences showcase the richness of the \ac{LHC} programme.
They also show that a big fraction of the interesting processes have small cross sections.
The large rate reduction provided by the trigger is therefore justifiable on physics grounds, besides being absolutely crucial given current technological limitations.

The needed data reduction is the reason for existence of the \ac{CMS} trigger system: only events deemed interesting are selected for further processing.
The \ac{CMS} trigger adopts a two-tiered approach.
In the first level, or \ac{L1}, the trigger constrains the \ac{LHC} rate from \SI{40}{\mega\hertz} to \SI{100}{\kilo\hertz}, with a \SI{3.8}{\micro\second} latency.
In the second level, the \ac{HLT}, a further reduction down to \SI{1}{\kilo\hertz} with a \SI{\sim 300}{\milli\second} latency is performed.
Latency here refers to the time taken for the data to be processed in a particular trigger system.
It includes the time taken to read the data from the detectors, the algorithmic time, the \ac{L1A} signal distribution time, and further delays introduced by potential limitations at the level of buffer capacity [[cite:&trigger_tdr_phase1_vol1;&l1TDR;&trigger_tdr_phase1_vol2;&l1_performance;&hlt_performance;&zabi]].

* The Level-1 trigger
:PROPERTIES:
:CUSTOM_ID: sec:l1
:END:

#+NAME: fig:l1_trigger_design_phase1
#+CAPTION: Diagram of the \phase{1} \ac{CMS} L1 during \run{2}. No L1 tracking is present. TPs are generated from the DT, RPC and CSC muon systems and from the HF, ECAL and hadronic barrel and endcap calorimetric subdetectors (where the latter refers to the HCAL). The two separate paths are merged into the GT, which make a L1A decision on whether each particular event should be kept. TPs from GEM are currently being validated, but are not yet used. Adapted from [[cite:&l1_performance]]. 
#+BEGIN_figure
\centering
#+ATTR_LATEX: :width .7\textwidth :center
[[~/org/PhD/Thesis/figures/detector/L1TriggerDesignPhase1.pdf]]
#+END_figure

The first trigger level is implemented in custom electronics and \acp{FPGA}, and uses coarsely segmented data from calorimeter and muon detectors to ensure a low latency.
All high-resolution data are nevertheless held in pipeline memories in the \ac{FE} electronics, waiting for a \ac{L1A} signal to be issued.
The data is stored for \SI{3.8}{\micro\second}, equivalent to \num{152} \acp{BX}, including \SI{\sim 1}{\micro\second} dedicated to trigger algorithms.
Each event forwarded to the \ac{HLT} takes \SI{\sim 2}{\mega\byte} which implies, at a rate of \SI{100}{\kilo\hertz}, a bandwidth usage of \SI{200}{\giga\byte\per\second} supported by the \ac{DAQ} system.
The \ac{L1} trigger receives as input the raw data from the \ac{FE} readout electronics.
The first processing stages are handled literally on the detector, to allow for direct access and thus faster processing.
The remaining stages are taken care of by racks of \acp{FPGA} located in the experimental cavern.
The small time budget implies a compromise on the quality of the reconstruction, which is a recurrent theme in all trigger-related matters.
The muon and calorimeter trigger paths produce data called \acp{TP}, which are merged into the \ac{GT}, and used to issue a \ac{L1A} decision.
\Acp{TP} from the \ac{GEM} subdetector are currently being validated, but are not yet used.
The tracker does not participate in the \phase{1} \ac{L1} trigger decisions.
The \ac{L1} rate has been increased to \SI{\sim 110}{\kilo\hertz} during \run{3} to favor the introduction of the new parking techniques discussed in [[#sec:parking_scouting]].

** Calorimeter Trigger
The calorimeter trigger is split in two layers.
The first one receives, calibrates, and sorts the \acp{TP} provided by the \ac{HF}, \ac{ECAL} and \ac{HCAL} subdetectors.
The second layer reconstructs and calibrates physics objects such as electrons and photons, tau leptons, jets, and energy sums.
It should be noted that the calorimeter trigger cannot distinguish photons from electrons; they all correspond to energy deposits named /candidates/, which are formed based on information collected in groups of $5\times5$ crystals in the \ac{ECAL} and one projective readout unit in the \ac{HCAL}, jointly known as \acp{TT}.
\ac{L1} reconstruction algorithms work with a granularity dictated by the \acp{TT}, using discriminative quantities such as the ratio of \ac{HCAL} to \ac{ECAL} energy, or the shape of energy deposits as seen by data-aggregating sliding windows.
The calorimeter trigger takes advantage of a spatially-pipelined mechanism, the \ac{TMT}, which enables the treatment of all event data in a single processing board, in contrast with the parallelism that is usually required [[cite:&time_multiplex]].
It works by converting the parallel processing of spatially decoupled detector regions in a $\Delta\text{T}$ time period into the sequential processing of the full detector in $\text{N}\,\Delta\text{T}$ time.
N corresponds to the "depth" of the \ac{TMT}, which is equivalent to the number of available boards with enough memory to process one full event.
The benefit of using the \ac{TMT} is the removal of region separations in the event.
It was found that the trade-off between the delay from organizing the data in series and the increased throughput and reduced buffering time brought by the \ac{TMT} is largely favorable [[cite:&zabi]].
A demultiplexer board then reorders, re-serializes, and formats the events for the \ac{GT} processing.
The running algorithms are fully pipelined and start processing the data as soon as it is received.

** Muon Trigger
The redundancy of the muon system in terms of \acp{DT}, \acp{CSC} and \acp{RPC} is used to robustly define the \ac{GMT}.
The subdetectors identify track segments from the information produced by the hits in the gas chambers.
The segments are collected and transmitted via optical fibers to three muon track finders: the \ac{EMTF}, the \ac{BMTF} and the \ac{OMTF}.
These run pattern recognition algorithms to identify candidates as muons and extract their momenta.
The track finders play the role of regional triggers, and send the best muon candidates to the \ac{GMT}.
The \ac{GMT} receives the information and tries to correlate the muon candidates from the three regions in order to make a decision.
It also merges compatible muon candidates found by more than one single system, and can reject candidates based on their identification quality or the lack of confirmation from other muon systems [[cite:&l1_trigger_upgrade]].

** Global Trigger
The \ac{GT} collects all \ac{L1} muon and calorimeter candidates, and executes multiple selection and identification algorithms, in parallel, for the final trigger decision.
Kinematical quantities of the candidates are exploited, such as the invariant mass of \egamma{} or $\mu$ objects, or the angular distance between two objects.

* The High Level Trigger
:PROPERTIES:
:CUSTOM_ID: sec:hlt
:END:

The HLT is provided by a subset of the thousands of commercial \acp{CPU} and \acp{GPU} which constitute the online farm located at the \ip{5}, running the full reconstruction software framework of \ac{CMS}, \ac{CMSSW}.
The goal of the \ac{HLT} is to reduce the rate from \num{100} to \SI{\sim 1}{\kilo\hertz}, while keeping the most interesting events.
More than \num{400} trigger paths are available, targeting a broad range of physics signatures.
Given the tight event time budget of \SI{\sim 300}{\milli\second}, the reconstruction is performed starting from previously selected \ac{L1} seeds, which correspond to events that issued a \ac{L1A}.
The \ac{HLT} farm processes the data starting with \acp{BU}, which gather data from multiple subdetectors, and later with \acp{FU}, which decompress, reconstruct and filter the events.
The full granularity information is available, together with tracker information, which is absent from the \ac{L1}.
The selected events are acquired by the \ac{DAQ} and streamed to the Tier-0 at CERN, where they are prepared for offline reconstruction and organized into \acp{PD}.
The latter are defined based on collections of \ac{HLT} trigger paths, as for instance muon or \egamma{} triggers.
The reconstructed data is eventually sent for permanent tape storage in \tier{0} and \tier{1} sites, managed by the \ac{WLCG} [[cite:&wlcg1]].

In \run{3}, the addition of \ac{GPU} processing at the \ac{HLT} improved the performance of some triggers, which lead to an increase of data throughput from \num{1} to \SI{5}{\kilo\hertz}.
This was due to several advancements in \ac{CMSSW}, which can now leverage more parallelism while exploiting a new heterogeneous architecture.
\Ac{CMSSW} can now transparently switch between \ac{CPU} and \ac{GPU} implementations [[cite:&hlt_cpu_gpu_2023]].
The reconstruction of several subdetectors has been offloaded to \acp{GPU}, including the \ac{ECAL} [[cite:&hlt_ecal_gpu]], \ac{HCAL} [[cite:&hlt_hadron_pf]], and some parts of the reconstruction of pixels and vertices, including an improved track seeding.
As a consequence, the \ac{HLT} timing and throughput improved by 40% and 80%, respectively, and the computing power consumption was reduced by 30%.
The \ac{CMS} \ac{HLT} system is constantly subject to updates and optimizations, in order to improve algorithmic efficiency and increase allowed rates, while maintaining an excellent physics performance, despite the increasingly more challenging running conditions [[cite:&performance_calorimeter_trigger;&performance_muon_trigger]].

* Parking and Scouting
:PROPERTIES:
:CUSTOM_ID: sec:parking_scouting
:END:

The quest for ever higher event rates given current technology limitations and experimental benefits has lead \ac{CMS} to explore a non-standard use of triggers.
There are various constraints imposed on the trigger system and data processing framework which limit the number of events that can be selected, recorded and analyzed.
Some examples include /i/) the \SI{\sim 100}{\kilo\hertz} \ac{L1} acquisition rate, which is limited by the need to avoid losing recording time if the readout system is not ready for a new event, /ii/) the \ac{HLT} latency, which is constrained by the available number and speed of processing cores, or /iii/) the available permanent storage space, which is distributed across disks and tape, the former providing faster access but reduced storage size.
The absolute and relative costs of all hardware trigger components has a strong impact on the overall capacity and structure of the computing farm.
In the following, the parking and scouting strategies are presented [[cite:&parking_scouting1;&parking_scouting2;&parking_scouting_run3_cms]].
They are also summarized in [[fig:parking_scouting]].

#+NAME: fig:parking_scouting
#+CAPTION: A schematic view of the typical \run{2} data flow during 2018, showing the data acquisition strategy with scouting and parking data streams, together with the standard data stream. A value of $\mathcal{L}_{\text{inst}} = 1.2 \times 10^{34}\,\si{\cm\squared\per\second}$ over a typical 2018 fill, corresponding to an average pileup of \num{38}, is considered. The average collision rate lies below the \SI{40}{\mega\hertz} frequency due to occasional but required gaps between consecutive bunch trains. The parking and scouting data streams have been significantly extended during \run{3}. Taken from [[cite:&parking_scouting1]].
#+BEGIN_figure
#+ATTR_LATEX: :width 1.\textwidth :center
[[~/org/PhD/Thesis/figures/detector/parking_scouting.pdf]]
#+END_figure

** Parking
The essential idea of data parking is to record as much data as possible, and process it later, as soon as resources used for the \num{\sim 48} hours long prompt reconstruction become available, which often happens between data-taking periods.
One of the positive effects of this strategy is to lower the kinematical thresholds used by algorithms, and thus increase the acceptance to low-mass physics signals.
To sustain the high rates which necessarily arise from lower thresholds, the data is written directly to tape, usually during the latest stages of an \ac{LHC} fill, when the rate has substantially decreased due to lower instantaneous luminosities.

\ac{CMS} has exploited parking strategies since its inception to explore \ac{NP} in \ac{VBF}, Higgs, \bphys{} and \ac{SUSY} scenarios.
During \run{3}, an enhanced /b parking/ program has been envisaged, with the goal of collecting di-muon and di-electron final states from b hadron decays, in the wake of what had been done in \run{2}.
The physics goal is strongly centered around searches for \ac{LFV}.
Additionally, three new parking strategies have been introduced:

+ *VBF*: The VBF production mode of the Higgs boson drives the sensitivity of some of its decay modes, like \hmumu{} or \htt{}.
  It also brings an important constraining power to \ac{EFT} coupling measurements, and provides unique access to $\kvv$ in double Higgs processes.
  \Ac{VBF} triggers offer an alternative to raise thresholds in order to keep rates under control: the selection of the two forward jets.

+ *\acp{LLP}*: Particles with long lifetimes are central to many current \ac{BSM} scenarios, but imply large displacements relative to the \ac{PV}, which renders its final states often inadequate to be measured with standard trigger techniques.
  A group of dedicated \ac{HLT} paths has thus been introduced, targeting displaced jets and di-jets, taking into account their time delay with respect to prompt decays.

+ *HH*: As discussed in [[#sec:dihiggs]], the study of the Higgs boson self-coupling is one of the most relevant, if not the most important measurement to be pursued at the \ac{LHC} in the foreseeable future.
  \ac{CMS} has successfully deployed the \ac{PNet} b-tagging discriminant at \ac{HLT} level, which made possible to loosen jet $\pt$ thresholds, $\htvar$ selections and b-tagging requirements.
  These updates were combined with a reduction of the $\htvar$ requirement at \ac{L1}.
  As shown in [[fig:hh_parking_improv]], double and triple Higgs analyses benefit enormously from these developments, with much increased signal efficiencies.

#+NAME: fig:hh_parking_improv
#+CAPTION: Trigger efficiencies as a function of the invariant mass of Higgs boson pairs for simulated samples. $\kl=1$ is assumed in both plots. Very significant efficiency improvements are observed in the \hhhbbbbbb{} and \hhhbbbbtt{} analyses, driven by the inclusion of the HH parking stream. (Left) \hhbbbb{}, considering the full \run{2}, 2022 and 2023 data-taking periods. (Right) \hhbbtt{} in 2023 only. Taken from [[cite:&parking_hh_twiki]]; also available in [[cite:&parking_scouting_run3_cms]].
#+BEGIN_figure
#+ATTR_LATEX: :width .5\textwidth :center
[[~/org/PhD/Thesis/figures/detector/parking_bbbb_accept_gain.pdf]]
#+ATTR_LATEX: :width .5\textwidth :center
[[~/org/PhD/Thesis/figures/detector/parking_bbtt_accept_gain.pdf]]
#+END_figure

** Scouting
The offline reconstruction in \ac{CMS} notably increases the quality of the collected dataset.
Unfortunately but unsurprisingly, it also brings a significant growth in bandwidth and storage needs.
It is not possible to perform the offline reconstruction on all events processed and accepted by the \ac{HLT}.
The trigger scouting strategy proposes to save some events using \ac{HLT} reconstruction only, which provides a smaller event size at the cost of data resolution.
This effectively allows to keep events that would otherwise be lost, or to enhance the sensitivity to low-energy processes by lowering \ac{HLT} thresholds.
The strategy depends on the performance of the \ac{HLT} algorithm which must, as much as possible, approach the quality of the offline reconstruction.
In the future, there will always be the possibility to extend the current standard triggers to some specific scouting phase-space regions if some unexpected and promising trend is observed in the data.

The scouting stream was premiered in \run{1} to search for di-jet resonances with jets reconstructed only from calorimetric energy deposits.
This was considerably extended in \run{2}, with the addition of jet, muon and electron candidates to the scouting event record, closing the gap with respect to the standard \ac{CMS} data streams.
As an example, studies of \ac{BSM} low-mass di-muon resonances were able to reach a threshold close to twice the muon mass [[cite:&dimuon_low_trigger]].
The scouting stream in the on-going \run{3} has strongly benefited from the inclusion of \ac{GPU} processors and related software infrastructure at \ac{HLT} level [[cite:&patatrack]].
The scouting bandwidth is currently \SI{\sim 30}{\kilo\hertz}, ten times higher than the standard data stream.
This implies the exploration of lower kinematic thresholds, with the increase in physics sensitivity as a consequence.
There are plans to extend the scouting strategy to \ac{L1}, as will be discussed in [[#sec:phase2_trigger_system]].

