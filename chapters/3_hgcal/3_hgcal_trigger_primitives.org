:PROPERTIES:
:CUSTOM_ID: sec:hgcal_trigger_primitives
:END:

The importance of the \ac{L1} and High-level \ac{CMS} trigger systems was already highlighted in [[#sec:cms_trigger_system]].
The \Ac{HGCAL} will be integrated with the online firmware trigger system put in place by \ac{CMS}, the \ac{L1} [[cite:&l1TDR]], which precedes the \ac{HLT} running on commercial servers.
In this section we detail the hardware and software infrastructure powering the \ac{HGCAL} \ac{DAQ} and \ac{L1} \ac{TPG} systems.
The \ac{TPG} produces \acp{TP} which are physical quantities built to encapsulate the most discriminative information in an event.
\Acp{TP} are used as building blocks for \ac{L1} decisions.
In \ac{HGCAL}, \acp{TP} consist of (\ac{eta}, \ac{phi}) towers and cluster-related variables, such as energy, positions and shapes.
The \ac{TPG} includes all steps from data collection in the \ac{FE} chips at \SI{40}{\mega\hertz} to the production of \acp{TP} in the \ac{BE} electronics [[cite:&hgcalTDR]].
The splitting of resources between /on-detector/ and /off-detector/ electronics is related to several factors, including the flexibility provided by \acp{FPGA} or other programmable boards, the cost of boards and connectors, and the high radiation levels present close to the detector [[cite:&zabi]].

The \ac{TPG} must fit within a \SI{\sim 5}{\micro\second} latency, or \num{200} \acp{BX}, taken from the total \SI{12.5}{\micro\second} latency allocated to the \ac{L1} [[cite:&l1TDR]], and is constrained by the available bandwidth between the \ac{FE} and the \ac{BE}.
The bandwidth is limited by several factors, including the total power consumption, the multiplicity of routing lines, the space available for electronic boards in the \ac{USC}, and the overall cost of hardware, among which expensive \acp{FPGA} and optical links [[cite:&jb_hdr]].
A large number of throughput reduction techniques is thus applied as often and as soon as possible, exploiting pipelined algorithms whenever feasible.
The flow of \acp{TP} can be schematically visualized in [[fig:l1chain]].
\ac{HGCAL} is the second subdetector in \ac{CMS} with the largest fraction of allocated resources at \ac{L1}, due to its high granularity.

#+NAME: fig:l1chain
#+CAPTION: Simplified schematic of the \ac{TP} dataflow in HGCAL, starting (ending) in the top left  (bottom left) corner. The diagram follows the \ac{TP} processing in a Si layer through the \ac{FE} and \ac{BE}, and up to the \ac{L1}, including expected approximate bandwidths. Trigger decisions at \ac{L1} will impact the \ac{HLT} and, consequently, physics analysis. Adapted from [[cite:&bruno_chep23]].
#+BEGIN_figure
#+ATTR_LATEX: :width 1.\textwidth
[[~/org/PhD/Thesis/figures/hgcal/L1chain.pdf]]
#+END_figure

The most granular detector elements the \ac{TPG} is concerned with are \acp{TC}.
As detailed in [[#sec:frontend_electronics]], for the \ac{Si} section, \acp{TC} are arranged in a "three-fold diamond" configuration where groups of \num{4} or \num{9} \ac{Si} cells form one \ac{TC}.
This configuration was chosen because it allows the convenient definition of uniform groups of neighboring cells to form \acp{TP}, as shown in [[fig:tcs_geometry]].
It also simplifies the layout of the \ac{Si} module readout \ac{PCB}, given the subdivision of the module into symmetric domains [[cite:&hgcalTDR]].
In the \ac{Sci} section, \acp{TC} are defined as groups of $2\times2$ tiles, except close to boundaries, where some \acp{TC} containing less tiles exist [[cite:&sci_specific]].
The number of \acp{TC} and \acp{elink} associated to the \ac{Sci} section increases with the detector's depth, reflecting the overall distribution of \ac{Si} to \ac{Sci} regions with increasing layer number (recall [[fig:hgcal_side_view]]).

\Acp{TP} clearly are the basis for most of the physics analyses that can be done at \ac{CMS}, both now and in the future.
They stem from a complex interplay of advanced software and hardware approaches, with extremely tight bandwidth and latency budgets, forming the seed for all downstream decisions.
My Thesis work is deeply connected to the definition and processing of \ac{HGCAL} \acp{TP}, and will hopefully grant others the basis for important extensions, with the final goal of providing robust information to enhance the sensitivity to \ac{SM} and \ac{BSM} physics.
 
#+NAME: fig:tcs_geometry
#+CAPTION: Illustration of the three-fold diamond configuration of an hexagonal \SI{8}{\inch} module, used to associate single \ac{Si} cells to \acp{TC}. Low density modules (left) associate four sensors to each trigger cell, while high density modules (right) create \acp{TC} with nine channels each. All modules have exactly \num{48} \acp{TC}, effectively removing a layer of complexity when processing \acp{TC}. The actual physical dimensions of the \acp{TC} vary given the boundaries of the hexagonal modules. 
#+BEGIN_figure
#+ATTR_LATEX: :width 1.\textwidth :center
[[~/org/PhD/Thesis/figures/hgcal/TCs_geometry.pdf]]
#+END_figure

* The infrastructure of the backend trigger and data acquisition systems

The \ac{HGCAL} \ac{BE} electronics consist on the \ac{DAQ}, \ac{TPG}, \ac{DCS} and \ac{DSS}.
The first two form the so-called \ac{TDAQ}.
The \ac{BE} have been implemented on the \ac{ATCA} infrastructure [[cite:&atca_website]], also used for other \ac{CMS} subdetectors [[cite:&calo_barrel_upgrade]].
\Ac{ATCA} boards will be placed in \ac{ATCA} crates, which can house up to 14 boards.
The \ac{DAQ} system includes two parts, where the most prominent features the configuration, control and data acquisition system.
The second, smaller part holds the \ac{EMF} interface, which purpose is to propagate \ac{NZS} \ac{HGCAL} "regions of interest", as defined by extrapolated muon tracks in the muon detectors, aiming at reconstructing \ac{MIP} peaks for calibration without applying thresholds.
The larger part of the \ac{DAQ} consists on \num{6} identical hardware copies, following the three-fold polar angle symmetry of the two endcaps.
Each \SI{120}{\degree} sector hosts \num{16} =Serenity S= \ac{DAQ} boards [[cite:&serenity]] in two crates.
The interface to the smaller \ac{EMF} system has one =Serenity S= board per endcap, called "\ac{NZS} board".
Besides being responsible for data acquisition, the \ac{DAQ} system has other responsibilities.
Being the entry point to the \ac{FE} electronics, the DAQ is also equipped with synchronous fast control, asynchronous slow control, and transporting the timing information from \ac{CMS} to the \ac{FE}.
Concerning the \ac{HGCAL} \ac{TPG} system, its role is to build objects that are used as primitives by the central \ac{L1} trigger.
The \ac{TPG} system receives \ac{FE} data for every \ac{BX} at \SI{40}{\mega\hertz}, and creates trigger “primitives” which are passed to the CMS central Level-1 Trigger (L1T) system to use in making the L1A trigger decisions.
The \ac{DAQ} and \ac{TPG} systems receive data from the FE electronics through separate links, being independent systems as far as the \ac{BE} electronics are concerned.

The \ac{lpGBT} \ac{ASIC} [[cite:&lpgbt]] is the radiation-hard link driver standard to be used with \ac{HL-LHC} \ac{FE} electronics.
It provides a two-way connection between the \ac{BE} and \ac{FE}, including clock distribution, control and configuration signals, and transmission of the information collected by the detector.
The data is supplied to the \ac{lpGBT} inputs via up to \num{7} \acp{elink}, each providing \SI{32}{\bit} words at \SI{40}{\mega\hertz}, or \SI{1.28}{\giga\bit\per\second}.
This corresponds to \SI{8.96}{\giga\bit\per\second} data rates between the \ac{FE} and the \ac{BE}.
In \ac{HGCAL}, both the \ac{DAQ} and the \ac{TPG} communicate with the \ac{FE} via \acp{elink}, but only the \ac{DAQ} provides control and configuration.
The data is then forwarded to the \ac{BE} electronics, located at a distance of \SI{\sim 100}{\meter} from \ac{HGCAL}.
The \ac{VTRx+} [[cite:&vtrxp]] module provides an optical communication interface between the \acp{lpGBT} and the \ac{BE} computing boards.
Additionally, CERN has developed a custom radiation resistant version of the easy-to-install =Firefly= optical transceivers [[cite:&versatile_link;&firefly]], optimized for use in the \ac{BE} electronics to communicate with the \ac{VTRx+} modules.

The \ac{TCDS} interface is provided by one specialized \ac{ATCA} board per crate, named \ac{DTH400}.
The \ac{DTH400} also provides an interface to the \ac{cDAQ}, outputting \SI{\sim 400}{\giga\bit\per\second} via Ethernet \ac{D2S} links.
The data is then transported to the surface into the memory of commercial servers.
That \ac{DTH400} boards receives the \ac{CMS}-wide clock and control signals and distributes it to the remaining boards in the crate.
The communication with the \ac{TCDS} is two-way: this notably gives the ability to request a \ac{BE} resynchronization or reset, and to throttle the entire \ac{L1} system.
Communication between the \ac{BE} and the \ac{cDAQ} is established via \SI{25}{\giga\bit\per\second} fiber optics running the \ac{CMS}-wide =SLinkRocket= protocol, which ensures a strict specification based on \SI{128}{\bit} words [[cite:&hlttdr]].
The \ac{BE} electronics will be located in the \ac{CMS} \ac{USC}, where space is extremely limited and has to be distributed across all subdetectors.
The \ac{HGCAL} \ac{TDAQ} \ac{BE} has been assigned \num{8} racks, split into \num{3} and \num{5} to \ac{DAQ} and \ac{TPG}, respectively.
Each rack fits two \ac{ATCA} crates, and sufficient power is provided to the \ac{ATCA} boards.

* Frontend electronics
:PROPERTIES:
:CUSTOM_ID: sec:frontend_electronics
:END:

The \ac{HGCAL} \ac{L1} reconstruction chain, including the \ac{TPG} reconstruction, starts at the location where data is collected, namely the \ac{Si} cells and \ac{Sci} tiles described in [[#sec:hgcal_intro]].
From raw energy deposits to the creation of \acp{TP}, a complex chain of electronic components and data reduction and selection algorithms is in place.
The architecture surrounding the /on-detector/ steps, i.e., the steps taking place very close to where the raw data is collected, constitutes the \ac{FE} electronics.
The entry points of the reconstruction chain are the custom chips located on the hexaboards or tileboards, depending on the detector region.
They are called \acp{HGCROC} [[cite:&hgcroc;&hgcroc_paper]], and are \ac{HGCAL}-specific \acp{ASIC} which collect, amplify and filter the produced ionization or scintillation charged currents at \SI{\sim 300}{\tera\byte\per\second} [[cite:&hgcalTDR]].
The layout of a \ac{HGCROC} chip is shown in [[fig:hgcroc]].
In addition to a standard \SI{10}{\bit} \ac{ADC} charge measuring mode, the \ac{HGCROC} switches to a \ac{ToT} mode as soon as a threshold on the deposited charge is reached, of the order of the preamplifier saturation threshold of \SI{\sim 100}{\femto\coulomb}.
The time during which the preamplifier is saturated serves as a proxy for the amount of deposited charge.
During the saturation period, which can reach up to \SI{\sim 200}{\nano\second}, the channel is blind to new charge deposits.
Once the saturation is over, the time is digitized with a \SI{12}{\bit} \ac{TDC}.
Beyond the data paths, the chip includes a \ac{PLL}, which generates the clocks needed to operate the chip.
An \ac{I2C} interface is also present, enabling the modification of all static parameters of the chip, which are triplicated to prevent \acp{SEE}, which are stochastic, localized and non-cumulative effects disrupting the chip's functioning.

Due to the similarity of the algorithms and electronics of the \ac{Si} and \ac{Sci} detector regions, and also taking into account differences in their development stage, we focus on the \ac{Si} technology to simplify the overall description.
The \ac{HGCROC} trigger path aggregates the data into \acp{TC} by summing their energies, in what constitutes the first of many data reduction algorithms in the \ac{TPG}.
\Acp{TC} are defined as energy sums of neighboring sensor cells, and represent a simple method to reduce the prohibitive data throughput.
They group \num{4} or \num{9} channels, depending on the \ac{Si} module granularity.
In the \ac{CE-E}, only odd-numbered layers are considered for \ac{TC} reduction.
The summation of single channels requires all inputs to use the same energy scale, and this is not the case due to the two charge digitization modes being used, the \ac{ADC} and the \ac{ToT}.
The produced digitized values are thus rescaled, but the rescaling procedure needs to take into account the strong non-linearity of the \ac{ToT} response for medium charge values, close to the \ac{ADC} regime.
An approximate approach is employed to avoid an extremely demanding linearization procedure at \SI{40}{\mega\hertz}.
Besides the reduction in granularity, \acp{TC} also decrease the algorithms' complexity, in the sense that all modules have exactly \num{48} \acp{TC}, and thus \ac{HGCAL} \ac{L1} algorithms can ignore differences arising from low- and high-granularities.
After building \acp{TC}, the charge values to be sent to the \ac{BE} are compressed by a factor of \num{\sim 3} using a floating point encoding.
The compression exploits the fact that a high resolution is generally not required at \ac{L1} for particles lying well above the energy thresholds.
In parallel, the full-granularity data is kept in circular buffers and is sent out via \SI{1.28}{\giga\bit\per\second} \acp{elink} as soon as a \ac{L1A} signal arrives.
Despite the chip's ability to also measure the \ac{ToA} of the charged pulses, timing information cannot be exploited in the trigger path due to bandwidth constraints.

#+NAME: fig:hgcroc
#+CAPTION: Block diagram of the \ac{HGCROC} [[cite:&hgcroc;&hgcroc_paper]]. It is composed of two data paths: the \ac{DAQ} path (in blue), connected to the \ac{ECON-D}, and the trigger path (in green), connected to the \ac{ECON-T}. It also includes a \ac{PLL}, which generates the clocks needed to operate the chip, and an \ac{I2C} interface, which enables the modification of all static parameters of the chip. Taken from [[cite:&bruno_chep23]]. 
#+BEGIN_figure
#+ATTR_LATEX: :width 1.\textwidth :center
[[~/org/PhD/Thesis/figures/hgcal/HGCROC.pdf]]
#+END_figure

The \ac{TPG} reconstruction chain continues via the \ac{ECON-T} chip, which is located very close to the \acp{HGCROC}, in the so-called "concentrator mezzanine", next to the hexaboard.
The chip concentrates, selects and/or aggregates \acp{TC} within a single module, yielding one data packet per \ac{BX}.
Each module has either \num{3} or \num{6} \acp{HGCROC}, depending on the concerned granularity density region.
The chip first calibrates the input charges coming from the \ac{HGCROC}, converting them into transverse energy values [[cite:&hgcal_backend_tdaq]].
It then builds /module sums/, where the energies of \acp{TC} in a module are summed without any energy threshold being applied.
The \ac{ECON-T} can operate in a number of modes, of which we mention the ones most likely to be used during data-taking, also illustrated in [[fig:econalgos]]:

+ *Threshold algorithm*:
  Selects all \acp{TC} with an energy above a given threshold, subject to bandwidth limits. The size of the output varies event-by-event, and for different modules in the same event. 

+ *Best-Choice algorithm*:
  Selects a fixed number of \acp{TC} with the highest energy. The size of the output is fixed and thus known in advance. Requires sorting, which is implemented via batcher odd-even sorting networks [[cite:&sort_net2;&calorPortales;&sort_net]]. No truncation is applied before the sorting, enabling the configuration of the selected number of \acp{TC}.

+ *Super Trigger Cell algorithm*:
  Reduces the data granularity by summing nearby \acp{TC}. At the same time, information on the energy distribution within a \ac{STC} is kept, by propagating the \ac{TC} with the maximum energy. \acp{STC} will most likely be composed of $2\times2$ \acp{TC} in the scintillator, and will not be considered for the \ac{Si} section. 

The current plan envisions the usage of the \ac{BC} algorithm for the \ac{CE-E} and the \ac{STC} algorithm for the \ac{CE-H}.
This combination is preferred over the threshold algorithm due to the fixed output data size, which leads to a simpler, buffer-less \ac{BE} data unpacking.
Several studies covered different algorithm choices, including using just one for the entire detector, or other combinations.
In spite of the granularity reduction put forward by the \ac{STC} algorithm, its usage is required where the available optical links are not sufficient to transmit all required information to the \ac{BE}.
It was found that, given the existing event-to-event rate inhomogeneities, the \ac{BC} algorithm, given the number of \acp{TC}, occasionally misses an important fraction of the event [[cite:&rate_studies_tps;&cristina_perez_thesis]].
The effect was particularly visible for hadronic jets, where serious cost and space constraints can impose limits on the fiber optics, and thus on the number of \acp{TC} the algorithm can keep.
On the other hand, the usage of \acp{STC} across the \num{47} layers leads to an unacceptable decrease in \ac{EM} resolution.

#+NAME: fig:econalgos
#+CAPTION: Schematic illustration of three data reduction algorithms currently implemented in the \ac{ECON-T} chip. We show low-density modules, but the algorithms are identical for high-density regimes. For displaying purposes, we are assuming the maximum supported bandwidth translates to \num{5} \acp{STC} and \num{6} \acp{TC} per \ac{BX}, where "id" refers to a different block of data being sent to the \ac{BE}, coming from a different module or from a different event in the same module. The threshold algorithm requires a variable data size format. The \ac{STC} visualization represents the scenario where each \ac{STC} corresponds to \num{4} \acp{TC}, or \num{16} \ac{Si} cells in a low-density module.
#+BEGIN_figure
#+ATTR_LATEX: :width 1.\textwidth :center
[[~/org/PhD/Thesis/figures/hgcal/ECONTAlgos.pdf]]
#+END_figure

Another flavour of concentrator chips gathers the \ac{DAQ} data: the \ac{ECON-D}, again one per module.
The \ac{ECON-D} can optionally apply zero suppression, where only channels with an energy above a certain threshold are kept, and then merges all of \ac{HGCAL}'s data into a single packet.
One of the major challenges of the \ac{FE} is the ability to deal with extremely inhomogeneous data rates across \ac{HGCAL}, which may occasionally vary by almost two orders of magnitude.
The \ac{ECON-D} thus relies on a buffering system which supports variations in the size of the packets and in the \ac{L1A} rate.
Despite ensuring one sent package per \ac{L1A} signal, the \ac{ECON-D} cannot guarantee the package's data integrity due to the buffers being full.
\ac{L1} throttling might be required in some cases.

The data is finally then sent via \SI{1.28}{\giga\bit\per\second} e-links to \ac{lpGBT} \acp{ASIC} [[cite:&lpgbt]] located in the \ac{FE} motherboards, or engines.
Each motherboard is connected with up to \num{6} \acp{ECON-T} and \acp{ECON-D}.
The \acp{ASIC} serialize the \ac{ECON} data to \SI{10.24}{\giga\bit\per\second}, and send it to the \ac{VTRx+} interface, which in turn distributes it to the off-detector \ac{BE} via fiber optics.
In total, \SI{\sim 90}{\tera\byte\per\second} are transferred to the \ac{BE} [[cite:&econ]].

* Backend electronics

The \ac{BE}, located at \SI{\sim 100}{\meter} from the detector, receives \ac{FE} data with the goal of building cluster-shape variables within a \SI{\sim 2.5}{\micro\second} latency budget.
Clusters, together with simpler \acp{TT} built out of \acp{STC} and of module sums along the longitudinal direction, amount to the final \ac{HGCAL} \acp{TP} to be transmitted to \ac{L1}.
The \ac{BE} layout is split in two processing stages, called \ac{S1} and \ac{S2}, which run on =Serenity= boards [[cite:&serenity]] with \num{128}-link =Xilinx VU13P= \acp{FPGA}.
The first stage is required to assemble data coming from multiple detector locations into a single board, and thus provide a large enough phase-space to better reconstruct clusters.
Indeed, each \ac{FE} optical link sends data belonging to a few modules only, which get translated into a mere \SI{2}{\percent} of the detector per \ac{S1} \ac{FPGA}.
A second stage can then gather the data corresponding to a larger fraction of \ac{HGCAL} to robustly build \acp{TP}.
Additionally, the more data fits into a single \ac{FPGA}, the less data duplication is required to handle boundaries, especially when taking into account that different \ac{BE} \acp{FPGA} do not communicate with each other.
The current design allots \SI{120}{\degree} of \ac{HGCAL} to each \ac{S2} board, with a \ac{TMT} period and hence a board multiplicity of \num{18}, effectively representing \num{6} identical subsystems.

The \ac{S1} thus receives \ac{ECON-T} data from multiple modules, but from a single \ac{BX}, into \num{14} \acp{FPGA} per \SI{120}{\degree} sector, where the number of boards is driven by the existing optical link multiplicity.
The data is unpacked and an energy rescaling is applied, to correct for the different encodings used by the \ac{BC} and \ac{STC} algorithms.
The \acp{TC} are routed into projective \ac{phi} bins.
In parallel, module sums and \acp{STC} are summed into partial (\ac{eta}, \ac{phi}) \acp{TT}, being formed separately for the \ac{CE-E} and \ac{CE-H}.
Finally, the data is sent to \ac{S2} with a \SI{\sim 140}{\tera\bit\per\second} throughput after time-multiplexing it with a \num{18} \ac{BX} period [[cite:&hgcal_backend_tdaq]].
The \ac{TPG} \ac{BE} architectural layout is illustrated in [[fig:daq_system_overview]], from the \ac{FE} inputs to sending \acp{TP} to the central \ac{L1} system.

#+NAME: fig:daq_system_overview
#+ATTR_LATEX: :width 1.\textwidth
#+CAPTION: Layout of Stage 1 and Stage 2 boards for one HGCAL endcap. The \SI{120}{\degree} symmetry is used to process the data in terms of three identical and independent firmware regions. The full TPG system consists of two identical and independent copies of this layout. Taken from [[cite:&hgcal_backend_tdaq]].
#+BEGIN_figure
[[~/org/PhD/Thesis/figures/hgcal/daq_system_overview.pdf]]
#+END_figure

Before describing \ac{S2}, we briefly dwell on the unintuitive fact that the data throughput between \ac{S1} and \ac{S2} is actually larger than the one between the \ac{ECON-T} and \ac{S1}.
Where does the additional data come from?
The answer is two-fold.
Firstly, one needs to account for the data duplication required to handle boundaries between \SI{120}{\degree} sectors, which is nicely illustrated in [[fig:daq_system_overview]].
Secondly, the data has to be inflated since:
+ the memory addresses have to be encoded on a larger number of bits, because the \ac{S1} covers larger detector regions than the \ac{ECON-T};
+ the energies have to be encoded on a larger number of bits to absorb different energy scales in the different detector regions used by the ECON-T;
+ more bandwidth has always to be allocated to \ac{TC} bins in order to absorb fluctuations and limit truncation effects in the \ac{S1}. This is also true for the fixed-size \ac{BC} algorithm, since it provides a fixed number of \acp{TC} per module, not per bin.

#+NAME: fig:stage2chain
#+CAPTION: Schematic flowchart of \ac{S2}’s reconstruction chain. \Acp{TC} from \ac{S1} are unpacked and processed in a pipelined fashion up to the creation of cluster-related variables, which are fed to the \ac{L1}. The description of the steps can be found in the text, where "histogramming" refers to the first two steps in this figure. Adapted from [[cite:&bruno_chep23]].
#+BEGIN_figure
#+ATTR_LATEX: :width 1.02\textwidth :center
[[~/org/PhD/Thesis/figures/hgcal/Stage2Chain.pdf]]
#+END_figure

The \ac{S2} is designed to perform the main \ac{TPG} reconstruction work: building clusters and \acp{TT}.
Partial tower energies are accumulated into (\ac{eta}, \ac{phi}) bins and clusters are built following the steps highlighted in [[fig:stage2chain]]:

+ *Histogramming*:
  \Acp{TC} are mapped to a projective \coordsa{} space with (216, 42) bins, where $\text{R}=(x^{2}+y^{2})^{1/2}$ and $\tan(\theta)=$ \si{\rz} (see [[#sec:coordinate_system]]).
  These coordinates are chosen since a constant \si{\rz} corresponds to a constant particle angle $\theta$, where R is defined in the plane perpendicular to the \ac{LHC} beamline.
  The coordinates are "projective", since 3D deposits are mapped to a 2D space.
  Energy deposits of neutral particles originating from the center of the detector and spanning several layers will thus lie in a single \rz{} bin.
  The binning further reduces the spatial granularity and, due to its grid-like structure, facilitates vectorized and hence parallel processing in the firmware.
  Each bin contains the energy sum of all its \acp{TC}, together with their \tmip[fn:: The algorithms and data flow of the trigger reconstruction chain are in constant evolution. After the work related in this chapter had been finalized, it was decided to perform the calibration to energy values ($\si{\GeV}$) directly in the \ac{ECON-T}. This means that energy is now measured in transverse energy units, not in \tmip{} units.]-weighted $x/z$ and $y/z$ positions, where \tmip{} is defined as $\text{MIP}/\cos(\theta)$, with one \ac{MIP} being the energy deposited by a minimum ionizing particle [[cite:&PDG \S34.2.3]], and $\theta$ the polar angle introduced in [[#sec:coordinate_system]].
  The weighted positions are defined as follows:

  #+NAME: eq:weighted_position
  \begin{equation}
  \frac{x}{z}\bigg\rvert_{\text{weighted}} = \sum_{i}^{\text{N}_{\text{TC}}}  \frac{\text{MIP}_{\text{T}}^{i}\,x^i}{z^i}
  \kern .5cm
  ,
  \kern .5cm
  \frac{y}{z}\bigg\rvert_{\text{weighted}} = \sum_{i}^{\text{N}_{\text{TC}}}  \frac{\text{MIP}_{\text{T}}^{i}\,y^i}{z^i}
  \kern .4cm
  ;
  \end{equation}

+ *Smearing*:
  An energy smearing step is applied to the \coordsa{} bins to decrease overall variations in their energy distribution.
  This is meant to address biases discussed in [[#sec:cluster_splitting]].
  A convolutional kernel is iteratively slid along both directions, independently.
  For each bin, the energy of all its neigh ours covered by the finitely-sized kernel is multiplied by the corresponding kernel weight, and the energy is updated.
  The kernels are shown in \cref{eq:smooth_kernel}, along \ac{phi} (left) and \si{\rz} (right):

  #+NAME: eq:smooth_kernel
  \begin{equation}
      \left[
        \renewcommand*{\arraystretch}{1.0}
        \begin{array}{ccccccccccc}
          ...&\frac{1}{16}&\frac{1}{8}&\frac{1}{4}&\frac{1}{2}&1&\frac{1}{2}&\frac{1}{4}&\frac{1}{8}&\frac{1}{16}&...
        \end{array}
      \right]
      \hspace{2cm}
      \left[
        \renewcommand*{\arraystretch}{1.0}
        \begin{array}{c}
          \frac{1}{2} \\[.15cm]
          1 \\[.15cm]
          \frac{1}{2} \\
        \end{array}
      \right]
  \end{equation}

  Variations are more prominent along \ac{phi} since the binning is finer.
  The length of the kernel along \ac{phi} is \si{\rz}-dependent, as illustrated by the dots in \cref{eq:smooth_kernel} (left).
  The \ac{phi} kernel collects the energy from more bins for lower \si{\rz} rows.
  The energy of each bin is normalized to its measured energy, in order to ensure that no energy is artificially added to the event.

+ *Seeding*:
  Seeds are local \tmip{} maxima in the histogram, and are so called since they indicate the starting \ac{TC} for clustering algorithms to gather other \acp{TC}.
  Seeds are found via a seeding window which, for each bin, spans its immediately adjacent bins and checks whether their \tmip{} energy is lower than the central bin.
  If so, and if the energy from the central bin lies above a threshold, the bin is promoted to a seed.
  The threshold cut limits the collection of clusters from pure noise.
  We define the /window size/ to be $k$ based on the number of $k^{\text{th}}\text{-order}$ neighbors considered by the seeding window.
  A size of \num{1} will consider the central bin plus its \num{8} closest neighbors, a size of \num{2} will consider $16+8+1=27$ bins, and so on.
  The window size can also be varied differently along the two directions.
  The default \ac{S2} reconstruction uses $k=1$.

+ *Clustering*:
  \Acp{TC} are associated to seeds and used to calculate cluster properties.
  Every seed leads to exactly one cluster.
  Contrary to previous steps, which run on a \coordsa{} space, the clustering uses the $(x/z,\,y/z)$ projective space.
  Two different clustering algorithms are currently defined in the \ac{TPG}, and illustrated in [[fig:clustering_algos]].
  A distance matching threshold is applied to both algorithms to ensure no \ac{TC} is associated to extremely distant seeds.
  The distance is calculated in the same projective space.
  The default matching radius slowly increases with the detector's depth, from 0.015 in the first layer to 0.050 in the last \ac{CE-H} layers.
  The first and default =min_distance= algorithm associates \acp{TC} to their closest seed, based on the 2D distance in the projective space.
  The second algorithm, called =max_energy=, prioritizes an association based on the seed energy, where the highest energy seed is associated to all \acp{TC} within its matching radius, the second-highest energy seed is associated to the remaining \acp{TC} within its (different) matching radius, and so forth.
  If no \ac{TC} is left for the lowest-energy seeds, then no cluster is formed.

#+NAME: fig:clustering_algos
#+CAPTION: Illustration of the two clustering algorithms considered in the \ac{HGCAL} \ac{TPG}. The grid represents the \coordsa{} bins. Black dots represent \acp{TC}. The crosses refer to the position of the seeds, ordered by color from the highest to the lowest energy: red, yellow and green. The respective colored bins corresponds to the bins where the seeds are located. The black circles represent the region of influence of a particular seed on its neighboring \acp{TC}. The two \acp{TC} in red are associated to different seeds depending on the used algorithm. The matching radii can be different for different seeds. \Acp{TC} outside the three circles are not associated to any seed. (Left) The =min_distance= algorithm associates \acp{TC} based on distance. The black dashed lines represent the border between the regions where a particular seed gathers all \acp{TC}. The brown \ac{TC} serves as an example: the distance to the three seeds is shown with brown lines, where the solid line shows the closest seed to that \ac{TC}. (Right) The =max_energy= algorithm prioritizes instead the association based on the energy of the seeds.
#+BEGIN_figure
\centering
#+ATTR_LATEX: :width .95\textwidth :center
[[~/org/PhD/Thesis/figures/hgcal/ClusteringAlgos.pdf]]
#+END_figure

\noindent Once the clusters are defined, cluster-shape variables can be computed.
The full list of variables is not yet defined, but they will surely include the barycenter's position and energy of the clusters.
Additionally, two separate \ac{HAD} and \ac{EM} energy interpretations will be defined, with possibly different parameters, such as radii or energy thresholds.
We refer to "interpretations" since in the \ac{TPG} no particle identification is performed.
