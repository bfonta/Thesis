+ Pythia: beams, hard-scattering, parton showering and hadronisation
+ jet matching and/or merging during hadronization
+ 

* Tunes
The underlying event (UE) consists of the beam-beam remnants (BBR) and the particles that arise from multiple-parton interactions (MPI).
The BBR are what remains after a parton is scattered out of each of the two initial beam hadrons, while the MPI are additional soft or semi-hard parton-parton scatterings that occur within the same hadron-hadron collision [[cite:&CMS_Tunes]].

A set of QCD parameters is derived in different ways (WHICH ONES??) to precisely describe aspects of the UE, such as the modelling of the hadronization, the initial and final state radiation and the BBR.
A complex fitting procedure, using data collected by CDF and CMS at different energies is used, extracting the parameters known as CMS Pythia (CP) Tunes.

* Monte Carlo and data processing in \ac{CMS}
** Data processing chain
+ @Follow section 2.5 of Alessandro's thesis@
  
** Pile-up production
There are two ways to produce samples with simulation of pile-up: premix and classical mixing [[cite:&pileup_production]].

Classical mixing implies a previous production of a \ac{MB} sample (with a datatier "GEN-SIM").
It contains the event at generator level and the interaction of the particles with the detector material.
For the generation of the sample with \ac{PU} (which happens in the =DIGI-RECO= step), a root =wmLHEGS/GS= request is digitized (namely the interaction of the particles with the detector material are used to simulate the signals in the detector cells) together with the PU sample.
The =DIGI-RECO= step needs the PU input dataset and the pile-up scenario, namely according to which distribution the pile-up should be simulated (and added to the root request).
Since the =DIGI= step is quite consuming and happens for both root request and \ac{MB} sample, classical mixing is generally more time and CPU consuming.

Premix is different because the \ac{PU} sample is digitized separately (at the time of the production of the premix library).
A \ac{MB} sample (datatier =GEN-SIM=) is produced in the same way as before, but it is here used for the production of a SingleNeutrino sample (basically nothing in the final state) which is interfaced with the simulation of the \ac{PU} according to a certain scenario and using the \ac{MB} sample.
The output of the SingleNeutrino sample is a =GEN-SIM-DIGI= sample (already digitized).
For root requests using premix \ac{PU} simulation, the =DIGI= step is run only on them, while the \ac{PU} simulation is added after this step.
Since the =DIGI= step is run only once, premix requests are much faster and less CPU consuming than classical mixing requests.


* Additional bibliography :noexport:
+ [[https://indico.cern.ch/event/816226/contributions/3606740/attachments/1947228/3230794/Talk_Tuning_MPI.pdf][Tuning]]
+ [[https://cms-pdmv.gitbook.io/project/untitled-4#what-is-the-difference-between-premix-and-classical-mixing][Classical mixing vs premixing]]
  
* Alessandro :noexport:
Monte Carlo\footnote{The choice of the name dates back to the dawn of the Monte Carlo method during World War II. It reflects the intrinsic random nature of the method, similar to the roulette game one can play at the Monte Carlo Casino} (MC) samples are a powerful and indispensable tool in particle physics. They model complex physics processes, predict experimental outcomes, and help in understanding the behaviour of particles in detectors, simulating a real-life scenario. In this thesis, MC samples are extensively utilised, both in the physics analysis presented in Part \ref{part2} and the development of the HGCAL reconstruction showcased in Part \ref{part3}. In particular, the so-called \textit{full simulation} is used. The concept of full simulation refers to a comprehensive simulation of the entire experimental setup, including the interaction of particles with the detector material, the propagation of particles through the detector components, and the response of the various sub-detectors. The CMS collaboration has implemented a standardised workflow for full MC simulations. Once MC events are generated and real data are collected, both undergo the same data processing, ultimately producing the data formats that the different CMS analysis teams will use. All the steps are illustrated in Fig.~\ref{ch2:fig:simulation}.

+ *GEN*: The initial step involves the GENeration (GEN) of the physics process to be studied, which can range from simulating a single particle to more complex scenarios involving particle production and decay in perturbation theory, along with consideration of the underlying event. The underlying event represents all activity originating from a single particle-particle interaction occurring on top of the process of interest. This includes both initial and final state radiation as well as the interaction between the beam remnants. Unlike the PU, the underlying event is characterised by having the same vertex as the hard scatter, resulting in being tied to the process of interest. 
	
+ *SIM* Once the physics process to study is generated, the output of the GEN step is merely a list of stable particles. This list of particles is then fed to the SIMulation (SIM) of the detector, which is carried out by \texttt{GEANT4} \cite{GEANT4:2002zbu,Allison:2006ve}. \texttt{GEANT4} is a widely used software toolkit for the simulation of the passage of particles through matter. It propagates the particles produced in the GEN step in very little steps in the volume of the detector. At each step, it simulates the interactions with materials and the external electromagnetic field. These very granular iterations cause the simulation with \texttt{GEANT4} to be very time-consuming. One important feature of this step is the geometry loaded in the simulation. A detailed geometry of the CMS detector is embedded in \texttt{GEANT4}, and the one for phase-2 is continuously updated and fixed following the changes in the design of the new detectors to simulate the environment foreseen during the HL-LHC faithfully. The output of this step is a collection of \textit{hits}, representing energy deposits in different volumes of the detector.
	
+ *DIGI* The output of the SIM step is then DIGItalised (DIGI) into electric signals, commonly referred to as \textit{digis}. During this process, the hits generated by \texttt{GEANT4} are converted into the same digital format produced by the actual detector electronics. This includes simulating noise, readout logic, and shaping and digitalising the pulse to faithfully replicate the output of the detector electronics. Since the DIGI level faithfully replicates the output of the detector electronics, the Level-1 (L1) is emulated at this stage, ensuring consistency with the actual data processing. Additionally, if foreseen by the simulation, the simulation of PU is merged at the DIGI level. The generation of PU relies on the same GEN and SIM steps outlined earlier, requiring the production of a number of \textit{minimum bias} events. This number is determined by a random extraction from the PU distribution. Minimum bias events model inelastic proton-proton collisions and are named after the trigger used to select and study them. This trigger is very loose, hence introducing a minimal bias. In CMS, the minimum-bias trigger is solely based on the Hadronic Forward (HF) calorimeter, requiring at least one trigger tower to register a signal above a certain threshold.
	
+ *RAW* The digis are then formatted and packed into the RAW data format, which is the same format provided by the CMS detector. This is also the right format to emulate the HLT. The RAW step is the \textit{trait d'union} between simulation and real data, from this point onward, both types of data follow the same path.

#+NAME: fig:simulation
#+ATTR_LATEX: :width 1.\textwidth
#+CAPTION: Workflow for MC events generation and data processing within the CMS collaboration.
[[~/org/PhD/Thesis/figures/CMSsimulation.pdf]]

The data processing chain begins with the RAW data that are unpacked back into digis.
This marks the starting point of data processing for real data.
These digis and the previous ones must be identical for simulations to ensure consistency between simulated and real data.
Once this step is completed, the data is ready for offline reconstruction, as outlined in Section~\ref{ch2:reco}. 
However, in most cases, the digis are not directly utilised for reconstruction; instead, they are clustered into \textit{recHits} (reconstructed hits).
The output of the offline reconstruction is a data format usually called RECO, which contains detailed information on the reconstructed physics objects.
Due to its high computational intensity, the RECO data format is produced a few times per period of collected data.
This typically occurs within 48 hours of data collection (prompt-reco), at the end of the yearly data-taking period (ReReco), and once during the LS periods (Legacy).
An improved calibration of the detector characterizes each reprocessing.
Run 2 has also seen a second legacy reprocessing, named ultra-legacy. 
Since RECO files contain the most detailed and comprehensive information, they are quite large ($\sim$3 MB/event) and unsuitable for efficient physics analyses.
The AOD format prioritizes the physics object collections used in analyses, retaining only essential hits and a few detector-level details.
The space needed by each event drops to ~500 kB per event.
During Run 1, this format served as the standard reference for CMS analysts.
However, Run 2 collected significantly more data than Run 1, making AOD files impractically large for the related analyses.
To address this challenge, the CMS collaboration introduced a condensed data format known as MiniAOD \cite{Petrucciani:2015gjw}.
MiniAODs are only 10\% of the size of AODs and can be produced from the AOD dataset in 1--2 days.
The size reduction was achieved by reducing numerical precision when not necessary, using lightweight formats for high-level physics objects, storing only those with transverse momentum above a certain threshold and really necessary for physics analyses.
MiniAOD has been the reference data format for Run 2, effectively meeting the needs of most analyses.
To further decrease the data size, the CMS collaboration has now introduced the NanoAOD format \cite{Rizzi:2019rsi}.
The content has been chosen based on Run 2 analysis experience and interactions within the collaboration: a large set of analyses use the same high-level information, requiring fewer lower-level details.
A NanoAOD resembles the typical structure and size of private ntuples, with new features to make it more universal and compatible with central processing tools.
