<<sec:cms_trigger_system.org>>

With proton beams crossing every \SI{25}{\nano\second}, and considering the average \ac{PU} measured during \phase{1} (see [[ref:fig:lhc_lumi_results]]), around \num{e9} $pp$ interactions are expected every second.
Given \SI{\sim 1}{\mega\bit} zero-supressed events, a \SI{\sim 40}{\tera\bit\per\second} throughput would follow.
Meanwhile, the maximum rate which can be realistically archived in real time by the \ac{CMS} online computer farm sits at \SI{\sim 1}{\kilo\hertz}, which already represents a significant improvement with respect to \run{1}.
Collision and the overall data rates are thus much higher than the rate at which data can be written to mass storage.
This implies a necessary throughput reduction by a factor of \num{e4} [[cite:&trigger_tdr_phase1_vol1;&trigger_tdr_phase1_vol2;&trigger_tdr_phase1_vol1;&trigger_tdr_phase1_vol2;&hlt_performance]].

However, among all events produced at \ac{CMS}, only a small minority might carry physical significance.
Most events are related to well known physics processes which are not meant to be covered by the \ac{LHC} programme.
As seen in [[ref:fig:cms_xsec_summary]], the processes studied at \ac{CMS} span at least \num{14} orders of magnitude.
Only processes with cross-sections of \SI{10e7}{\femto\barn} or lower require a significant part of the luminosity which the \ac{LHC} is able to provide.
The data reduction factor, being absolutely crucial due to current technological limitations, can also be justified on physics grounds only.
The reduction forms the basis of the trigger system at \ac{CMS}, which adopts a two-tiered approach.
In the first level, or \ac{L1}, the trigger reduces the \ac{LHC} rate from \SI{40}{\mega\hertz} to \SI{100}{\kilo\hertz} with a \SI{3.8}{\micro\second} latency.
In the second level, the \ac{HLT}, a further reduction down to \SI{1}{\kilo\hertz} with a \SI{300}{\milli\second} is performed.
Latency here refers to the time taken for the data to be processed, from the moment the hard-collison takes place, on to a final decision at \ac{HLT} level on whether the event should be kept for storage.

* The \ac{L1} trigger
The first trigger level is implemented in custom electronics and \acp{FPGA}, and uses reduced granularity to achieve a lower latency.
The data is stored for \SI{3.8}{\micro\second}, equivalent to \num{152} beam crossings.
Each event forwarded to the \ac{HLT} takes \SI{\sim 2}{\mega\byte} which implies, at a rate of \SI{100}{\kilo\hertz}, a bandwidth of \SI{200}{\giga\byte\per\second} supported by the \ac{DAQ}.

The \ac{L1} trigger receives as input the raw data from the \ac{FE} readout electronics.
Some of \ac{L1}'s first stages are handled literally on the detector, to allow for direct access and thus faster processing.
The remaining stages are usually taken care of by racks of \acp{FPGA} still located in the experimental cavern.
The small time budget implies a compromise on the quality of the reconstruction, which is a recurrent theme in all trigger-related matters.

In order to make a quick decision, the \ac{L1} produces so-called /\ac{L1} candidates/, based on low-granular detector information coming from the calorimeter and muon systems.
The tracker does not participate in \ac{L1} trigger decisions during \phase{1}.

@define trigger primitives here@

#+NAME: fig:l1_trigger_design_phase1
#+CAPTION: (Left) Diagram of the \phase{1} \ac{CMS} \ac{L1} during \run{2}. No \ac{L1} tracking is present. (Right) Fractions of the \SI{100}{\kilo\hertz} rate allocation for single- and multi-object triggers and cross triggers in a typical \ac{CMS} physics menu during \run{2}. Adapted from [[cite:&l1_performance]]. 
#+BEGIN_figure
#+ATTR_LATEX: :width .4\textwidth :center
[[~/org/PhD/Thesis/figures/detector/L1TriggerDesignPhase1.pdf]]
#+ATTR_LATEX: :width .6\textwidth :center
[[~/org/PhD/Thesis/figures/detector/L1TriggerFractions.pdf]]
#+END_figure

\paragraph{Calorimeter Trigger}

The ECAL and HCAL information is combined in trigger towers to reconstruct jets, hadronic-taus, e/γ objects, and evaluate the energy sums.
Since the tracker information cannot be used, 

The calorimeter trigger is also split in two layers.
The first one receives, calibrates, and sorts the \acp{TP} sent to the trigger by the \ac{ECAL} and \ac{HCAL}.
The second layer uses these calibrated trigger primitives to reconstruct and calibrate the physics objects
such as electrons/photons, tau leptons, jets, and energy sums.
The calorimeter trigger follows a time-multiplexed trigger design [[cite:&zabi]].
Each main processing node has access to a whole event with a granularity of ∆η×∆φ of 0.087×0.087 radians (where phi is azimuthal angle) in most of the calorimeter acceptance (a slightly coarser granularity is used at high |η|).
A demultiplexer (DeMux) board then reorders, reserializes, and formats the events for the global
trigger processing.
Because the volume of incoming data and the algorithm latency are fixed, the position of all data within the system is fully deterministic and no complex scheduling mechanism is required.
The benefits of time multiplexing include removal of regional boundaries for the object reconstruction and full granularity when computing energy sums.
The multiplicity of processing nodes provides the flexibility to add nodes as required by complex trigger algorithms.
These algorithms are fully pipelined and start processing as soon as the minimum amount of data is received.

\paragraph{Muon Trigger}

On the other hand,
the redundancy of the muon system is used to define the Global Muon Trigger, combining
data from DTs, CSCs, and RPCs. The Calo and Global Muon Trigger are then merged
into the Global Trigger, used by the L1 trigger system to accept or discard the event.

The muon trigger system includes three muon track finders (MTF) which reconstruct muons in
the barrel (BMTF), overlap (OMTF), and endcap (EMTF) regions of the detector, and the global
muon trigger (μGMT, pronounced micro-GMT) for final muon selection. The μGT finally col-
lects muons and calorimeter objects and executes every algorithm in the menu in parallel for
the final trigger decision.
In the upgraded trigger, the BMTF, μGMT, μGT, and Layer-2 use the same type of processor
card. The OMTF and EMTF electronic boards similarly share a common design, whereas Layer-
1, TwinMux, and CPPF each use a different design. All processor cards, however, use a Xilinx
Virtex-7 Field Programmable Gate Array (FPGA). Thus many firmware and control software
components, e.g., data readout and link monitoring, can be reused by several systems, reducing
the workload for development and maintenance.

\paragraph{Frontend electronics}

\paragraph{Backend electronics}

* HLT

Receiving as input the \acp{TP} provided by the \ac{L1}, the \ac{HLT} performs further selection, exploiting the full detector granularity on commercial CPU and GPU processors.
Its target rate and latency are, respectively, \SI{1}{\kilo\hertz}\SI{\sim 200}{\milli\second}.
The selected events are acquired by the \ac{DAQ} and sent for permanent on-tape storage, managed by the \ac{WLCG} [[cite:&wlcg1]].

The CMS HLT performs online event selection using the full reconstruction software framework of CMS called CMSSW
The task of the HLT is to reduce the data rate by a factor of 100, in order to fulfill the requirement of an output rate below 1 kHz.
If one assumes a mean computing time of O(10−2 ) s for each Level-1 accept with a data input rate of O(10 5 ) Hz, it means that the computing cluster that will host the HLT system will be constituted of the order of 1000 CPUs
more than 400 trigger paths targeting a broad range of physics signatures and purpose
In Run-3, usage of GPUs at HLT improves the performance of some triggers with an improved track seeding
Thanks to the advancements in the CMS software framework that can leverage more of the underlying parallelism, after the introduciton of multithreading in run2, in Run-3 the HLT is running jobs with 32 threads, each processing 24 concurrent events, without any significant loss in performance.
[[cite:&hlt_performance]]

Using the full detector readout, with full granularity
reconstruction for the HCAL, ECAL, Pixel Local Reconstruction, Pixel Only Track (used to seed the full tracking and standalone for scouting) and Vertex Reconstruction have been offloaded to GPUs. As a result, the HLT timing and throughput improved by 40% and 80%, respectively. Power consumption is also reduced by 30%.

The output of the HLT is then streamed to the Tier-0 at CERN, where it
is prepared for offline reconstruction and organised into primary datasets (PDs). These
PDs constitute collections of HLT paths, representing the selected events that the CMS
collaboration will further process and analyse.


Ongoing GPU developmental efforts on multiple fronts, such as migration from traditional CMS data formats to Structure of Arrays (SOAs) for better utilization of CPUs and GPUs, rewriting other algorithms (e.g. Particle Flow) to run on GPUs and porting of Heterogeneous code to Alpaka performance portability library [15][16] to reduce code duplication and dependency on a particular architecture.
[[cite:&hlt_run3_gpus]]

increase data throughput from \SI{1}{\kilo\hertz} to \SI{5}{\kilo\hertz} [[cite:&hlt_run3_gpus]]



* Alessandro :noexport:
With a proton-proton interaction rate of about 40 MHz, the CMS detector produces a large amount of data that should be stored for offline analyses. This leads to an overwhelming volume of data that cannot be feasibly stored, as the full detector information amounts to approximately 1 Mb per event, and there is no technology nowadays able to read out and store such vast volumes of data. However, most of the collisions occurring at the LHC are not of interest to the LHC physics programme. The CMS experiment therefore demands the task of identifying events worthy of saving to the Trigger and Data Acquisition System (TriDAS) \cite{CMS:2000mvk,Sphicas:2002gg}. The trigger system is organised into two layers, the \textit{Level-1 trigger}, which reduces the rate from 40 MHz to 100 kHz with a latency time, i.e., time available for data processing, of 3.8~$\mu s$, and the \textit{High-Level Trigger} (HLT), which reduces further the rate down to 1 kHz with a latency time of 300 ms.

The L1 trigger takes the raw data from the front-end readout electronic and has to take in few microseconds a decision about the event. It resembles a reader swiftly scanning newspaper headlines to spot captivating stories. Given its need for quick decision-making, the L1 trigger is positioned close to the detector, and it is mounted on custom hardware, such as Field Programmable Gate Arrays (FPGAs) and Application Specific Integrated Circuits (ASICs), for direct and rapid access to the detector information. The small amount of time allocated to the L1 trigger prevents a detailed event reconstruction. Instead, the L1 trigger produces the so-called \textit{L1 candidates}, based on low-granular detector information and reconstruction of low-resolution physics objects. These L1 candidates rely only on the calorimeters and muon chambers, with the tracker excluded from the L1 reconstruction. The ECAL and HCAL information is combined in \textit{trigger towers} to reconstruct jets, hadronic-taus, $e/\gamma$ objects, and evaluate the energy sums. Since the tracker information cannot be used, electrons and photons are reconstructed as the same object. On the one hand, the calorimeter information forms the \textit{Calo Trigger}. On the other hand, the redundancy of the muon system is used to define the \textit{Global Muon Trigger}, combining data from DTs, CSCs, and RPCs. The Calo and Global Muon Trigger are then merged into the \textit{Global Trigger}, used by the L1 trigger system to accept or discard the event.

Events meeting the L1 trigger requirements proceed to the HLT trigger, which operates within a software computing farm with 32,000 CPU cores. The HLT works with a dataset that has already been enriched with interesting physics events, thanks to the previous L1 trigger sel [[https://gitlab.cern.ch/tdr/notes/AN-21-082/-/merge_requests/56/diffs]] ection, thereby reducing the number of events that should be discarded. Unlike the L1 trigger, the HLT can work with the same raw data as offline reconstruction and include the tracker information previously excluded at the L1 stage. It also employs more sophisticated algorithms akin to those used offline. To meet time constraints, the HLT reconstruction is carried out only around an L1 seed, significantly reducing the computational time. The output of the HLT is then streamed to the Tier-0 at CERN, where it is prepared for offline reconstruction and organised into \textit{primary datasets} (PDs). These PDs constitute collections of HLT paths, representing the selected events that the CMS collaboration will further process and analyse. 

* Jona :noexport:
The full information from all CMS subdetectors amounts to $\sim1\unit{Mb}$ per event; therefore, if read out at the nominal LHC bunch crossing rate of $40\unit{MHz}$, they would produce a total throughput of $\sim40\unit{Tb/s}$. At the present day, technology falls short of efficiently reading and storing such formidable data quantities. However, a substantial portion of these collisions yields low-energy proton-proton interactions, which hold no relevance to the CMS physics program, which targets hard scattering processes. Figure \ref{fig:xs_summary} shows the summary of the cross section measurements of SM processes at CMS; as it can be appreciated, the process with the highest cross section is single $\PW$ boson production with $\sigma(W|\sqrt{s}=13\TeV)=1.8\cdot10^5\unit{pb}$. This value stands six orders of magnitude below the inclusive proton-proton interaction cross section that towers at $\sigma(pp)\sim10^{11}\unit{pb}$. The knowledge of this huge discrepancy can be exploited to perform an online event selection with the goal of reducing the data acquisition rate by $\sim10^5$. This procedure is the so-called \textit{triggering process}, and the CMS Trigger system performs it. After the trigger selection, the data is sent to storage by the Data Acquisition System (DAQ). The Trigger and DAQ are generally jointly referred to as the TriDAS project \cite{TriDAS-TDR}.

\begin{figure}[htbp]
    \centering
    \includegraphics[angle=90,origin=c,width=0.99\textwidth]{figures/Ch2/TriDAS/CMSCrossSectionSummaryBarChart.pdf}
    \caption{Summary of the cross section measurements of Standard Model processes at CMS. The process, centre-of-mass energy of the measurement, and the associated publication are reported on the left of the panel; the integrated luminosity used for each result is reported on the right \cite{CMS_XS_pub}. Values are to be compared to the total proton-proton interaction cross section of about $10^{11}\unit{pb}$.}
    \label{fig:xs_summary}
\end{figure}

The trigger system acts as the bridge linking online data-taking and offline data analysis, the latter being impossible without the former. Therefore, to fulfil the ambitious CMS physics program, the trigger system must adhere to both the technical constraints set by the online hardware system and the stringent efficiency benchmarks and background suppression expected on the side of the analyses. Moreover, adaptability to varied data conditions and resilience against the instantaneous luminosity and PU challenges posed by the LHC are paramount prerequisites for the system. These are the all-important and exacting guidelines that underpin the trigger system design.
    
To achieve the best flexibility of the trigger system, the CMS experiment adopts a two-tiered approach in which the event selection is based on the kinematic properties of the particles produced in an LHC bunch crossing. The two steps have to fulfil very different requirements and are implemented in different kinds of hardware and with different levels of sophistication. The first selection is performed by the Level-1 (L1) trigger, which is composed of dedicated hardware that processes the information from calorimeters and muon systems only with reduced granularity; the L1 has at its disposal a maximum processing time (the so-called \textit{latency}) of $3.8\mus$ and selects the most interesting events for a rate up to $100\unit{kHz}$. The second selection is performed by the High-Level Trigger (HLT), which exploits the full detector granularity on commercial CPU and GPU processors; the HLT has a latency of $\sim200\unit{ms}$ and selects the most interesting events for a rate up to $1\unit{kHz}$. The events thus selected are acquired by the DAQ system and sent for permanent storage in the tapes of the CERN Tier-0 (the core of the so-called \textit{grid}). As it can be appreciated, the triggering process needs to perform a real-time reduction of the data by a factor $4\cdot10^4$ while retaining the most interesting events for physics analysis.

The TriDAS system is detailed in the following, with particular attention given to the Level-1 trigger, especially its calorimeter-based part, as it is a central topic of this Thesis.
