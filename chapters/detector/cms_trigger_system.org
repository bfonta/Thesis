<<sec:cms_trigger_system.org>>

With proton beams crossing every \SI{25}{\nano\second}, and considering the average \ac{PU} measured during \phase{1} (see [[ref:fig:lhc_lumi_results]]), around \num{e9} \ac{pp} interactions are expected every second.
Given \SI{\sim 1}{\mega\bit} zero-supressed events, a \SI{\sim 40}{\tera\bit\per\second} throughput follows.
Meanwhile, the maximum rate which can be realistically archived in real time by the \ac{CMS} online computer farm sits at \SI{\sim 1}{\kilo\hertz}, which already represents a ten-fold improvement with respect to \run{1}.
Collision and data rates are thus much higher than the rate at which data can be written to mass storage.
This implies a throughput reduction by a factor of \num{e4}.
However, among all events produced in \ac{CMS}, only a small fraction carries physical significance.
Most events are related to well known physical interactions which are not meant to be covered by the \ac{LHC} programme.
As seen in [[ref:fig:cms_xsec_summary]], the processes studied by \ac{CMS} span at least \num{14} orders of magnitude.
Only processes with cross-sections of \SI{e7}{\femto\barn} or lower require a significant part of the luminosity which the \ac{LHC} is able to provide.
The large reduction factor, being absolutely crucial given current technological limitations, can therefore be justified on physics grounds.

The needed data reduction is the reason for existence of the \ac{CMS} trigger system: only events deemed interesting are selected for futher processing.
The \ac{CMS} trigger adopts a two-tiered approach.
In the first level, or \ac{L1}, the trigger constrains the \ac{LHC} rate from \SI{40}{\mega\hertz} to \SI{100}{\kilo\hertz}, with a \SI{3.8}{\micro\second} latency.
In the second level, the \ac{HLT}, a further reduction down to \SI{1}{\kilo\hertz} with a \SI{\sim 300}{\milli\second} latency is performed.
Latency here refers to the time taken for the data to be processed in a particular trigger system.
It includes the time taken to read the data from the detectors, the algorithmic time, the \ac{L1A} signal distribution time, and further delays introduced by potential limitations at the level of buffer capacity [[cite:&trigger_tdr_phase1_vol1;&l1TDR;&trigger_tdr_phase1_vol2;&l1_performance;&hlt_performance;&zabi]].

* The Level-1 trigger
<<sec:l1>>

The first trigger level is implemented in custom electronics and \acp{FPGA}, and uses coarsely segmented data from calorimeter and muon datectors to ensure a low latency.
All high-resolution data are nevertheless held in pipeline memories in the \ac{FE} electronics, waiting for a \ac{L1A} signal to be issued.
The data is stored for \SI{3.8}{\micro\second}, equivalent to \num{152} beam crossings, including \SI{\sim 1}{\micro\second} dedicated to trigger algorithms.
Each event forwarded to the \ac{HLT} takes \SI{\sim 2}{\mega\byte} which implies, at a rate of \SI{100}{\kilo\hertz}, a bandwidth usage of \SI{200}{\giga\byte\per\second} supported by the \ac{DAQ} system.
The \ac{L1} trigger receives as input the raw data from the \ac{FE} readout electronics.
The first processing stages are handled literally on the detector, to allow for direct access and thus faster processing.
The remaining stages are taken care of by racks of \acp{FPGA} located in the experimental cavern.
The small time budget implies a compromise on the quality of the reconstruction, which is a recurrent theme in all trigger-related matters.
The muon and calorimeter trigger paths produce data called \acp{TP}, which are merged into the \ac{GT}, and used to issue a \ac{L1A} decision.
\Acp{TP} from the \ac{GEM} muon subdetector are currently being validated, but are not yet used.
The tracker does not participate in \ac{L1} trigger decisions in \phase{1}.
The \ac{L1} rate has been increased to \SI{\sim 110}{\kilo\hertz} during \run{3} to favour the introduction of new parking techniques, discussed in [[ref:sec:parking_scouting]].

#+NAME: fig:l1_trigger_design_phase1
#+CAPTION: (Left) Diagram of the \phase{1} \ac{CMS} \ac{L1} during \run{2}. No \ac{L1} tracking is present. \Acp{TP} are generated from the \ac{DT}, \ac{RPC} and \ac{CSC} muon systems and from the \ac{HF}, \ac{ECAL} and \ac{HBHE} calorimetric subdetectors (where the latter refers to the \ac{HCAL}). The two separate paths are merged into the \ac{GT}, which make a \ac{L1A} decision on whether each particular event should be kept. \Acp{TP} from \acp{GEM} are currently being validated, but are not yet used. (Right) Fractions of the \SI{100}{\kilo\hertz} rate allocation for single- and multi-object triggers and cross triggers in a typical \ac{CMS} physics menu during \run{2}. Adapted from [[cite:&l1_performance]]. 
#+BEGIN_figure
#+ATTR_LATEX: :width .4\textwidth :center
[[~/org/PhD/Thesis/figures/detector/L1TriggerDesignPhase1.pdf]]
#+ATTR_LATEX: :width .6\textwidth :center
[[~/org/PhD/Thesis/figures/detector/L1TriggerFractions.pdf]]
#+END_figure

\paragraph{Calorimeter Trigger}

Like the \ac{CMS} trigger system, the calorimeter trigger is split in two layers.
The first one receives, calibrates, and sorts the \acp{TP} provided by the \ac{HF}, \ac{ECAL} and \ac{HCAL} subdetectors.
The second layer reconstructs and calibrates physics objects such as electrons and photons, tau leptons, jets, and energy sums.
It should be noted that the calorimeter trigger cannot distinguish photons from electrons; they all correspond to energy deposits named /candidates/, which are formed based on information contained in groups of \num{25} crystals, the \acp{TT}.
\ac{L1} reconstruction algorithms work with a granularity dictated by the \acp{TT}, and use discrimination quantities such as the ratio of \ac{HCAL} to \ac{ECAL} energies, or the shape of energy deposits as seen by data-aggregating sliding windows.
The calorimeter trigger takes advantage of the \ac{TMT} mechanism, which allows the treatment of all event data in a single processing board, in contrast with the parallelism that is usually required.
It works by converting the parallel processing in a $\Delta T$ time period of spatially decoupled regions of the detector into the sequential processing of the full detector in $N\Delta T$ time, where $N$ corresponds to the "depth" of the \ac{TMT}, equating to the number of available boards with enough memory to process one event in full.
The benefit of using the \ac{TMT} is the removal of event region boundaries, enabling the reconstruction of a full event at once.
It was found that the tradeoff between the additional time required to organize the data in series and the time saved thanks to the faster global processing is largely favourable [[cite:&zabi]].
A \ac{DeMux} board then reorders, reserializes, and formats the events for the \ac{GT} processing.
The running algorithms are fully pipelined and start processing data as soon as it is received.

\paragraph{Muon Trigger}

The redundancy of the muon system in terms of \acp{DT}, \acp{CSC} and \acp{RPC} is used to robustly define the \ac{GMT}.
The subdetectors identify track segments from the information produced by the hits in the gas chambers.
The segments are collected and transmitted via optical fibers to three muon track finders: the \ac{EMTF}, the \ac{BMTF} and the \ac{OMTF}.
These run pattern recognition algorithms to identify candidates as muons and extract the their momenta.
The track finders play the role of regional triggers, and send the best muon candidates to the \ac{GMT}.
The \ac{GMT} receives the information and tries to correlate the muon candidates from the tree regions in order to make a decision.
It also merges compatible muon candidates found by more than one single system, and can reject candidates based on their identification quality or the lack of confirmation from other muon systems [[cite:&l1_trigger_upgrade]].

\paragraph{Global Trigger}

The \ac{GT} collects all \ac{L1} muon and calorimeter candidates, and executes multiple selection and identification algorithms, in parallel, for the final trigger decision.
Kinematical quantities of the candidates are exploited, such as the invariant mass of \egamma{} or $\mu$ objects, or the angular distance between two objects.

* The High Level Trigger
<<sec:hlt>>

The HLT is provided by a subset of thousands of commercial \ac{CPU} and \ac{GPU} processors of the online farm located at the \ip{5}, running the full reconstruction software framework of \ac{CMS}, \ac{CMSSW}.
The goal of the \ac{HLT} is to reduce the rate from \num{100} to \SI{\sim 1}{\kilo\hertz}, while keeping the most interesting events.
More than \num{400} trigger paths are available, targeting a broad range of physics signatures.
Given the tight time budget of \SI{\sim 300}{\milli\second} per event, the reconstruction is performed starting from previously selected \ac{L1} seeds, which correspond to events that issued a \ac{L1A}.
The \ac{HLT} farm processes the data starting with \acp{BU}, which gather data from multiple subdetectors, and later with \acp{FU}, which decompress, resconstruct and filter the events.
The full granularity information is available, together with tracker information, which was absent in \ac{L1}.
The selected events are acquired by the \ac{DAQ} and streamed to the Tier-0 at \ac{CERN}, where it is prepared for offline reconstruction and organised into \acp{PD}.
The latter are defined based on collections of \ac{HLT} trigger paths, as for instance muon or $e/\gamma$ triggers.
The reconstructed data is eventually sent for permanent on-tape storage, managed by the \ac{WLCG} [[cite:&wlcg1]].

In \run{3}, the added usage of \acp{GPU} at the \ac{HLT} improved the performance of some triggers, including an improved track seeding, which lead to an increase of the data throughput from \num{1} to \SI{5}{\kilo\hertz}.
This is due to several advancements in \ac{CMSSW}, which can now leverage more parallelism while exploiting a new heterogeneous architecture.
The reconstruction of several subdetectors has been offloaded to \acp{GPU}, including the \ac{HCAL}, \ac{ECAL} and some parts of the reconstruction of pixels and vertices.
As a consequence, the \ac{HLT} timing and throughput improved by 40% and 80%, respectively, and the computing power consumption was reduced by 30%.
Other potential improvements are currently being explored, such as the migration from traditional \ac{CMS} data formats to \acp{SoA} for better utilization of \acp{CPU} and \ac{GPU}, the rewriting of other algorithms having \ac{GPU} parallelism in mind, and the porting of heterogeneous code to the Alpaka performance portability library [[cite:&alpaka1;&alpaka3;&alpaka2]] to reduce code duplication and dependency on a particular vendor architecture [[cite:&hlt_run3_gpus]].

The \ac{CMS} \ac{HLT} system is constantly subject to optimizations and updates, in order to improve algorithmic efficiency to potentially increasing rates, while maintaing an excellent physics performance, despite the increasingly more challeging running conditions [[cite:&performance_calorimeter_trigger;&performance_muon_trigger]].


* Parking and scouting
<<sec:parking_scouting>>

The quest for ever higher event rates with the current technology, and the experimental benefits that can be derived, has lead \ac{CMS} to explore the non-standard use of triggers.
There are various constraints imposed on the trigger system and on the data processing framework which limit the number of events that can be selected, recorded and analyzed.
Some examples include the \SI{\sim 100}{\kilo\hertz} \ac{L1} acquisition rate, which is limited by dead time, the \ac{HLT} latency, which is constrained by the available number and speed of processing cores, or the available permanent storage space, which is distributed across disks and tape, the former providing faster accesse but reduced storage size.
The absolute and relative cost of all these components also has a strong impact on the overall capacity and structure of the computing farm.

- explain each technique
+ mention expected improvements
+ finalize with future benefits on H physics, specifically hh and hhh

  
parking, which consists of writing events directly to tape during
the latest stages of an LHC fill when the rate is substantially decreased due to the lower instan-
taneous luminosity. Therefore, the parked events consist of the complete raw data information
that can be exploited to increase signal acceptance and perform quasi-unbiased physics analyses.

scouting
events take a lot of space with offline reco; we can't afford saving too many of those with our bandwidth/storage capacity; but HLT reconstruction yields smaller events (at the cost of lower granularity & less info saved), so saving events with HLT reco instead of offline reco allow for MORE events to be recorded

hh and hhh
[[cite:&parking_scouting;&parking_scouting_run3_cms]]
