<<sec:trigger_primitives_dataflow>>


+ Talk about trigger and how important it is
+ mention online and offline
+ give overview about the full cms trigger system, including current one
+ explain the processing of hgcal trigger primitives, include quantitiative estimates

Following the present established design [9, 10], the CMS trigger and data acquisition system will continue to feature two trigger levels, with only one synchronous hardware-based Level-1 Trigger, consisting of custom electronic boards and operating on dedicated data streams, and a second level, the High Level Trigger (HLT), using software algorithms running asynchronously [[cite:&hlttdr]]

A completely redesigned Level-1 trigger system [11], making use of tracking information and particle-flow algorithms, will select events at a rate up to 750 kHz (to be compared with a maximum Level-1 accept rate of 100 kHz for Phase-1) to maintain the efficiency of the signal selection to the level of the Phase-1 performance while enabling or enhancing the selection of possible new physics leading to unconventional signatures [[cite:&hlttdr]]

\Ac{HGCAL} will be integrated with the online firmware trigger system put in place by \ac{CMS}, the \ac{L1} [[cite:&l1TDR]], which precedes the \ac{HLT} running on standard servers.
\Ac{L1} performs an online selection of interesting physics processes, whose cross sections are typically orders of magnitude lower than the total proton-proton cross section.

These proceedings describe the \ac{HGCAL} \ac{L1} reconstruction chain, from raw energy deposits to the creation of \acp{TP}, which are detector-specific inputs to the \ac{L1}.
Special emphasis is placed on the study of cluster splitting, which represents a known and so far unstudied shortcoming of the \ac{TP} chain.
The \ac{phi} in the transverse plane, the radial coordinate $R$ and the $z$-axis lying parallel to the beam-line form the binned projective \coordsa{} coordinate system used in this work [[cite:&cms_collab]].
Since $\tan(\theta) = R/z$, where $\theta$ is measured from the $z$-axis, for a constant angle $\theta$ corresponds a constant \rz{}.
Energy deposits of neutral particles spanning several layers will thus lie in a single \rz{} bin.
The latter's scintillation light is read out by direct on-tile \ac{Si} photo-multipliers. Stainless steel and \ch{Cu} are used as absorbers.
\ac{Si} sensors are available in three types with varying thicknesses (120, 200 and 300 \si{\micro\meter}), capacitances and sizes, to widthstand different radiation conditions according to their physical location.
The size of each \ac{Si} sensor is \SI{0.52}{\cm\squared} (for \SI{120}{\micro\meter} \ac{Si} sensors) and \SI{1.18}{\cm\squared} (\qty{200} and \SI{300}{\micro\meter}).
Scintillators range in size from \qtyrange{4}{30}{\cm\squared}, and the number of \ac{Si} (scintillator) channels is \num{\sim 6} million (\num{\sim 240} thousand).
In total, \SI{\sim 620}{\meter\squared} of \ac{Si} and \SI{\sim 400}{\meter\squared} of plastic scintillator are employed.
The full system operates at a temperature of \SI{-35}{\celsius} maintained by a \ch{CO2} cooling system.

The area of the \ac{Si} cells is constrained, among other factors, by the size of \acp{TC}.
There is the need to maintain an integer number of \acp{TC} per module, both in the low- and high-density regimes.
The next larger size allowed, despite significant cost benefits, would lead to an unnaceptable loss of physics performance.

The goal of \ac{TPG} is to provide valuable information to the CMS Level-1 (L1) trigger, within limited time and bandwidth budgets, via \acp{TP}.
\acp{TP} are the building blocks of \acp{L1}.
In HGCAL, they consist of module towers and cluster-related variables, such as energy, positions and shapes.
\acp{TPG} includes all steps from data collection in the front-end (FE) chips at \SI{40}{\mega\hertz} to the production of \acp{TP} in the \ac{BE} electronics [[cite:&hgcalTDR]].
It follows the principle of reducing data throughput as much and as soon as possible, exploiting pipelined algorithms whenever feasible.
It must fit within a latency of \SI{\sim 5}{\micro\second}, taken from the total L1 \SI{12.5}{\micro\second} latency [[cite:&l1TDR]].
It has to cope with power, cost, space and channel routing constraints [[cite:&jb_hdr]].

The "three-fold diamond" configuration was chosen because it allows the convenient definition of symmetric groups of neighbouring cells to form trigger primitives, as shown in ref:fig:tcs_geometry, and the subdivision of the module into symmetric domains for the readout chips, simplifying the layout of the module readout \ac{PCB} [[cite:&l1TDR]].

#+NAME: fig:tcs_geometry
#+CAPTION: Illustration of the three-fold diamond configuration of an hexagonal \SI{8}{\inch} module, used to associate single \ac{Si} cells to groups of cells used for triggering, or \acp{TC}. Low density modules (left) associate four sensors to each trigger cell, while high density modules (right) create \acp{TC} with nine channels each. All modules have exactly \num{48} \acp{TC}, effectively removing a layer of complexity when processing \acp{TC}. The actual physical dimensions of the \acp{TC} vary given the boundaries of the hexagonal modules. Taken from [[cite:&hgcalTDR]]. 
#+BEGIN_figure
#+ATTR_LATEX: :width 1.\textwidth :center
[[~/org/PhD/Thesis/figures/hgcal/TCs_geometry.pdf]]
#+END_figure

#+NAME: fig:l1chain
#+CAPTION: Simplified schematic of the dataflow of \acp{TP} in HGCAL, starting (ending) in the top left  (bottom left) corner. The diagram follows the \ac{TP} processing in a Si layer through the \ac{FE} and \ac{BE}, and up to the \ac{L1}, including expected approximate bandwidths. Trigger decisions at this stage will impact the \ac{HLT} and, consequently, physics analysis. Taken from [[cite:&bruno_chep23]].
#+BEGIN_figure
#+ATTR_LATEX: :width 1.\textwidth
[[~/org/PhD/Thesis/figures/hgcal/l1chain.pdf]]
#+END_figure

#+NAME: fig:stage2chain
#+CAPTION: Schematic flowchart of S2â€™s reconstruction chain. TCs from S1 are unpacked and processed in a pipelined fashion up to the creation of cluster-related variables, which are fed to L1. The description of the steps can be found in the text. Taken from [[cite:&bruno_chep23]].
#+BEGIN_figure
#+ATTR_LATEX: :width 1.02\textwidth :center
[[~/org/PhD/Thesis/figures/hgcal/stage2chain.pdf]]
#+END_figure

#+NAME: fig:daq_system_overview
#+ATTR_LATEX: :width 1.\textwidth
#+CAPTION: Caption. Taken from.
#+BEGIN_figure
[[~/org/PhD/Thesis/figures/hgcal/daq_system_overview.pdf]]
#+END_figure

#+NAME: fig:hgcroc_econt
#+CAPTION: Taken from [[cite:&bruno_chep23]]. 
#+BEGIN_figure
#+ATTR_LATEX: :width 1.\textwidth :center
[[~/org/PhD/Thesis/figures/hgcal/HGCROC_ECONT.pdf]]
#+END_figure

#+NAME: fig:econt_algorithms
#+CAPTION: Taken from [[cite:&bruno_chep23]]. 
#+BEGIN_figure
#+ATTR_LATEX: :width 1.\textwidth :center
[[~/org/PhD/Thesis/figures/hgcal/econt_algorithms.pdf]]
#+END_figure

#+NAME: fig:si_sci_custom_geoms
#+CAPTION: Taken from [[cite:&bruno_chep23]]. 
#+BEGIN_figure
#+ATTR_LATEX: :width 1.\textwidth :center
[[~/org/PhD/Thesis/figures/hgcal/SimplifiedGeometry.pdf]]
#+END_figure

#+NAME: fig:geom_impl_flow
#+CAPTION: Caption 
#+BEGIN_figure
#+ATTR_LATEX: :width 1.\textwidth :center
[[~/org/PhD/Thesis/figures/hgcal/flowchart.pdf]]
#+END_figure

In the \ac{FE}, trigger data processing is performed by \ac{HGCAL}'s dedicated read-out chips (\acp{HGCROC} [[cite:&hgcroc]]) at \SI{300}{\tera\byte\per\second}, and by \ac{ECON-T} chips at \SI{90}{\tera\byte\per\second} [[cite:&econ;&hgcalTDR]].
The \ac{HGCROC} reduces the prohibitive data throughput by grouping 4 or 9 channels into \acp{TC}, where each \ac{Si} module comprises 48 \acp{TC}.
Only \acp{TC} in odd-numbered layers are considered.
Timing information cannot be exploited in the trigger path due to bandwidth constraints.
The ECON-T concentrates, selects and/or aggregates TCs within a single module (3 or 6 \acp{HGCROC}) and builds \textit{module sums}, where the energies of TCs in a module are summed without applying any threshold.
The data is then sent via \SI{1.28}{\giga\bit\per\second} e-links to lpGBT ASICs [[cite:&lpgbt]], serialized to \SI{10.24}{\giga\bit\per\second}, and sent via optical-links [[cite:&vtrxp]] to the off-detector \ac{BE}.

My work is mostly concerned with the \ac{BE}, which is composed of two processing stages (\ac{S1} and \ac{S2}) running on Serenity boards [[cite:&serenity]] with 128-link Xilinx VU13P FPGAs.
Their assigned latency budget is \SI{\sim 2.5}{\micro\second}.
\acp{FPGA} in \ac{S1} cover \SI{\sim 2}{\percent} only of one endcap and, just like \ac{S2} boards, do not communicate with each other[fn::Handling boundaries thus requires data duplication.].
The \ac{S1} receives \ac{ECON-T} data, unpacks and calibrates it.
It then routes and sorts \acp{TC} in energy into projective \SI{2}{\azimuth{}} vs. \SI{42}{\rz} bins per \SI{120}{\degree} sector, where $\text{R}=(x^{2}+y^{2})^{1/2}$ in the plane perpendicular to the beamline and $\tan(\theta)=$ \si{\rz} (a constant \si{\rz} corresponds to a constant particle angle $\theta$).
The sorting uses batcher odd-even sorting networks [[cite:&sort_net2;&calorPortales;&sort_net]], where on-the-fly truncation reduces the total number of comparators required.
Modules sums are here partially summed into module towers, and time multiplexing [[cite:&zabi]] with a bunch-crossing period of 18 is applied before sending the data to \ac{S2}.
\ac{S2} accumulates partial tower energies into (\rapidity{},$\,$\azimuth{} ) bins and builds clusters from \acp{TC}:

+ *Histogramming*:
  TCs are mapped to a \coordsa{} space with (216, 42) bins.
  This further reduces spatial granularity and facilitates vectorized/parallel processing in the firmware due to its grid-like structure.
  Each bin contains the energy sum of all its \acp{TC}, together with their \tmip{}[fn:: \tmip{} is defined as $\text{mip}/\cos(\theta)$, where one mip stands for the energy deposited by a minimum ionizing particle [[cite:&PDG \S34.2.3]] .]-weighted ($x/z, y/z$) positions.

+ *Smoothing*:
  An energy smearing step is applied to \coordsa{} bins to decrease overall variations in their energy distribution.
  A kernel is applied, where to each bin's energy a fraction of the energy of its neighbors is added.
  The kernels are shown in \cref{eq:smooth_kernel}, along \azimuth{} (left) and \si{\rz} (right):

  #+NAME: eq:smooth_kernel
  \begin{equation}
      \left[
        \renewcommand*{\arraystretch}{1.0}
        \begin{array}{ccccccccccc}
          ...&\frac{1}{16}&\frac{1}{8}&\frac{1}{4}&\frac{1}{2}&1&\frac{1}{2}&\frac{1}{4}&\frac{1}{8}&\frac{1}{16}&...
        \end{array}
      \right]
      \hspace{2cm}
      \left[
        \renewcommand*{\arraystretch}{1.0}
        \begin{array}{c}
          \frac{1}{2} \\[.15cm]
          1 \\[.15cm]
          \frac{1}{2} \\
        \end{array}
      \right]
  \end{equation}

  Variations are more prominent along \azimuth{} since the binning is finer.
  The kernel along \azimuth{} is \si{\rz}-dependent, as illustrated by the dots in \cref{eq:smooth_kernel}.
  The \azimuth{} kernel collects the energy from more bins for lower \si{\rz} rows.
  The energy of each bin is normalized to ensure no energy is artificially added to the event.

+ *Seeding*:
  Seeds are local \tmip{} maxima in the histogram.
  They are found via a seeding window which, for each bin, spans its immediately adjacent bins and checks whether their \tmip{} energy is lower.
  If it is, and if its energy lies above a threshold, the bin is promoted to a seed.

+ *Clustering*:
  \acp{TC} are associated to seeds and used to calculate cluster properties.
  Every seed originates a cluster.
  Contrary to previous steps, the clustering uses a $(x/z,\,y/z)$ projective space.
  Two algorithms exist, one associating \acp{TC} to their closest seed (default), the other prioritizing association based on seed energy.

During my PhD I have implemented from scratch the entire \ac{S2} reconstruction chain in a standalone =Python= code[fn:: \url{https://github.com/bfonta/bye_splits}]
It was previously only available in =C++=, within CMSSW [[cite:&cmssw]].
The code enables exponentially faster prototyping, testing and optimization, which are the basis of the following studies.

** Random
+ [[cite:&hlttdr]] (I wrote Section 12.3)
  
** Reconstruction code
Given its location and number of active sensors, data rates of \SI{\sim 100}{\tera\byte\per\second} are expected.

This requires the development of reconstruction code capable of fully exploiting the increased granularity under the expected extreme conditions.
The biggest contributor to CPU usage is event reconstruction, of which currently âˆ¼5% is
used by HGCAL [5]. CMS plans to port part of its reconstruction to Graphics Processing
Units (GPUs), which represent one of the most promising hardware accelerator technologies on
the market. GPUs are a key element when one considers taking advantage of heterogeneous
architectures available on traditional and High-Performance Computing grid sites, including the
upgraded Worldwide LHC Computing Grid. GPUs also promote the development of algorithms
with better computing performance, and profit from a potentially favourable cost when compared
to CPUs, per unit capacity. CMS is planning to adopt a heterogeneous High Level Trigger (HLT)
farm already in Run 3 (2022â€“2025), where âˆ¼30% of the workflow will be offloaded to GPUs (50%
and 80% by the end of Run 4 and 5, respectively) [6]. 

The reconstruction model envisioned for \ac{HGCAL} is intended to be fast and flexible, comprising a sequence of modules/stages which transform raw data into physics objects.
After the initial generation, simulation, digitization [5]
and calibration steps, energy deposits (hits) are clustered by CLUE, a fully-parallelizable density-
based clustering algorithm [8], in order to form two-dimensional objects. In a nutshell, CLUE
assigns an energy density and a separation distance to all hits, which are later used to classify
each hit as either a seed, a follower (based on the hitâ€™s nearest highest density), or an outlier.
Clusters are built by traversing the tree of followers of each seed, assigning the index of the
seed to all its followers. This work includes the calculation of the cluster energy and cartesian
positions, which are computed in the device (section 3.1). In addition, a heterogeneous approach
for navigating through the detectorâ€™s geometrical/topological information is devised and used
within CLUE (section 3.2).

* Additional bibliography :noexport:
+ JB reference: https://cernbox.cern.ch/pdf-viewer/public/cLosQkewmONZakQ/220606_Dauncey_DN-19-032-V2.pdf?contextRouteName=files-public-link&contextRouteParams.driveAliasAndItem=public%2FcLosQkewmONZakQ&items-per-page=100
+ Mentin my proceedings [[cite:&bruno_chep23]]
