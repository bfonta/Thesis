These proceedings describe the \ac{HGCAL} \ac{L1} reconstruction chain, from raw energy deposits to the creation of \acp{TP}, which are detector-specific inputs to the \ac{L1}.
Special emphasis is placed on the study of cluster splitting, which represents a known and so far unstudied shortcoming of the \ac{TP} chain.
The \ac{phi} in the transverse plane, the radial coordinate $R$ and the $z$-axis lying parallel to the beam-line form the binned projective \coordsa{} coordinate system used in this work [[cite:&cms_collab]].
Since $\tan(\theta) = R/z$, where $\theta$ is measured from the $z$-axis, for a constant angle $\theta$ corresponds a constant \rz{}.
Energy deposits of neutral particles spanning several layers will thus lie in a single \rz{} bin.
The latter's scintillation light is read out by direct on-tile \ac{Si} photo-multipliers (\cref{fig:hgcal}). Stainless steel and \ch{Cu} are used as absorbers.
\ac{Si} sensors are available in three types with varying thicknesses (120, 200 and 300 \si{\micro\meter}), capacitances and sizes, to widthstand different radiation conditions according to their physical location.
The size of each \ac{Si} sensor is \SI{0.52}{\cm\squared} (for \SI{120}{\micro\meter} \ac{Si} sensors) and \SI{1.18}{\cm\squared} (\qty{200} and \SI{300}{\micro\meter}).
Scintillators range in size from \qtyrange{4}{30}{\cm\squared}, and the number of \ac{Si} (scintillator) channels is \num{\sim 6} million (\num{\sim 240} thousand).
In total, \SI{\sim 620}{\meter\squared} of \ac{Si} and \SI{\sim 400}{\meter\squared} of plastic scintillator are employed.
The full system operates at a temperature of \SI{-35}{\celsius} maintained by a \ch{CO2} cooling system.

The goal of \ac{TPG} is to provide valuable information to the CMS Level-1 (L1) trigger, within limited time and bandwidth budgets, via \acp{TP}.
\acp{TP} are the building blocks of \acp{L1}.
In HGCAL, they consist of module towers and cluster-related variables, such as energy, positions and shapes.
\acp{TPG} includes all steps from data collection in the front-end (FE) chips at \SI{40}{\mega\hertz} to the production of \acp{TP} in the \ac{BE} electronics [[cite:&hgcalTDR]].
It follows the principle of reducing data throughput as much and as soon as possible, exploiting pipelined algorithms whenever feasible.
It must fit within a latency of \SI{\sim 5}{\micro\second}, taken from the total L1 \SI{12.5}{\micro\second} latency [[cite:&l1TDR]].
It has to cope with power, cost, space and channel routing constraints [[cite:&jb_hdr]].

In the \ac{FE}, trigger data processing is performed by \ac{HGCAL}'s dedicated read-out chips (\acp{HGCROC} [[cite:&hgcroc]]) at \SI{300}{\tera\byte\per\second}, and by \ac{ECON-T} chips at \SI{90}{\tera\byte\per\second} [[cite:&econ;&hgcalTDR]].
The \ac{HGCROC} reduces the prohibitive data throughput by grouping 4 or 9 channels into \acp{TC}, where each \ac{Si} module comprises 48 \acp{TC}.
Only \acp{TC} in odd-numbered layers are considered.
Timing information cannot be exploited in the trigger path due to bandwidth constraints.
The ECON-T concentrates, selects and/or aggregates TCs within a single module (3 or 6 \acp{HGCROC}) and builds \textit{module sums}, where the energies of TCs in a module are summed without applying any threshold.
The data is then sent via \SI{1.28}{\giga\bit\per\second} e-links to lpGBT ASICs [[cite:&lpgbt]], serialized to \SI{10.24}{\giga\bit\per\second}, and sent via optical-links [[cite:&vtrxp]] to the off-detector \ac{BE}.

My work is mostly concerned with the \ac{BE}, which is composed of two processing stages (\ac{S1} and \ac{S2}) running on Serenity boards [[cite:&serenity]] with 128-link Xilinx VU13P FPGAs.
Their assigned latency budget is \SI{\sim 2.5}{\micro\second}.
\acp{FPGA} in \ac{S1} cover \SI{\sim 2}{\percent} only of one endcap and, just like \ac{S2} boards, do not communicate with each other[fn::Handling boundaries thus requires data duplication.].
The \ac{S1} receives \ac{ECON-T} data, unpacks and calibrates it.
It then routes and sorts \acp{TC} in energy into projective \SI{2}{\azimuth{}} vs. \SI{42}{\rz} bins per \SI{120}{\degree} sector, where $\text{R}=(x^{2}+y^{2})^{1/2}$ in the plane perpendicular to the beamline and $\tan(\theta)=$ \si{\rz} (a constant \si{\rz} corresponds to a constant particle angle $\theta$).
The sorting uses batcher odd-even sorting networks [[cite:&sort_net2;&calorPortales;&sort_net]], where on-the-fly truncation reduces the total number of comparators required.
Modules sums are here partially summed into module towers, and time multiplexing [[cite:&zabi]] with a bunch-crossing period of 18 is applied before sending the data to \ac{S2}.
\ac{S2} accumulates partial tower energies into (\rapidity{},$\,$\azimuth{} ) bins and builds clusters from \acp{TC}:

+ *Histogramming*:
  TCs are mapped to a \coordsa{} space with (216, 42) bins.
  This further reduces spatial granularity and facilitates vectorized/parallel processing in the firmware due to its grid-like structure.
  Each bin contains the energy sum of all its \acp{TC}, together with their \tmip{}[fn:: \tmip{} is defined as $\text{mip}/\cos(\theta)$, where one mip stands for the energy deposited by a minimum ionizing particle [[cite:&PDG \S34.2.3]] .]-weighted ($x/z, y/z$) positions.

+ *Smoothing*:
  An energy smearing step is applied to \coordsa{} bins to decrease overall variations in their energy distribution.
  A kernel is applied, where to each bin's energy a fraction of the energy of its neighbors is added.
  The kernels are shown in \cref{eq:smooth_kernel}, along \azimuth{} (left) and \si{\rz} (right):

  #+NAME: eq:smooth_kernel
  \begin{equation}
      \left[
        \renewcommand*{\arraystretch}{1.0}
        \begin{array}{ccccccccccc}
          ...&\frac{1}{16}&\frac{1}{8}&\frac{1}{4}&\frac{1}{2}&1&\frac{1}{2}&\frac{1}{4}&\frac{1}{8}&\frac{1}{16}&...
        \end{array}
      \right]
      \hspace{2cm}
      \left[
        \renewcommand*{\arraystretch}{1.0}
        \begin{array}{c}
          \frac{1}{2} \\[.15cm]
          1 \\[.15cm]
          \frac{1}{2} \\
        \end{array}
      \right]
  \end{equation}

  Variations are more prominent along \azimuth{} since the binning is finer.
  The kernel along \azimuth{} is \si{\rz}-dependent, as illustrated by the dots in \cref{eq:smooth_kernel}.
  The \azimuth{} kernel collects the energy from more bins for lower \si{\rz} rows.
  The energy of each bin is normalized to ensure no energy is artificially added to the event.

+ *Seeding*:
  Seeds are local \tmip{} maxima in the histogram.
  They are found via a seeding window which, for each bin, spans its immediately adjacent bins and checks whether their \tmip{} energy is lower.
  If it is, and if its energy lies above a threshold, the bin is promoted to a seed.

+ *Clustering*:
  \acp{TC} are associated to seeds and used to calculate cluster properties.
  Every seed originates a cluster.
  Contrary to previous steps, the clustering uses a $(x/z,\,y/z)$ projective space.
  Two algorithms exist, one associating \acp{TC} to their closest seed (default), the other prioritizing association based on seed energy.

During my PhD I have implemented from scratch the entire \ac{S2} reconstruction chain in a standalone =Python= code[fn:: \url{https://github.com/bfonta/bye_splits}]
It was previously only available in =C++=, within CMSSW [[cite:&cmssw]].
The code enables exponentially faster prototyping, testing and optimization, which are the basis of the following studies.

* Additional bibliography :noexport:
+ JB reference: https://cernbox.cern.ch/pdf-viewer/public/cLosQkewmONZakQ/220606_Dauncey_DN-19-032-V2.pdf?contextRouteName=files-public-link&contextRouteParams.driveAliasAndItem=public%2FcLosQkewmONZakQ&items-per-page=100
