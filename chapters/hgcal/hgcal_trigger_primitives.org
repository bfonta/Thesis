:PROPERTIES:
:CUSTOM_ID: sec:hgcal_trigger_primitives
:END:

The importance of the online and offline \ac{CMS} trigger systems was already covered in [[#sec:cms_trigger_system]].
Here we detail the hardware and software infrastructure powering the \ac{HGCAL} \ac{L1} \ac{DAQ} and \ac{TPG} systems, the latter producing so-called \acp{TP}, which correspond to the building blocks for \ac{L1} decisions.
The flow of \acp{TP} can be schematically visualized in [[fig:l1_trigger_design_phase2]], where \ac{HGCAL} is the second subdetector with the most allocated resources, due to its high granularity.

@reintroduce TPs, which were already mentioned in [[#sec:cms_trigger_system]]@

\Ac{HGCAL} will be integrated with the online firmware trigger system put in place by \ac{CMS}, the \ac{L1} [[cite:&l1TDR]], which precedes the \ac{HLT} running on standard servers.

The goal of \ac{TPG} is to provide valuable information to the CMS Level-1 (L1) trigger, within limited time and bandwidth budgets, via \acp{TP}.
\acp{TP} are the building blocks of \acp{L1}.
In HGCAL, they consist of module towers and cluster-related variables, such as energy, positions and shapes.
\acp{TPG} includes all steps from data collection in the front-end (FE) chips at \SI{40}{\mega\hertz} to the production of \acp{TP} in the \ac{BE} electronics [[cite:&hgcalTDR]].
It follows the principle of reducing data throughput as much and as soon as possible, exploiting pipelined algorithms whenever feasible.
It must fit within a latency of \SI{\sim 5}{\micro\second}, taken from the total L1 \SI{12.5}{\micro\second} latency [[cite:&l1TDR]].
It has to cope with power, cost, space and channel routing constraints [[cite:&jb_hdr]].



+ explain the processing of hgcal trigger primitives, include quantitative estimates
  + how are tcs physically arranged
  + frontend (HGCROC, ECON-T, ECON-D, ...)  
+ discuss frontend and backend electronics from [[cite:&jb_hdr]] and compare with [[#sec:l1]]

* The infrastructure of the backend trigger and data acquisition systems
# Intro
The \ac{HGCAL} \ac{BE} electronics consist on the \ac{DAQ}, \ac{TPG}, \ac{DCS} and \ac{DSS}.
The first two form the so-called \ac{TDAQ}.
The \ac{DAQ} system includes two parts, where the most proeminent features the configuration, control and data acquisition system.
The second, smaller part holds the \ac{EMF} interface, which purpose is to propagate \ac{NZS} \ac{HGCAL} "regions of interest", as defined by extrapolated muon tracks in the muon detectors, aiming at reconstructing \ac{MIP} peaks for calibration without applying any threshold.

The larger part of the \ac{DAQ} consists on \num{6} identical hardware copies, following the three-fold polar angle symmetry of the two endcaps.
Each \SI{120}{\degree} sector hosts \num{16} =Serenity S= boards in two crates; these are called "DAQ boards" in the following.
The interface to the smaller \ac{EMF} system has one =Serenity S= board per endcap, called "NZS board".

# TPG
The role of the HGCAL TPG system is to build objects that are used as primitives by the central L1 trigger.
The \ac{TPG} system receives \ac{FE} data for every bunch crossing at \SI{40}{\mega\hertz}, and creates trigger “primitives” which are passed to the CMS central Level-1 Trigger (L1T) system to use in making the L1A trigger decisions. The
DAQ and TPG systems receive data from the FE electronics through separate links and so are
effectively independent systems as far as the BE electronics are concerned.

# ATCA boards and crates
The \ac{BE} across \ac{CMS} have been implemented on the \ac{ATCA} infrastructure [[cite:&atca_website]].
\Ac{ATCA} boards will be placed on \ac{ATCA} crates, which can house up to 14 boards

# DAQ
Besides being responsible for data acquistion, the \ac{DAQ} system has other responsibilities.
Being the entry point to the \ac{FE} electronics, the DAQ is also charged with synchronous fast control, asynchronous slow control, and transporting the timing information from \ac{CMS} to the \ac{FE}.

# lpGBT
The \ac{lpGBT} \ac{ASIC} [[cite:&lpgbt]] is the radiation-hard link driver standard to be used with \ac{HL-LHC} \ac{FE} electronics.
It provides a two-way connection between the \ac{BE} and \ac{FE}, including clock distribution, control and configuration signals, and transmission of the information collected by the detector.
The data is supplied to the \ac{lpGBT} inputs via up to \num{7} \acp{elink}, each providing 32-bit words at \SI{40}{\mega\hertz}, or \SI{1.28}{\giga\bit\per\second}.
This corresponds to \SI{8.96}{\giga\bit\per\second} data rates between the \ac{FE} and the \ac{BE}.
In \ac{HGCAL}, both the \ac{DAQ} and the \ac{TPG} communicate with the \ac{FE} via \acp{elink}, but only the \ac{DAQ} provides control and configuration.

# vtrx and firefly
The data is then forwarded to the \ac{BE} electronics, located at a distance of \SI{100}{\meter} from \ac{HGCAL}.
The \ac{VTRx+} [[cite:&vtrxp]] module povides an optical communication interface between the \acp{lpGBT} and the computing boards.
On top, CERN has developed a custom radiation resistant version of the easy-to-install =Firefly= optical transceivers [[cite:&versatile_link;&firefly]], optimised for use in the \ac{BE} electronics to communicate with the \ac{VTRx+} modules.

# timing and control distribution systems
The \ac{TCDS} interface is provided by one specialized \ac{ATCA} board per crate, named \ac{DTH400}.
The \ac{DTH400} also provides an interface to the \ac{cDAQ}, outputting \SI{\sim 400}{\giga\bit\per\second} via ethernet \ac{D2S} links.
The data is then transported to the surface into the memory of commercial servers.
That \ac{DTH400} boards receives the \ac{CMS}-wide clock and control signals and distributes it to the remaining boards in the crate.
The communication with the \ac{TCDS} is two-way: this notably gives the ability to request a \ac{BE} resynchronisation or reset, and to throttle the entire \ac{L1} system.
Communication between the \ac{BE} and the \ac{cDAQ} is established via \SI{25}{\giga\bit\per\second} fiber optics running the \ac{CMS}-wide =SLinkRocket= protocol, which ensures a strict specification based on \SI{128}{\bit} words [[cite:&hlttdr]].

# USC
The \ac{BE} electronics will be located in the \ac{CMS} \ac{USC}, where space is extremely limited and has to be distributed across all subdetectors.
The \ac{HGCAL} \ac{TDAQ} \ac{BE} has been assigned \num{8} racks, split into \num{3} and \num{5} to \ac{DAQ} and \ac{TPG}, respectively.
Each rack fits two \ac{ATCA} crates, and sufficent power is provided to the \ac{ATCA} boards.

#+NAME: fig:daq_system_overview
#+ATTR_LATEX: :width 1.\textwidth
#+CAPTION: Layout of Stage 1 and Stage 2 boards for one HGCAL endcap. The full TPG system consists of two identical and independent copies of this layout. Taken from [[cite:&hgcal_backend_tdaq]].
#+BEGIN_figure
[[~/org/PhD/Thesis/figures/hgcal/daq_system_overview.pdf]]
#+END_figure


#+NAME: fig:tcs_geometry
#+CAPTION: Illustration of the three-fold diamond configuration of an hexagonal \SI{8}{\inch} module, used to associate single \ac{Si} cells to \acp{TC}. Low density modules (left) associate four sensors to each trigger cell, while high density modules (right) create \acp{TC} with nine channels each. All modules have exactly \num{48} \acp{TC}, effectively removing a layer of complexity when processing \acp{TC}. The actual physical dimensions of the \acp{TC} vary given the boundaries of the hexagonal modules. 
#+BEGIN_figure
#+ATTR_LATEX: :width 1.\textwidth :center
[[~/org/PhD/Thesis/figures/hgcal/TCs_geometry.pdf]]
#+END_figure

* Frontend electronics

The \ac{HGCAL} \ac{L1} reconstruction chain, including the \ac{TPG} reconstruction, starts at the location where data is collected, namely the \ac{Si} cells and \ac{Sci} tiles described in [[#sec:hgcal_intro]].
From raw energy deposits to the creation of \acp{TP}, a complex chain of electronic components and data reduction and selection algorithms is defined.
The architecture surrounding the /on-detector/ steps, i.e., the steps taking place very close to where the raw data is collected, constitutes the \ac{FE} electronics.
The entry points of the reconstruction chain are the custom chips located on the hexaboards or tileboards, depending on the detector region.
They are called \acp{HGCROC} [[cite:&hgcroc;&hgcroc_paper]], and are \ac{HGCAL}-specific \acp{ASIC} which collect, amplify and filter the produced ionization or scintillation charged currents at \SI{\sim 300}{\tera\byte\per\second} [[cite:&hgcalTDR]].
The layout of a \ac{HGCROC} chip is shown in [[fig:hgcroc]].
The chips aggregate the data into \acp{TC}, in what constitutes the first of many data reduction algorithms in the \ac{TPG}.
Due to the similarity of the electronic and algorithms of the \ac{Si} and \ac{Sci} detector regions, and also taking into account differences in their development stage, we will focus on the \ac{Si} technology to simplify the overall description.
\Acp{TC} are defined as groups of neighbouring sensor cells, and are meant to reduce the prohibitive data throughput by grouping \num{4} or \num{9} channels into a single \ac{TC}, depending on the \ac{Si} module granularity.
Only \acp{TC} in odd-numbered layers are considered for further reduction.
\Acp{TC} also decrease the algorithms' complexity, in the sense that all modules have exactly \num{48} \acp{TC}, and thus differences arising from low- and high-granularities do not have to be taken into account.
In parallel, the full-granularity data is kept in circular buffers and is sent out via \SI{1.28}{\giga\bit\per\second} \ac{elinks} as soon as a \ac{L1A} signal arrives.
Despite the chip's ability to also measure the \ac{ToA} of the charged pulses, timing information cannot be exploited in the trigger path due to bandwidth constraints.
In addition to a standard \num{10} bit \ac{ADC} charge measuring mode, the \ac{HGCROC} switches to a \ac{ToT} mode as soon as a threshold on the deposited charge is reached, of the order of the preamplifier saturation threshold of \SI{\sim 100}{\femto\coulomb}.
The time during which the preamplifier is saturated serves a proxy for the amount of deposited charge.
During the saturation period, which can reach up to \SI{\sim 200}{\nano\second}, the chip is blind to new charge deposits.
Once the saturation is over, the time is digitized with a \num{12} bit \ac{TDC}.
Beyond the data paths, the chip also inlcludes a \ac{PLL}, which generates the clocks needed to operate the chip, and an \ac{I2C} interface.
The latter enables the modification of all static parameters of the chip, which are triplicated to prevent \acp{SEE}, which are stochastic, localised and non-cumulative effects disrupting the chip's functioning.




#+NAME: fig:hgcroc
#+CAPTION: Block diagram of the \ac{HGCROC} [[cite:&hgcroc;&hgcroc_paper]]. It is composed of two data paths: the \ac{DAQ} path (in blue), connected to the \ac{ECON-D}, and the trigger path (in green), connected to the \ac{ECON-T}. It also includes a \ac{PLL}, which generates the clocks needed to operate the chip, and an \ac{I2C} interface, which enables the modification of all static parameters of the chip. Taken from [[cite:&bruno_chep23]]. 
#+BEGIN_figure
#+ATTR_LATEX: :width 1.\textwidth :center
[[~/org/PhD/Thesis/figures/hgcal/HGCROC.pdf]]
#+END_figure

The ECON-T chip concentrates, selects and/or aggregates TCs within a single module (3 or 6 \acp{HGCROC}) and builds \textit{module sums}, where the energies of TCs in a module are summed without applying any threshold.
at \SI{90}{\tera\byte\per\second} [[cite:&econ]].
The data is then sent via \SI{1.28}{\giga\bit\per\second} e-links to lpGBT ASICs [[cite:&lpgbt]], serialized to \SI{10.24}{\giga\bit\per\second}, and sent via optical-links [[cite:&vtrxp]] to the off-detector \ac{BE}.







The "three-fold diamond" configuration was chosen because it allows the convenient definition of symmetric groups of neighbouring cells to form trigger primitives, as shown in ref:fig:tcs_geometry, and the subdivision of the module into symmetric domains for the readout chips, simplifying the layout of the module readout \ac{PCB} [[cite:&l1TDR]].


* Backend electronics
+ split between on detector electronics and outside FPGAs is also related tothe cost and limitation of optical links available [[cite:&zabi]]
  
* Reconstruction of Trigger Primitives

/why is the data throughput between Stage1 and Stage2 larger than the one between the ECON-T and Stage1/

The data throughput is larger because of the duplication needed to handle sector boundaries and because the data need to be inflated:
+ the addresses need to be encoded on a larger number of bits because the Stage 1 covers larger detector regions than the ECON-T
+ the energies need also to be encoded on a larger number of bits to absorb different energy scales used in different regions of the detector in the ECON-T
+ the numbers you are referring to are for the system using the Threshold data reduction I suppose (I don't think we have numbers for the current system). So in that case, since we have a fixed size format between S1 and S2 we need to allocate more bandwidth in order to absorb fluctuations and limit truncation in the S1. To be noted that something similar will also apply with BC even though its data is fixed size. This is due to the fact that BC provides a fixed number of TCs per module, but not a fixed number of TCs per bin. While the Stage 2 needs a fixed data size per bin. So we need to allocate more bandwidth per bin than necessary.
  
#+NAME: fig:l1chain
#+CAPTION: Simplified schematic of the dataflow of \acp{TP} in HGCAL, starting (ending) in the top left  (bottom left) corner. The diagram follows the \ac{TP} processing in a Si layer through the \ac{FE} and \ac{BE}, and up to the \ac{L1}, including expected approximate bandwidths. Trigger decisions at this stage will impact the \ac{HLT} and, consequently, physics analysis. Taken from [[cite:&bruno_chep23]].
#+BEGIN_figure
#+ATTR_LATEX: :width 1.\textwidth
[[~/org/PhD/Thesis/figures/hgcal/l1chain.pdf]]
#+END_figure

#+NAME: fig:stage2chain
#+CAPTION: Schematic flowchart of S2’s reconstruction chain. TCs from S1 are unpacked and processed in a pipelined fashion up to the creation of cluster-related variables, which are fed to L1. The description of the steps can be found in the text. Taken from [[cite:&bruno_chep23]].
#+BEGIN_figure
#+ATTR_LATEX: :width 1.02\textwidth :center
[[~/org/PhD/Thesis/figures/hgcal/stage2chain.pdf]]
#+END_figure

#+NAME: fig:econt_algorithms
#+CAPTION: Taken from [[cite:&bruno_chep23]]. 
#+BEGIN_figure
#+ATTR_LATEX: :width 1.\textwidth :center
[[~/org/PhD/Thesis/figures/hgcal/econt_algorithms.pdf]]
#+END_figure

In the \ac{FE}, trigger data processing is performed by \ac{HGCAL}'s dedicated read-out chips (\acp{HGCROC} [[cite:&hgcroc]]) at \SI{300}{\tera\byte\per\second}, and by \ac{ECON-T} chips at \SI{90}{\tera\byte\per\second} [[cite:&econ;&hgcalTDR]].
The \ac{HGCROC} reduces the prohibitive data throughput by grouping 4 or 9 channels into \acp{TC}, where each \ac{Si} module comprises 48 \acp{TC}.
Only \acp{TC} in odd-numbered layers are considered.
Timing information cannot be exploited in the trigger path due to bandwidth constraints.
The ECON-T concentrates, selects and/or aggregates TCs within a single module (3 or 6 \acp{HGCROC}) and builds \textit{module sums}, where the energies of TCs in a module are summed without applying any threshold.
The data is then sent via \SI{1.28}{\giga\bit\per\second} e-links to lpGBT ASICs [[cite:&lpgbt]], serialized to \SI{10.24}{\giga\bit\per\second}, and sent via optical-links [[cite:&vtrxp]] to the off-detector \ac{BE}.


My work is mostly concerned with the \ac{BE}, which is composed of two processing stages (\ac{S1} and \ac{S2}) running on Serenity boards [[cite:&serenity]] with 128-link Xilinx VU13P FPGAs.
Their assigned latency budget is \SI{\sim 2.5}{\micro\second}.
\acp{FPGA} in \ac{S1} cover \SI{\sim 2}{\percent} only of one endcap and, just like \ac{S2} boards, do not communicate with each other[fn::Handling boundaries thus requires data duplication.].
The \ac{S1} receives \ac{ECON-T} data, unpacks and calibrates it.
It then routes and sorts \acp{TC} in energy into projective \SI{2}{\azimuth{}} vs. \SI{42}{\rz} bins per \SI{120}{\degree} sector, where $\text{R}=(x^{2}+y^{2})^{1/2}$ in the plane perpendicular to the beamline and $\tan(\theta)=$ \si{\rz} (a constant \si{\rz} corresponds to a constant particle angle $\theta$).
The sorting uses batcher odd-even sorting networks [[cite:&sort_net2;&calorPortales;&sort_net]], where on-the-fly truncation reduces the total number of comparators required.
Modules sums are here partially summed into module towers, and time multiplexing [[cite:&zabi]] with a bunch-crossing period of 18 is applied before sending the data to \ac{S2}.
\ac{S2} accumulates partial tower energies into (\rapidity{},$\,$\azimuth{} ) bins and builds clusters from \acp{TC}:

+ *Histogramming*:
  TCs are mapped to a \coordsa{} space with (216, 42) bins.
  This further reduces spatial granularity and facilitates vectorized/parallel processing in the firmware due to its grid-like structure.
  Each bin contains the energy sum of all its \acp{TC}, together with their \tmip{}[fn:: \tmip{} is defined as $\text{mip}/\cos(\theta)$, where one mip stands for the energy deposited by a minimum ionizing particle [[cite:&PDG \S34.2.3]] .]-weighted ($x/z, y/z$) positions.

+ *Smoothing*:
  An energy smearing step is applied to \coordsa{} bins to decrease overall variations in their energy distribution.
  A kernel is applied, where to each bin's energy a fraction of the energy of its neighbors is added.
  The kernels are shown in \cref{eq:smooth_kernel}, along \azimuth{} (left) and \si{\rz} (right):

  #+NAME: eq:smooth_kernel
  \begin{equation}
      \left[
        \renewcommand*{\arraystretch}{1.0}
        \begin{array}{ccccccccccc}
          ...&\frac{1}{16}&\frac{1}{8}&\frac{1}{4}&\frac{1}{2}&1&\frac{1}{2}&\frac{1}{4}&\frac{1}{8}&\frac{1}{16}&...
        \end{array}
      \right]
      \hspace{2cm}
      \left[
        \renewcommand*{\arraystretch}{1.0}
        \begin{array}{c}
          \frac{1}{2} \\[.15cm]
          1 \\[.15cm]
          \frac{1}{2} \\
        \end{array}
      \right]
  \end{equation}

  Variations are more prominent along \azimuth{} since the binning is finer.
  The kernel along \azimuth{} is \si{\rz}-dependent, as illustrated by the dots in \cref{eq:smooth_kernel}.
  The \azimuth{} kernel collects the energy from more bins for lower \si{\rz} rows.
  The energy of each bin is normalized to ensure no energy is artificially added to the event.

+ *Seeding*:
  Seeds are local \tmip{} maxima in the histogram.
  They are found via a seeding window which, for each bin, spans its immediately adjacent bins and checks whether their \tmip{} energy is lower.
  If it is, and if its energy lies above a threshold, the bin is promoted to a seed.

+ *Clustering*:
  \acp{TC} are associated to seeds and used to calculate cluster properties.
  Every seed originates a cluster.
  Contrary to previous steps, the clustering uses a $(x/z,\,y/z)$ projective space.
  Two algorithms exist, one associating \acp{TC} to their closest seed (default), the other prioritizing association based on seed energy.

  JB: There are two energy interpretations computed, one for hadrons and one for EM showers, with possibly different radii. There is only one clustering radius but the energy interpretation can use a different radius smaller than the clustering radius.







During my PhD I have implemented from scratch the entire \ac{S2} reconstruction chain in a standalone =Python= code[fn:: \url{https://github.com/bfonta/bye_splits}]
It was previously only available in =C++=, within CMSSW [[cite:&cmssw]].
The code enables exponentially faster prototyping, testing and optimization, which are the basis of the following studies.

** Random
+ [[cite:&hlttdr]] (I wrote Section 12.3)
+ ECON-D does zero suppression
  

* sci
Not entirely well defined.
Trigger sum of 4 adjacent tiles

* Additional bibliography :noexport:
+ JB reference: https://cernbox.cern.ch/pdf-viewer/public/cLosQkewmONZakQ/220606_Dauncey_DN-19-032-V2.pdf?contextRouteName=files-public-link&contextRouteParams.driveAliasAndItem=public%2FcLosQkewmONZakQ&items-per-page=100
+ Mentin my proceedings [[cite:&bruno_chep23]]
  
