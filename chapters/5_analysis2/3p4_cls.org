:PROPERTIES:
:CUSTOM_ID: sec:cls
:END:

We can now use the Neyman-Pearson lemma to rewrite the optimal test statistic using the likelihood function.
To test a given value of the signal strenght modifier $\mu$, we define the /\ac{PRL}/ [[cite:&asimov]]:

#+NAME: eq:PRL
\begin{equation}
\lambda(\mu) \equiv \frac{L(\mu,\hat{\theta}_{\mu})}{L(\hat{\mu},\hat{\theta})} \: ,
\end{equation}

\noindent where $\hat{\mu}$ and $\hat{\theta}$ in the denominator are the maximum likelihood estimators of $\mu$ and $\theta$, respectively, and $\hat{\theta}_{\mu}$ denotes the value of $\theta$ that maximizes the likelihood for a given $\mu$.
In other words, the denominator is the unconditional maximized likelihood function, obtained when the minimization is performed simultaneously on $\mu$ and $\theta$, while the numerator is conditioned on the value of $\mu$ being probed.
$\lambda(\mu)$ is by construction bounded between 0 and 1, and higher values correspond to better compatibilities between the data and the probed $\mu$.
The profile likelihood ratio satisfies the conditions required by /Wilks' theorem/, which states that, given a large dataset and assuming some regularity conditions on the likelihood, $-2 \ln \lambda(\mu)$ is asymptotically distributed according to a $\chi^{2}$ distribution with $k$ degrees of freedom, where $k$ is given by the number of \acp{POI} [[cite:&Wilks:1938dza]].
Importantly, the square root of the $\chi^{2}$ distribution at its minimum provides an approximate estimate of the significance, avoiding the generation of large quantities of pseudo-data to simulate $\lambda(\mu)$.
To give an idea, a \SI{5}{\sigma} p-value represents a \num{\sim e7} probability, thus requiring \num{\sim e9} samples for a precise determination.

In order to quantify an excess, the following test statistic is defined:
#+NAME: eq:excess_quantify
\begin{equation}
q_0 \equiv \left\{
        \begin{array}{ll}
        -2\ln{\lambda(0)} & \mbox{if } \hat{\mu}\geq0 \\
                0 & \mbox{if } \hat{\mu}<0  \:\: .
        \end{array}
      \right.
\end{equation}
  
\noindent Instead, in the case of the \xhhbbtt{} analysis, we are looking for a signal that has never been observed.
In such cases, /upper limits/ are instead quoted, since an excess is not expected.
When setting upper limits on the strength parameter $\mu$, the following test statistic $q_{\mu}$ is used:
#+NAME: eq:upper_limits
\begin{equation}
q_{\mu} \equiv \left\{
	\begin{array}{ll}
	  -2\ln{\lambda(\mu)} & \mbox{if } \hat{\mu}\leq\mu \\
	  0 & \mbox{if } \hat{\mu}>\mu \:\: .
	\end{array}
\right.
\end{equation}

\noindent Due to the negative sign, larger values of $q_{\mu}$ indicate that the probability for observing the data given $\mu$ is smaller, \ie{} the data and parameters become increasingly incompatible.
The test statistic is set to zero for $\hat{\mu}>\mu$ because, in the specific case of an upper limit, that situation would not be interpreted as a rejection of the null hypothesis, as we would be probing a value of $\mu$ that lies closer to the background-only hypothesis than the maximum likelihood estimator $\hat{\mu}$.

The test statistic in [[eq:upper_limits]] can be exploited to define exclusion limits via the modified frequentist confidence level criterion, or "$\text{CL}_{\text{s}}$ method" [[cite:&cls1;&cls2]].
Given $q_{\mu}^{\text{obs.}}$, we calculate the probabilities for $q_{\mu}$ to be equal or larger under the null or alternative hypotheses:
#+NAME: eq:pvalue_like
\begin{align}
  p_{s+b} &= P(q_{\mu} \geq q_{\mu}^{\text{obs.}} | \, s + b) = \int_{q_{\mu}^{\text{obs.}}}^{\infty} f(q_{\mu} | \, s+b) \, dq_{\mu} \: , \nonumber \\
  p_{b} &= P(q_{\mu} \geq q_{\mu}^{\text{obs.}} | \, b) = \int_{q_{\mu}^{\text{obs.}}}^{\infty} f(q_{\mu} | \, b) \, dq_{\mu} \: .
\end{align}

\noindent A given signal strength $\mu$ is said to be excluded at a confidence level $\text{CL} = 1 - \alpha$ if:
#+NAME: eq:cls
\begin{equation}
    \text{CL}_{\text{s}}(\mu) \equiv \frac{p_{s+b}}{p_b} < \alpha \: ,
\end{equation}
\noindent where $\alpha$ is the significance level.
The inclusion of the denominator protects against cases where $s \ll b$, for which signal models can be excluded when there is no sensitivity due to underfluctuations of the background.
A value of 5% is commonly chosen for the significance level, leading to the 95% \ac{CL} results quoted in this work.
Would the experiment be repeated a large number of times, we would expect to observe $\mu$ within the \ac{CL} in 95% of cases.
The limits are obtained on the \ac{POI} $\mu$, but are rescaled to the cross section of the signal process.

When looking for resonances as a function of multiple hypothesis, as done for \xhhbbtt{} in terms of the resonance masses $\mx$, the p-value underestimates the chances of observing fluctuations when jointly considering all signals being probed.
In other words, the /local/ significance, as computed for a fixed value of a parameter, systematically overestimates the more correct, /global/ significance, in a phenomenon dubbed the /look-elsewhere effect/.
This effect takes place because $q$ depends on several hypotheses $m = (m_1,m_2,m_3,...)$, and not just on a single $m_i$, and thus one has to take into account background fluctuations at any hypothesis value in the relevant domain.
The local p-value is thus converted into a global one through a new test statistic, computed by taking the maximum value of test statistics over the hypotheses' domain [[cite:&stat_procedure_comb_higgs]]:
#+NAME: eq:global_pvalue
\begin{equation}
q(\mu) = \max_{i} q(\mu; m_i) \: .
\end{equation}
\noindent We remark that the effect becomes stronger for worse detector resolutions, since it becomes more likely to observe multiple mass shifts accumulating at the same point, thus mimicking an excess over the background [[cite:&lista]].

* Extra :noexport:
+ discuss the flip-flop, or when to quote a measurement or a limit
