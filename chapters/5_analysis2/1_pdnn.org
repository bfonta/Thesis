:PROPERTIES:
:CUSTOM_ID: sec:pdnn
:END:

The adoption of \ac{DL} in the \xhhbbtt{} analysis naturally arises from the wish to extract the best possible limits with current technology.
The latest nonresonant \hhbbtt{} analysis [[cite:&higgs_bbtautau_nonres]] has already exploited such techniques, following the approach introduced in \newcite{prospects_hh_hllhc_2018}, achieving significant improvements with respect to previous publications.
However, while in the nonresonant analysis one wished to extract the \ac{SM} HH signal alone, in the resonant analysis here presented such discrimination must be done for each signal sample, associated to a given mass and spin hypothesis.
In order to create an optimal discriminant for N signal samples, one could define N separate discriminants, and optimize all of them separately.
This approach is however highly impractical with $\text{N} = 50$, as in our case, leading to an unsustainable complexity in data processing, and to prohibitively large resource needs.
Instead, using a single network is more convenient, encodes all the information for different hypotheses in the same location and, if needed, enables the properties of under-populated signal samples to be extrapolated from other samples.

A \ac{pDNN} [[cite:&parameterised_nn]] is exploited in this Thesis, where the parameters refer to the hypotheses being tested, namely the mass and spin values.
Parameterized learning has already been tested with the \hhbbtt{} topology, and was found to be useful in \newcite{angelas_thesis}.
Additionally, we have observed that the \ac{pDNN} here developed is more performant than a non-parameterized version with similar architecture and size.
Despite the performance improvement, the parameterized approach introduces some complexities.
During the training of the model, covered in [[#sec:training]], the signal events are passed to the network together with their mass and spin values.
For the background, the mass and spin are randomly sampled from all hypothesis being tested.
During inference, for data and background, the mass and spin features are fixed to one combination, such that the network's scores are given for one specific hypothesis.
This means that $\text{N}$ predictions must be generated per event.
[[tab:training_inference]] summarizes the approach.

#+NAME: tab:training_inference
#+CAPTION: Treatment of resonance mass and spin values during training and inference with the \ac{pDNN}. $\text{N}=50$ stands for the number of signal hypotheses, the combination of 25 mass values and 2 spin values. $m$ and $s$ refer to specific mass and spin values, respectively.
\begin{table}[htbp]
\centering
\setlength{\tabcolsep}{10pt}
\begin{tabular}{ccc}
    \hline \\[-1em]
     & \textbf{Sample Type} & \textbf{Description} \\ [+0.3em]\hline \\[-.8em]

    \multirow{2}{*}{\textbf{Training}} & Background & Randomly sample $m$ and $s$ from all possible values. \\[+0.3em]
                                       & Signal     & Use the $m$ and $s$ the signal sample was generated with. \\ [+0.3em]\hline \\[-.8em]

    \multirow{4}{*}{\textbf{Inference}} & Data  & Generate a prediction for each event $\text{N}$ times. \\[+0.3em]
                                        & Background & Generate a prediction for each event $\text{N}$ times. \\[+0.3em]

                                        & \multirow{2}{*}{Signal} & Generate a prediction using the $m$ and $s$ \\
                                        &                         &  the signal was made with. \\ [+0.3em]\hline \\[-1em]
\end{tabular}
\end{table}

The parameterized approach also introduces some challenges in the determination of systematic uncertainties.
Given that $\text{N} = 50$, and that each of those predictions has an independent set of associated systematic uncertainties, the total number of systematic uncertainties to be considered increases enormously.
For each variation affecting the input features of the \ac{pDNN}, a corresponding systematic uncertainty must be computed for all 50 combinations.
A significant part of this Thesis was devoted to process, distribute and store all the data necessary to correctly compute systematic uncertainties within a parameterized approach.

* Architecture
:PROPERTIES:
:CUSTOM_ID: sec:architecture
:END:

The network's architecture is illustrated in [[fig:combined_model]].
It has a densely connected structure [[cite:&huang2018densely]], prepended by a \ac{LBN} [[cite:&Erdmann_2019]] and the regression head of the network described in [[#sec:tautau_regression]].
The total set of input features is split into the subset of features for the $\mtautau$ regression network and the subset of features fed into the \ac{LBN}, which receives all four-momenta of the two reconstructed leptons and of the two reconstructed b-jets in the event.
Depending on the resolved or boosted topology of an event, default values are assigned to some input nodes.
The outputs of these two networks are concatenated in an intermediate layer and fed into a densely connected \ac{DNN}, which consists of 8 hidden layers, each containing 128 nodes.
This architecture has shown to smooth out the surface of the loss function [[cite:&li2018visualizing]], helping the training procedure when using large networks [[cite:&he2015deep]].
A dropout [[cite:&dropout]] of 5% is considered.
Batch-normalization layers are added after the concatenation of the two heads and before every dense layer [[cite:&batchnorm]].
Swish-1 activation functions are used for hidden layers [[cite:&swish]].
For both the $\mtautau$ regression network and the LBN, categorical features are embedded using entity embeddings [[cite:&cat_embed]].
At the end of the network, each event is categorized according to one of three possible classes: resonant \ac{ggF} \bbtt{} signal, \ac{DY} background or $\ttbar$ background.
For simplicity, we are currently exploiting the signal score only.

#+NAME: fig:combined_model
#+CAPTION: Architecture of the \xhhbbtt{} final discriminant. The layout corresponds to a combination of the $\mtautau$ regression network, the \ac{LBN} and a densely connected \ac{DNN}. The weights of the regression network are left initialize fixed, and are slowly activated as training time passes, which we refer to as "fade-in". "BN" stands for batch-normalization, and "ACT" stands for activation function. Courtesy of Marcel Rieger.
#+BEGIN_figure
\centering
#+ATTR_LATEX: :width 1.\textwidth :center
[[~/org/PhD/Thesis/figures/analysis2/combined_model.pdf]]
#+END_figure

* Training
:PROPERTIES:
:CUSTOM_ID: sec:training
:END:

In our analysis, in order to avoid overfitting, the available samples are divided into five folds, using the same cross-validation technique that was described in [[#sec:tautau_regression]].
Each \ac{DNN} is initialized with an independent "He-Uniform" weight-initialization scheme [[cite:&he2015weights]]. 
After the training, the predictions are averaged out across all discriminants.

The training data consist of all background resonant signal samples, for all 4 data-taking periods.
Three classes of input features are considered:
+ *continuous*: all continuous features listed in [[tab:input_features]], already used by the $\mtautau$ regression model, and additional inputs to the \ac{LBN}, namely the $\tau\tau$ and bb four-momenta;
+ *categorical*: decay channel, the charges of the reconstructed \taus{}, the data-taking period, and a boolean value standing for whether an AK8 jet is present in the event;
+ *parameterized*: the mass and spin values representing the hypotheses being tested.
        
# training loop
The data is processed in /batches/, each with a size of 4096 events.
The =Adam= optimizer [[cite:&adam]] is considered, starting with a learning rate of 0.003.
As the training progresses, the learning rate is reduced so that the minimum of the loss function is not overshoot.
A dynamic learning rate scheduler steadily decreases the learning rate based on the validation loss.
The training is stopped as soon as 10 epochs have passed without the improvement of the validation loss.

Since the network utilizes the weights of the $\mtautau$ regression head that were already optimized, care must be taken during the final discriminant optimization to avoid hurting the performance.
For this reason, the weights of the $\mtautau$ regression network are initially kept fixed, and are introduced only after 150 training steps.
Additionally, the connection is established through a /fade-in/ layer.
The layer slowly introduces the output of the $\mtautau$ regression network to the densely connected \ac{DNN}, such that the weights are not changed too abruptly.
This is achieved by multiplying the outputs of the connection between the two networks by a factor which is linearly increased over 20 epochs.

Finally, the signal, \ac{DY} and $\ttbar$ classes are represented in equal fractions within each batch.
This is required to prevent the discriminant to ignore signal events, given the multiplicity disproportion between signal and background.
Within each fraction, the $\ttbar$ and \ac{DY} samples are distributed according to \ac{MC} weights, to reflect the true background source composition.

* DNN Score Distributions
The distributions can be seen in [[fig:pdnn_distributions]], where a signal sample is also included for reference.
The cross section of the signal is arbitrary, and in the plot it is scaled for visualization purposes to the final upper limits, shown in [[#sec:final_limits]], multiplied by the \bbtt{} \ac{BR}.
Some data points are shown to assess compatibility.
The chosen region has a \ac{DNN} score below 0.8, to avoid biasing the results by looking at the data in signal-rich regions.

Occasionally, the \ac{QCD} background contribution is so small that statistical fluctuations with the ABCD method can lead to a negative bin content.
In such cases, the \ac{QCD} contribution is removed from the histograms.

The binning follows a "flat-signal" approach, where the number of signal events per bin is, as much as possible, required to be the same, and certain requirements are imposed on the number N of background events in each bin:
+ $\text{N}_{\text{DY}}>1$;
+ $\text{N}_{\ttbar}>1$;
+ $\text{N}_{\text{DY}}+\text{N}_{\text{DY}}>4$;
\noindent where the goal is to avoid bins with too little background, while ensuring that the two most dominant background sources are always present.
For simplicity, the bin with the lowest \ac{DNN} score is left with less signal events, due to the termination conditions of the binning algorithm.
It does not affect the final results, since that bin provides no sensitivity.
  
#+NAME: fig:pdnn_distributions
#+CAPTION: Distribution of the \ac{pDNN} score for the \eletau{} (top row), \mutau{} (middle row) and \tautau{} (bottom row) channels, in 2018. The three analysis categories are also shown, namely \rescat{1} (left column), \rescat{2} (middle column) and \boostcat{} (right column). The signal distribution is mostly flat, and is scaled for visualization to the product of the \bbtt{} \ac{BR} with the expected limits shown in [[#sec:final_limits]]. The plots are partially unblinded, for all background dominated bins lying below 0.8. Details are provided in the text.
#+BEGIN_figure
\centering
#+ATTR_LATEX: :width .325\textwidth :center
[[~/org/PhD/Thesis/figures/analysis2/dnn/shapes_cat_2018_etau_resolved1b_noak8_os_iso_spin_0_mass_1000.pdf]]
#+ATTR_LATEX: :width .325\textwidth :center
[[~/org/PhD/Thesis/figures/analysis2/dnn/shapes_cat_2018_etau_resolved2b_first_os_iso_spin_0_mass_1000.pdf]]
#+ATTR_LATEX: :width .325\textwidth :center
[[~/org/PhD/Thesis/figures/analysis2/dnn/shapes_cat_2018_etau_boosted_notres2b_os_iso_spin_0_mass_1000.pdf]]
#+ATTR_LATEX: :width .325\textwidth :center
[[~/org/PhD/Thesis/figures/analysis2/dnn/shapes_cat_2018_mutau_resolved1b_noak8_os_iso_spin_0_mass_1000.pdf]]
#+ATTR_LATEX: :width .325\textwidth :center
[[~/org/PhD/Thesis/figures/analysis2/dnn/shapes_cat_2018_mutau_resolved2b_first_os_iso_spin_0_mass_1000.pdf]]
#+ATTR_LATEX: :width .325\textwidth :center
[[~/org/PhD/Thesis/figures/analysis2/dnn/shapes_cat_2018_mutau_boosted_notres2b_os_iso_spin_0_mass_1000.pdf]]
#+ATTR_LATEX: :width .325\textwidth :center
[[~/org/PhD/Thesis/figures/analysis2/dnn/shapes_cat_2018_tautau_resolved1b_noak8_os_iso_spin_0_mass_1000.pdf]]
#+ATTR_LATEX: :width .325\textwidth :center
[[~/org/PhD/Thesis/figures/analysis2/dnn/shapes_cat_2018_tautau_resolved2b_first_os_iso_spin_0_mass_1000.pdf]]
#+ATTR_LATEX: :width .325\textwidth :center
[[~/org/PhD/Thesis/figures/analysis2/dnn/shapes_cat_2018_tautau_boosted_notres2b_os_iso_spin_0_mass_1000.pdf]]
#+END_figure

* Biblio :noexport:
+ [[https://res-hbt-dnn-outputs.web.cern.ch/prod8/][dnn plots]]
