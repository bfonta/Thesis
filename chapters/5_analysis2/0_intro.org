:PROPERTIES:
:CUSTOM_ID: sec:analysis2_intro
:END:

We have so far described in detail what in simple words amounts to a complex data processing step.
Techniques that have been repeated and refined over the years were covered, including selections, estimates and related validations, with one goal only: increase the analysis sensitivity.
The material presented in this Chapter finally substantiates all efforts into a quantifiable measurement.
We want to be able to state whether the observed data matches the backgrounds, and thus the \ac{SM}, or if it can alternatively accommodate one or more signal excesses.
To draw such conclusion, we make use of a /discriminating variable/, sensitive to the underlying probability distributions of data and backgrounds, and which optimally encodes the analysis information so far discussed.
The higher the discrimination power of such observable is, the more sensitive the analysis becomes, equating to measurements of smaller and smaller \ac{BSM} cross sections.
Conversely, would such observable be poorly defined, the analysis would not be able to detect \ac{NP}, even if abundant.
Therefore, the design of a discriminating variable is of dramatic importance, representing the nexus of all analysis decisions so far taken.

Historically, \ac{HEP} discriminants were built from kinematical and topological variables related to the events, as these are expected to encode the most information regarding the processes under study[fn:: Matrix element discriminants are occasionally employed, as they have been shown to provide the maximal information for the description of the underlying physics processes. They can only be successfully applied for "clean" backgrounds, without jets, for instance. The approach is applied via the MELA package [[cite:&mela1;&mela2;&mela3;&mela4;&mela5]].].
Similarly, for many resonant analysis, as was the case for the analysis here reported, the combined invariant mass of the final state particles, or a modified version thereof, was usually considered.
Such a choice naturally stems from the "bumpy" signature expected, \ie{} the presence of a localized excess over a broadly distributed background.

In recent years, a paradigm shift took place in \ac{HEP}: machine learning techniques, and in particular \ac{DL}, became widespread, due to their exponential improvement in classification and regression problems.In 
Despite being based on established principles [[cite:&perceptron;&Linnainmaa;&backprop]], \acp{DNN} rose to prominence only about 15 years ago, but are today used in most industries, companies and academic institutions around the world, for tasks as varied as language generation, visual recognition, machine translation and fraud detection, among many, many others.
The main reasons that lead to such a technological shift were the increased computational power, the introduction of new high-performant \ac{NN} architectures, and crucially the vast amounts of available data, which lie at the center of most \ac{DL} techniques.
\Ac{HEP} has accompanied the trend, exploiting such algorithms to improve the analyses' performances.
Even triggers have been included in the process, achieving improvements that surpass all expectations.

The results here presented are shown in the form of expected 95% \ac{CL} upper limits.
Observed upper limits will be computed once the \ac{CMS} internal analysis review process is completed, which should happen by early 2025 the latest, in preparation for the 2025 Moriond conference.
Until then, analyzers do not have permission to show real data in the final discriminant's distributions, or to consider real data when extracting limits.

In [[#sec:pdnn]], we start by explaining how \ac{DL} techniques are leveraged to define a robust discriminating variable.
We then proceed, in [[#sec:systematics]], with a description of all systematic uncertainties taken into account, arising from variations to the inputs of the \ac{DL} procedure, and also due to shifts to the several \ac{MC} correction weights considered in our analysis.
The quantification of the differences between two alternative scenarios is the task of the statistical treatment presented in [[#sec:stats_intro]].
Specifically, we formulate the hypothesis that the data is not described by the \ac{SM}, and verify if the measurements agree with such a claim.
The 95% \ac{CL} expected upper limits are presented in [[#sec:results_intro]].
