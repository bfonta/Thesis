+ Andrew: mention we consider the assymptotic limit and cite "the paper everybody cites"
  

In statistics, the term /hypothesis/ indicates a given set of predicted probabilities, against which one compares observed data.
The hypothesis generally assumed as true is traditionally called the /null hypothesis/, or $H_{0}$, while the hypothesis we are comparing with the null are dubbed /alternative/, or $H_{1}$, $H_{2}$, $H_{3}$, ...
Hypothesis denote \acp{PDF} $f$ which depend on the data measured $x = (x_1,x_2,x_3,...)$ and eventually on free parameters $\theta = (\theta_1,\theta_2,\theta_3,...)$ which are in turn estimated from data.
To measure the agreement between a given hypothesis and the data being measured, one constructs a function of the variables being measured, called /test statistic/, or $t(x)$.
Each hypothesis implies a different \ac{PDF} for the test statistic, denoted as $f(t|H_0)$, $f(t|H_1)$, $f(t|H_2)$, ...
One usually aims at constructing the simplest test statistic enabling the largest discrimination possible between hypothesis being compared.
In order for accepting or rejecting a given $H_0$, one has to define a test statistic /cut/ $t_{\text{cut}}$ establishing an acceptance and a rejection region.
The decision is then made by comparing the observed value of $t(x)$ with the agreed $t_{\text{cut}}$.
When

#+NAME: eq:significance_level
\begin{equation}
\alpha = \int_{t_{cut}}^{\infty} g(t|H_{0})dt
\end{equation}

#+NAME: eq:inverse_power
\begin{equation}
\beta = \int_{-\infty}^{t_{cut}} g(t|H_{1})dt
\end{equation}




[[cite:&glen_cowan]]




* Additional bibliography :noexport:
+ 
