:PROPERTIES:
:CUSTOM_ID: sec:tautau_regression
:END:

A sizable fraction of the energy of the \htt{} system is lost due to the neutrinos resulting from the $\tau$ decays.
These are not measured and only show up in the detector as missing transverse energy.
Reconstructing the complete \htt{} system, however, is important for two reasons: 
+ Separating signal events from background events by reconstructing the mass of the \htt{} system, which already proved to be an important variable (using the =SVfit= algorithm) for the \ac{DNN} discriminator in \newcite{higgs_bbtautau_nonres};
+ Reconstructing the mass of the heavy resonance (the HH system) at the correct scale for which a good estimation of the momentum of the \htt{} system is needed.

  
+ introduce svfit:
  the invariant mass of the $\tau\tau$ pair is reconstructed using the SVfit algorithm [[cite:&svfit]] that is based on a likelihood function which quantifies the level of compatibility between a Higgs mass hypothesis and the measured momenta of the visible $\tau$ decay products plus the missing transverse energy reconstructed in the event.
  The SVfit algorithm improves the invariant mass reconstruction so that it is closer to 125 GeV for \htt{} events, when compared to the simple visible mass of the taus (or even the visible mass plus the missing transverse energy), therefore allowing a better identification of the signal events.
+ \Ac{SVFit} is computationally expensive and has problems reconstructing the Higgs at low masses

A neural network was trained with the target of reconstructing the momenta of the neutrinos.
In addition to the regression output (the neutrino momenta) a second "head" simultaneously classifies the events into signal and different background processes in order to improve the regression results during training.

# Samples
For each of the four data-taking periods, the following samples are used in the training:
+ \ac{ggF} \spin{0} and \spin{2} signals in the mass range of \SI{250}{\GeV} to \SI{3}{\TeV}
+ \ac{DY} samples binned in the $\pt$ of the Z;
+ $\ttbar$: fully-hadronic, semi-leptonic and fully-leptonic;
+ \tth{} (\htt{}).
\noindent The background choices cover the two dominant background sources, namely \ac{DY} and $\ttbar$.
The \tth{} process is also considered, since it is expected to have the same final state objects as the signal, with a peaking structure.

# Input features
The used input features can be continuous or categorical, and are described in [[tab:input_features]].
The continuous features are passed through a normalization layer ensuring zero mean and unitary standard deviation.
The categorical features are encoded in integers, deprived of physical meaning, which are passed through an embedding layer, with output dimension 10, before being concatenated with the continuous features.
Some categorical features are occasionally set to a default value depending if the topology of the event is resolved or boosted.
categorical values are set to 0 if no pair of AK4 jets or no AK8 is present) and two variants of the HH system (the sum of the \ditau{} and di-b systems and the sum of the di-$\tau$ system and the AK8 jet, again setting the respective values to 0 if no pair of AK4 jets or no AK8 jet is present)
The concatenation is used as input to the first layer of the network.

#+NAME: tab:input_features
#+CAPTION: Lists of continuous and categorical input variables used by the \ac{DNN}. The items marked with a $\dagger$ correspond to vectors that were rotated in the transverse plane with respect to the visible \ditau{} system.
#+ATTR_LATEX: :placement [!h] :center t :align cl :environment mytablewiderrows
|-------------+-------------------------------------------------------------------------|
| *Type*        | *Variable*                                                                |
|-------------+-------------------------------------------------------------------------|
| *Continuous*  | \texttt{DeepJetCvsB}: Charm versus bottom \texttt{DeepJet} score        |
|             | \texttt{DeepJetCvL}: Charm versus light flavour \texttt{DeepJet} score  |
|             | \texttt{DeepJet}: b-tagging score                                       |
|             | $\mtautau$: Invariant mass of the \htt{} candidate                      |
|             | $\mbb$: Invariant mass of the \hbb{} candidate                          |
|             | \hhbtag{}: Custom b-jet score                                           |
|             | $\text{MET}^{\dagger}$: $x$ and $y$ projections                               |
|             | $\text{Cov}(\text{MET})$: \ac{MET} covariance matrix                    |
|             | $\text{4-momenta}^{\dagger}$: 4-momenta of \taus{}, b jets, $\tau\tau$ and $\bbbar$ |
|-------------+-------------------------------------------------------------------------|
| *Categorical* | $\tau$ decay channel: \eletau{}, \mutau{}, \tautau{}                       |
|             | $\tau$ decay mode                                                          |
|             | Presence of two AK4 jet                                                 |
|             | Presence of one AK8 jet                                                 |
|             | Data-taking period                                                      |
|             | Spin hypothesis                                                         |
|             | Mass hypothesis                                                         |
|-------------+-------------------------------------------------------------------------|

# Network and training
#+NAME: fig:network
#+CAPTION: Visual representation of the $\mtautau$ regression network with its two "heads", for regression and classification. The regression estimates the three-momenta $\vec{p} = (p_x, p_y, p_z)$ of the neutrinos $\nu_1$ and $\nu_2$ produced in the \htt{} decay. Depending on the $\tau$ decay channel, the $\nu$ notation can refer to one or two neutrinos. The classification assigns every event a score, representing the probability to belong to one of four classes. Details are provided in the text. Courtesy from Tobias Kramer.
#+BEGIN_figure
\centering
#+ATTR_LATEX: :width .8\textwidth :center
[[~/org/PhD/Thesis/figures/analysis1/tautau_network.pdf]]
#+END_figure

The network consists of a common part with 5 dense layers and two "heads".
One of the heads is used for regressing the neutrino momenta, and the other for an additional classification, with 4 dense layers each.
Each layer has 128 nodes and uses the \ac{ELU} activation function [[cite:&elu]].
After each layer, batch normalization [[cite:&batchnorm]] is applied.
The regression head has 6 outputs, corresponding to the three-momenta of the neutrinos from the two $\tau$ legs.
For leptonic $\tau$ decays, the two neutrino momenta are summed.
The classification head has 4 outputs.
The network architecture is shown in [[tab:network]] and [[fig:network]].

The loss function includes a mean squared error term for the regression head and a cross-entropy term for the classification head.
Both terms are added.
On top, a regularization term is added, including the sum of the L2 norm $\sqrt{\sum^n_i|x_i|^2}$ of all the $n$ weights $i$ in the network.
This term is multiplied by a factor which depends on $n$, to avoid a dependency on network's size.
The factor is chosen such that the regularization term makes up \SI{\sim 20}{\percent} of the total loss.

The events are split into 5 /folds/ based on their event numbers. @remove same description from signal extraction@
The network is trained 5 times, so that each time one of the folds is held out of the training process.
That fold can then be used to evaluate the network without any bias, since the network has not seen the events in the training.
The training uses a batch size of 4096 and the =AdamW= optimizer [[cite:&adamw]] with an initial learning rate of \num{0.003}.
If the validation loss has not improved within 10 epochs, the learning rate is halved.
Once no effect on the validation loss is observed, this procedure is stopped.
As soon as the validation loss has not improved within 15 epochs, the training ends.

Event weights should be considered to account for different cross sections, selection efficiencies and other differences between individual processes.
Instead of applying such weights directly in the calculation of the batch's loss, the batch composition iteself is chosen based on these weights.
As a first step, each of the 4 sample classes makes up 1/4 of the events of a batch.
This is done to prevent the samples with large \ac{MC} statistics, such as \ac{DY} and $\ttbar$, from completely dominating the batch compared to samples with less number of events, such as the signal.
Within the sample classes, two different approaches are chosen: within the signal sample class, each mass/spin hypothesis counts the same, whereas in the background sample classes, the events are distributed based on the event weights.
The network is implemented using the =Keras= library [[cite:&keras]] with a =Tensorflow= backend [[cite:&tensorflow]].

# For background samples, the spin hypothesis is randomly chosen from the 2 possible hypotheses.

#+NAME: tab:network
#+CAPTION: Overview of the architecture of the $\mtautau$ regression network. A batch size of 4096 is employed. \Ac{ELU} activations are used throughout. Batch normalization is used after each layer.
#+ATTR_LATEX: :placement [!h] :center t :align ccccc :environment mytablewiderrows
|---------------------+--------+-------------+-------------------+-------------------------------------|
| *\ac{DNN} section*    | *Layers* | *Nodes/layer* | *Loss function*     | *Number of Outputs*                   |
|---------------------+--------+-------------+-------------------+-------------------------------------|
| Common block        |      5 |         128 | --                | --                                  |
| Regression head     |      4 |         128 | Mean Square Error | 6: $\pt$ of 2 $\nu$'s                 |
| Classification head |      4 |         128 | Cross-Entropy     | 4: HH, \ac{DY}, $\ttbar$ and \tth{} |
|---------------------+--------+-------------+-------------------+-------------------------------------|

The obtained regression reveals performance significantly superior to what \ac{SVFit} could ever achieve.
@some numerical comparison looking at old slides from Tobias@
In the left plot of [[fig:reg_tautau]] we show the result of the $\mtautau$ regression on the relevant backgrounds and on the signal.
The \ac{DY} and signal peaks can be fully disentangles, which was not the case with \ac{SVFit}.
This is achieved with minor $\ttbar$ backgrond sculpting.
In the right plot of the same figure, we can see the result of the regression on the full HH invariant mass, for various resonance masses.
The peaks are much narrower than previous results.
We notice that the method here described has the potential to be applied to other decay topologies, given appropriate training data and input features.

#+NAME: fig:reg_tautau
#+CAPTION: Distributions after running inference with the trained $\mtautau$ network. (Left) Regressed $\mtautau$, for dominant background sources, peaking \tth{} and HH signal. (Right) Regressed $\mhh$ for signal samples with varied resonance masses. Courtesy from Tobias Kramer.
#+BEGIN_figure
\centering
#+ATTR_LATEX: :width .49\textwidth :center
[[~/org/PhD/Thesis/figures/analysis1/reg_H_m.pdf]]
#+ATTR_LATEX: :width .49\textwidth :center
[[~/org/PhD/Thesis/figures/analysis1/reg_HH_m_signals.pdf]]
#+END_figure

* Biblio :noexport:
+ [[https://indico.cern.ch/event/1319569/contributions/5627455/attachments/2736470/4758946/HLepRare18Oct23.pdf][Tobias' slides]]
+ which H->\tau\tau leg is considered in the right plot?
