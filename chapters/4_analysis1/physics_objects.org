:PROPERTIES:
:CUSTOM_ID: sec:physics_objects
:END:

In this Section, we define the physics objects and related quantities which are used for event selection in [[#sec:selection]].
We focus on the objects used by the analysis, which happen to cover a wide variety: electrons, muons, tau leptons and \ac{MET} for the \htt{} decay, and jets for the \hbb{} decay.
Within the \ac{CMS} Collaboration, dedicated teams called \acp{POG} are responsible for the definition, calibration and validation of specific objects.
As such, some selection and calibration procedures are often common to a broad range of physics analyses.
In the context of our analysis, we had interactions with the EGamma, Tau, Muon, BTV (b-tagging and vertexing) and JetMET, including during review steps internal to \ac{CMS}.
We follow \ac{POG} recommendations whenever possible.

* Muons
The muon selections applied in this work were originally based on the selections used by the \ac{CMS} \htt{} analyses [[cite:&higgs_tautau1;&higgs_tautau2;&higgs_tautau3]], but have evolved according to updated analysis needs, further corrections, and sample updates.
Muons are required to be reconstructed by the Tracker or the Global muon reconstruction algorithms, which were described in [[#sec:offline_reco_muons]].
The muon selections later presented in [[tab:max_min_cuts]] apply kinematic, isolation and \ac{ID} thresholds.
In the following we define both isolation and \ac{ID}.

# isolation
One of the most efficient ways to reject backgrounds for lepton candidates is the usage of isolation quantities, a generic class of discriminating variables that are constructed
from the \ac{pt} sum of \ac{PF} particles inside a cone around the lepton, relative to its \ac{pt}.
#+NAME: eq:pf_iso
\begin{equation}
\mathcal{I}_{\text{rel}}^{\ell} = \frac{ \sum  p_{T}^\mathrm{charged} + \max \left[ 0, \sum p_{T}^\mathrm{neutral} + \sum p_{T}^{\gamma} - \frac{1}{2} \sum p_{T}^\mathrm{PU} \right] }{ p_{T}^{\ell} } \:\: , \text{with } \ell=\mu,e ,
\end{equation}
\noindent where the $\sum \pt^\mathrm{charged}$, $\sum \pt^\mathrm{neutral}$, and $\sum \pt^{\gamma}$ are the scalar sums of the transverse momenta of charged hadrons originating from the \ac{PV}, neutral hadrons and photons, respectively, and the $\sum p_{T}^\mathrm{PU}$ is the sum of transverse momenta of charged hadrons not originating from the \ac{PV}.
Notice that [[eq:pf_iso]] is valid both for muons and for electrons.
For the case of muon candidates, the cone is defined with $\Delta\text{R} < 0.4$.
An additional isolation criteria $\mathcal{I}_{\text{rel}}^{\text{Trk}}$ is used for high \ac{pt} muons, using exclusively the energy measured in the tracker, in a $\Delta\text{R} < 0.3$ cone.
The isolation criteria help disentangling prompt muons from muons originating from heavy-flavor quark decays, and from charged particles in jets misidentified as muons.
In this analysis, we required $\mathcal{I}_{\text{rel}}^{\mu} < 0.15$.

# identification
Three \ac{ID} selections for PF muons are centrally recommended by \ac{CMS}, in the form of =Loose=, =Medium= and =Tight= \acp{WP}.
In this analysis, the signal muon candidates are required to pass the =Tight= \ac{ID}, which uses global muons.
On top, requirements are applied on the number of hits in the muon chambers, strip tracker, and pixel detectors, plus a quality condition on the $\chi^{2}$ fit of the track.
The selections aim at decreasing the misidentification of other particles as muons.
The main background sources are cosmic rays, punch-through hadrons[fn:: Hadrons escaping the calorimeter and leaving energy deposits in the muon system.] and products of in-flight secondary decays.

# cuts
A correction factor is applied to \ac{MC} simulations to take into account differences with respect to data in the isolation and \ac{ID} efficiencies of muons.
These factors are derived from \zmumu{} events selected with a Tag & Probe technique (see the note below), and are provided by the \ac{CMS} Muon \ac{POG}.
For vetoed muons, $\mathcal{I}_{\text{rel}}^{\ell} < 0.3$ is required separately for the \logicor{} for two \acp{WP}, =Medium= and =Tight=.
The \logicor{} because since =Tight= is not a subset of the =Medium=, and therefore there will be a small number of events that fail =Medium= and pass =Tight=.
The veto selection on leptons is described in [[#sec:third_lepton_sel]].

# high pt WP
An additional \ac{WP} is available, dedicated to muons with a \ac{pt} above \SI{120}{\GeV}.
Given the fraction of such muons in our analysis, as listed in [[#sec:highpt_muons]], a decision was made not to consider \acp{SF} for the high-\ac{pt} \ac{WP}.
Moreover, binned distributions indicate a good agreement between muon data and \ac{MC}.
Such distributions are shown at the end of this Chapter, in [[#sec:binned_distributions]].

** Brief note on the Tag & Probe
This technique is often used in \ac{HEP} to measure the efficiency of a given process directly from data.
In a nutshell, for resonances decaying to two leptons, such as \zmumu{} or \zee{}, one of the leptons, the /tag/, is identified with tight selection criteria, while the other lepton, the /probe/, has to satisfy very loose criteria only.
The probe is matched to the tag by verifying the invariant mass of the tag + probe system, which should match the mass of the decaying resonance.
The tag is used to trigger the resonance, while the probe is exploited to make an unbiased estimate of the efficiency, since very little selections were applied on it.
The efficiency is measured by computing the ratio between passing probes and all available probes.

* Electrons
Just like for muons, the electron selections applied in this work were originally based on the \ac{CMS} \htt{} analyses [[cite:&higgs_tautau1;&higgs_tautau2;&higgs_tautau3]].
The standard \ac{CMS} electron reconstruction algorithm is used [[cite:&performance_muon_electron]], combining \ac{ECAL} and tracker information.
Electron candidates are reconstructed from clusters of energy deposits in the \ac{ECAL}, which are then matched to tracks in the inner silicon tracker, as discussed in [[#sec:offline_reco_electrons]].
Given that the electron reconstruction heavily relies on the deposits in the \ac{ECAL}, electrons in the the $1.44 < |\eta| < 1.57$ transition region between the barrel and endcaps of the \ac{ECAL} are excluded from this analysis, as they are typically of low reconstruction quality.

# isolation
When defining the isolation for electrons, a $\Delta\text{R} < 0.3$ cone is defined around the electron, with respect to the electron direction, and [[eq:pf_iso]] is used.
In this analysis, we required $\mathcal{I}_{\text{rel}}^{e} < 0.1$ for electrons in the \eletau{} channel, and $\mathcal{I}_{\text{rel}}^{e} < 0.3$ for vetoed electrons.

# identification
The electron \ac{ID} uses a \ac{MVA} which has been updated and improved for \ac{CMS} \run{2} analyses.
The discriminator is based on a \ac{BDT} [[cite:&tmva]] combining several observables sensitive to bremsstrahlung along the trajectory of the electron, to \ac{PF} isolation components and the energy density within the isolation cone, to shower-shape variables, to variables related to electron conversion, and to the matching between the trajectory of the electron and its related clusters.
The \ac{MVA} is trained on all electrons, regardless of whether they pass the trigger requirements or not, and is then tuned for electrons with $\pt>10\,\si{\GeV}$
Three categories are defined based on electron \ac{eta}: two in the barrel and one in the endcap.
Signal electrons require the =Tight= \ac{WP}, which has a \SI{\sim 70}{\percent} signal efficiency, while veto electrons must pass the =Medium= \ac{WP}, which has a \SI{\sim 80}{\percent} efficiency [[cite:&performance_muon_electron]].
The full veto selection is described in [[#sec:third_lepton_sel]].
A correction factor is applied to the \ac{MC} to take into account differences with respect to data in \ac{ID} efficiencies of electrons.
These factors are derived from \zee{} events, also selected with a Tag & Probe technique.

# scaling and smearing
Energy scaling and smearing corrections are applied to genuine electrons, following the recommendations of the E/Gamma \ac{POG}.
The correction is applied to \ac{MC} using a variable which represents the combined \ac{ECAL} and tracker electron energy after applying scale and smearing corrections.
The corrections are taken into account both in the selection of genuine electrons and in their veto.
They also lead to several systematic shape uncertainties, as described in [[#sec:syst_shape_genuine_electrons]].

* Hadronic Tau Leptons
:PROPERTIES:
:CUSTOM_ID: sec:hadronic_taus
:END:
Hadronically-decaying tau leptons are reconstructed by the \ac{HPS} algorithm, which targets multiple decay modes, as was described in [[#sec:offline_reco_taus]].
Its goal is to combined \ac{PF} information in jets to discriminate between $\tau$ jets and other jets, usually light jets from quarks and gluons.
In our analysis, as well as in others, hadronically-decaying \taus{} are the most important $\tau$ decays, given their large \ac{BR}.
There is thus a strong interest in improving the $\tau$ reconstruction performance, leading to the implementation of dedicated algorithms.
In \run{2}, =DeepTau= [[cite:&deeptau]] has demonstrated good performances, and is used in our analysis on top of \ac{HPS}.
As a consequence, there is no need to use more traditional quantities, like the ones described above for muon and electron candidates.
The goal of the =DeepTau= algorithm is to disentangle \tauhs{} from quark- and gluon-initiated jets, and also from electrons and muons, which can occasionally mimic a hadronic tau lepton decay. 
It uses information from all \ac{CMS} subdetectors, including variables used by \ac{HPS}.
It also considers information on candidates reconstructed within the \ac{HPS} tau signal and isolation cones, such as track and cluster properties and kinematics.
A multi-layered \ac{CNN}-based architecture is employed.
In total, the algorithm is trained with 140 million \tauh{} candidates, and validated with 10 million.
The final discriminators $D$ against electrons, muons and jets are the result of a softmax activation function, and are computed as follows:
#+NAME: eq:deeptau
\begin{equation}
y_{\alpha} = \frac{e^{x_{\alpha}}}{\sum_{\beta}e^{x_{\beta}}} \:\:\: , \:\:\: D_{\alpha} = \frac{y_{\tau}}{y_{\tau} + y_{\alpha}}
\end{equation}
\noindent with $\alpha \in {\text{jet}, \mu, e}$, and $x$ representing the four output nodes: $x_{\text{jet}}$, $x_{\mu}$, $x_{e}$ and $x_{\tau_{\text{h}}}$.
The discriminators are also known as =DeepTauVSjet=, =DeepTauVSe= and =DeepTauVSmu= for $D_{\text{jet}}$, $D_{\mu}$ and $D_{e}$, respectively.
The expected \tauh{} \ac{ID} efficiencies are obtained with validation samples.
The efficiencies for a particular =DeepJet= \ac{WP} are defined using genuine \tauhs{} in a \htt{} sample, where the \taus{} are reconstructed as \tauhs{} in a $30 < \pt < 170 \,\si{\GeV}$ range, and have passed that same \ac{WP}.
As shown in [[tab:deeptau_wps]], the efficiencies range from 40 to 98% for jets, from 60 to 99.5% for electrons, and from 99.5 to 99.95% for muons, depending on the \ac{WP}.
The $\text{jet} \rightarrow \tau$ misidentification rate varies jet \ac{pt} and quark flavor.
It has been estimated to be 0.43% for a genuine $\tau$ \ac{ID} efficiency of 70%, using simulated W + jets events.
The same rate for electrons and muons is 2.60(0.03)% for a genuine \tauh{} \ac{ID} efficiency of 80($>99$)%.
Significant updates are being put in place, mostly for \run{3} analyses, including using newer and extended data for training, improved training techniques, and optimized hyper-parameter tuning [[cite:&deeptau_run3]].

#+NAME: tab:deeptau_wps
#+CAPTION: \Ac{ID} efficiencies of \tauhs{} for all =DeepTau= \acp{WP}, considering its three classes. The efficiencies are measured with \htt{} samples for \tauhs{} in a $30 < \pt < 70\,\si{\GeV}$ range [[cite:&deeptau]].
#+ATTR_LATEX: :placement [!h] :center t :align ccccccccc :environment mytablewiderrows
|------------------+---------+--------+-------+--------+-------+--------+---------+----------|
|                  | =VVTight= | =VTight= | =Tight= | =Medium= | =Loose= | =VLoose= | =VVLoose= | =VVVLoose= |
|------------------+---------+--------+-------+--------+-------+--------+---------+----------|
| $D_{e}$          |     60% |    70% |   80% |    90% |   95% |    98% |     99% |    99.5% |
| $D_{\mu}$          |      -- |     -- | 99.5% |  99.8% | 99.9% | 99.95% |      -- |       -- |
| $D_{\text{jet}}$ |     40% |    50% |   60% |    70% |   80% |    90% |     95% |      98% |
|------------------+---------+--------+-------+--------+-------+--------+---------+----------|

* Jets
:PROPERTIES:
:CUSTOM_ID: sec:jets
:END:

The \ac{CMS} \ac{PF} algorithm creates a list of particle candidates which account for all tracker and muon tracks, and for all energy deposits in the calorimeters above a certain threshold. 
This information is assembled into jets using the anti-$\ktalgo$ clustering algorithm, described in [[#sec:offline_jet_object]], with distance parameters of 0.4 for AK4 jets and 0.8 for AK8 jets.
AK4 jets are required to satisfy $\pt > 20\,\si{\\GeV}$ and to not overlap with the two leptons from the \htt{} decay, with an angular selection of $\Delta\text{R}(\text{jet},\tau) < 0.5$.
Since tracking information is only available in the central region of the CMS detector and the b-tagging process heavily relies on it, all b-jet candidates are required to have $|\eta| < 2.5$ for the 2017 and 2018 datasets, while $|\eta| < 2.4$ is required in 2016.
The difference in \ac{eta} coverage between different years stems from the new \ac{CMS} pixel detector installed during the \phase{1} upgrade [[cite:&pixel_detector_eta_coverage]].
A more detailed description of jets coming from b quarks and identified as b-jets follows below.
The recommended set of jet energy corrections are applied to both AK4 and AK8 jets in data and \ac{MC}, as described in [[#sec:jets_corrections]].

Some jets must occasionally be vetoed due to their low reconstruction quality, or because they originate from electronic noise.
A \ac{PF} jet \ac{ID} criterion is available to \ac{CMS} analyzers, and all AK4 jets in our analysis are required to pass its =Tight= \ac{WP}.
The criterion is based on many jet observables, including the multiplicity of charged hadrons, the energy fraction deposited in \ac{ECAL} by hadrons, and the fraction of hadrons clustered within the jet.
The efficiency is around 98/99% or more for all \ac{eta} values, with a background rejection above 98% at $|\eta|<2.7$.

Jets are also often produced by \ac{PU}, being unrelated to the \ac{PV}.
These jets often result from the overlap of many low-energy jets, being thus broader than \ac{PV} jets.
To avoid such background jets, AK4 jets satisfying $\pt < 50\,\si{\GeV}$ are required to pass the =Loose= \ac{WP} of the discriminant.
The discriminant uses a \ac{BDT} to find an optimized decision boundary using information related to jet shape, object multiplicity and compatibility with the \ac{PV}.

Jets from b-quarks originating from the decay of high \ac{pt} Higgs bosons are often close enough to be merged into a single large radius jet by the anti-$\ktalgo$ algorithm, forming an AK8 jet.
In our analysis, the \ac{GNN}-based \ac{PNet} algorithm [[cite:&particle_net]] is used to discriminate \hbb{} decays from the multijet background, as detailed below.
We require AK8 jets to satisfy $\pt > 250\,\si{\GeV}$, and to not overlap with the two analysis leptons: $\Delta\text{R}(\text{jet},\tau) < 0.8$.
The jets must also have a =SoftDrop= mass above \SI{30}{\GeV}, where =SoftDrop= [[cite:&softdrop]] is a boosted jet grooming algorithm which removes soft and wide-angle radiation, aiming at mitigating the effects from contamination of \ac{ISR}, \ac{UE} and \ac{PU}.

** Identification of B-jets
Jets originated by the hadronization of b quarks distinguish themselves from other jets, inasmuch as they contain particles known to be relatively long-lived.
Such b mesons and hadrons can thus decay with a displacement of a few millimeters with respect to the \ac{PV}, defining the so-called /secondary vertex/.
Additionally, b hadrons decay into electrons or muons with a probability of \SI{\sim 20}{\percent}.
Distance parameters and displaced leptons can thus be exploited for discriminative purposes [[cite:&btag_performance]].

During \run{1}, the b-jet reconstruction algorithms available within \ac{CMS} worked by manually building discriminative variables.
The most advanced, the Combined Secondary Vertex (CSV) algorithm, used the secondary vertex mass and the number of tracks in a jet, among other variables.
Deep learning techniques first appeared in \run{2}, starting with =DeepCSV= [[cite:&deep_csv]], and later =DeepJet= [[cite:&deepjet;&deepjet_performance]], which is based on \acp{CNN} and \acp{RNN}.
Further improvements, particularly the widespread adoption of \acp{GNN}, have lead to \ac{PNet} [[cite:&particle_net]], and finally to \ac{ParT} [[cite:&transformer]], which exploits the state-of-the-art transformer technology [[cite:&transformers]], and should start being used in \run{3}.

In our analysis, AK4 jets originating from b quarks are identified using the =DeepJet= algorithm.
In order to separate b-jets from other jets, =DeepJet= combines secondary vertex properties, track-based variables and \ac{PF} jet constituents (neutral and charged candidates) in a \ac{DNN}.
It then classifies jets into six different categories, three of which are merged in order to tag b-jets in physics analyses.
The three merged categories focus on jets with at least two b hadrons, exactly one b hadron decaying hadronically, and exactly one b hadron decaying leptonically.

#+NAME: tab:bTagWPs
#+CAPTION: Thresholds for different years with associated \acp{WP} and efficiencies to be considered with \run{2} \ac{UL} datasets for the =DeepJet= and mass decorrelated \ac{PNet} \xbb{} taggers. LP, MP and HP refer to Low, Medium and High purities, respectively.
\begin{table}[htbp]
\hspace{1cm}
    \setlength{\tabcolsep}{10pt}
    \renewcommand{\arraystretch}{1.2} % Adjust line spacing
    \begin{tabular}{c|ccc|ccc}
        \hline
        \multirow{2}{*}{\textbf{Year}} &  & \textbf{DeepJet} &  &  & \textbf{PNet} &  \\
                              & \ac{WP} & Eff. [\%] & Cut & \ac{WP} & Eff. [\%] & Cut \\ \hline
        \multirow{3}{*}{2016} & \texttt{Loose}  & 86.3 & 0.0408 & \texttt{LP} & 40 & 0.9137  \\
                               & \texttt{Medium} & 71.4 & 0.2489 & \texttt{MP} & 60 & 0.9735  \\
                               & \texttt{Tight}  & 54.7 & 0.8819 & \texttt{HP} & 80 & 0.9883  \\ \hline
        \multirow{3}{*}{2016APV} & \texttt{Loose}  & 87.3 & 0.0508 & \texttt{LP} & 40 & 0.9088  \\
                                  & \texttt{Medium} & 73.3 & 0.2598 & \texttt{MP} & 60 & 0.9737  \\
                                  & \texttt{Tight}  & 57.5 & 0.8819 & \texttt{HP} & 80 & 0.9883  \\ \hline
        \multirow{3}{*}{2017} & \texttt{Loose}  & 91.0 & 0.0532 & \texttt{LP} & 40 & 0.9105  \\
                               & \texttt{Medium} & 79.1 & 0.3040 & \texttt{MP} & 60 & 0.9714  \\
                               & \texttt{Tight}  & 61.6 & 0.7476 & \texttt{HP} & 80 & 0.987   \\ \hline
        \multirow{3}{*}{2018} & \texttt{Loose}  & 91.5 & 0.0490 & \texttt{LP} & 40 & 0.9172  \\
                               & \texttt{Medium} & 80.7 & 0.2783 & \texttt{MP} & 60 & 0.9734  \\
                               & \texttt{Tight}  & 65.1 & 0.7100 & \texttt{HP} & 80 & 0.988   \\ \hline
    \end{tabular}
\end{table}

AK8 jets originating from merged \hbb{} decays are instead tagged by the \ac{PNet} algorithm.
This algorithm is able to identify hadronic decays of highly Lorentz-boosted top quarks and W, Z, and Higgs bosons, and classify different decay modes, such as $\bbbar$, $\ccbar$ or $\qqbar$ pairs.
The tagger is trained with \xbb{}, \xcc{} and \xqq{} signal jets, where X is a \spin{0} scalar, and with \ac{QCD} multijet background samples.
It accordingly outputs four discriminant scores, each representing the probability P for one of the four following processes to occur: \xbb{}, \xcc{}, \xqq{} and \ac{QCD}.
We use a mass-decorrelated version of \ac{PNet}.
The decorrelation is achieved by reweighting the training samples into uniform jet \ac{pt} and jet =SoftDrop= mass distributions.
The \xbb{} discriminant is given by:
#+NAME: eq:pnet
\begin{equation}
  \frac{\text{P}(\text{X}\rightarrow \text{b}\bar{\text{b}})}{\text{P}(\text{X} \rightarrow \text{b}\bar{\text{b}}) + \text{P}(\text{QCD})} \: .
\end{equation} 
\noindent Three \acp{WP} are defined with \hbb{} signal jets at efficiencies of 40%, 60%, and 80%: \ac{LP}, \ac{MP}, and \ac{HP}, respectively.
In order to select the most performant \ac{WP}, the full analysis workflow is run once per \ac{WP}, and the LP \ac{WP} is found to provide the most stringent results.
It is however important to note that discrepancies between data and \ac{MC} require the application of dedicated \acp{SF} to all jets passing the \ac{PNet} \acp{WP}.
AK8 analysis jets must thus be corrected, in a procedure described in [[#sec:pnet_sfs]].
The thresholds on the =DeepJet= and \ac{PNet} discriminator values, and corresponding efficiencies, are listed in [[tab:bTagWPs]].

** Jet Energy Scale and Resolution Corrections
:PROPERTIES:
:CUSTOM_ID: sec:jets_corrections
:END:

The measured jet energy can significantly differ from the underlying true hadron energy.
Differences can arise due to detector noise, \ac{PU} or a non-linear calorimetric response.
The precise understanding of \acp{JEC}, scales and resolutions, is of crucial importance for multiple analyses, also entering as an important component in their systematic uncertainties.
The energy of jets must therefore be appropriately corrected, in order to match the true particle-level deposited energy [[cite:&jet_corr1;&jet_corr2]].
In [[fig:jerc]] we illustrate the approach adopted by \ac{CMS} in \run{2}.
It consists on a sequential series of steps, where each step is responsible to independently correct a different effect.
Each data-taking period has its own set of corrections.
The first step addresses the spurious energy deposits from \ac{PU} interactions.
For each type of \ac{PF} candidate an offset energy is subtracted from the jet energy.
In the second step, detector response corrections are applied, in order to fix its non-uniformity across the jet \ac{pt} and \ac{eta} phase-space.
Next, remaining differences between data and \ac{MC} are corrected by accounting for \ac{PU} effects, which also depend on the \ac{pt} and \ac{eta} of jets.
Finally, optional flavour dependent corrections can be applied.
For all jet types, the energy scale uncertainties are smaller than 3% for $\pt > 50\,\si{\GeV}$ in the $|\eta| < 3.0$ region, increasing to 5% for $3.0 < |\eta| < 5.0$.

#+NAME: fig:jerc
#+CAPTION: Illustration of the jet energy correction stages that must be sequentially applied in order to obtain a calibrated jet, as done for \run{2} in \ac{CMS}. Taken from [[cite:&jet_corr2]].
#+BEGIN_figure
#+ATTR_LATEX: :width 1.\textwidth :center
[[~/org/PhD/Thesis/figures/analysis1/Run2-JERC.pdf]]
#+END_figure

Since measurements show that the jet energy resolution in data is worse than in the simulation, resolution corrections must be applied to \ac{MC} jets.
The latter are smeared to describe the data.
The smearing procedure uses a ``hybrid'' approach recommended by \ac{CMS} BTV \ac{POG}, and composed of two methods.
If a matched generator-level jet exists, then the four-momentum of the corresponding reconstructed jet is rescaled, with a factor which depends on the \ac{pt} of the reconstructed and generated jet:
#+NAME: fig:hybrid1
\begin{equation}
	c_{\text{JER}} = 1+(s_{\text{JER}}-1)\,\frac{\pt-\pt^{\text{Gen.}}}{\pt}
\end{equation}
\noindent where $s_{\text{JER}}$ is the data-to-simulation core resolution scale factor.
If the jet was not matched (and thus $\pt^{\text{Gen.}}$ is not available), then a stochastic smearing is applied, performing the four-momentum rescaling using a different factor:
#+NAME: fig:hybrid2
\begin{equation}
	c_{\text{JER}} = 1+\mathcal{N}(0, \sigma_{\text{JER}})\sqrt{\max(s^2_{\text{JER}}-1, 0)}
\end{equation}
\noindent where $\sigma_{\text{JER}}$ is the relative \ac{pt} resolution in simulation, and $\mathcal{N}(0, \sigma)$ denotes a random number sampled from a normal distribution with zero mean and standard deviation $\sigma$.
The resolution corrections are computed after applying the above jet energy corrections.
The data/MC \acp{SF} usually vary between 1 and 1.2, but are larger in the transition region between the endcaps and the forward detectors.
No significant dependencies on the \ac{pt} and \ac{eta} of the jets are observed, except in the transition region [[cite:&jec_jer_performance]].

* Missing Transverse Energy
As discussed in [[#sec:offline_reco_met]], \ac{MET} is the negative vector sum of all \ac{PF} reconstructed particles in an event.
Despite being well defined, the "raw", uncorrected \ac{MET} is systematically different from the transverse momentum actually carried by invisible particles.
This happens due to a variety of detector effects, most notably the non-compensating nature of the \ac{CMS} calorimeters, which was explained in [[#sec:offline_reco_pf]], and due to detector misalignments.
In this analysis, we apply corrections as instructed by the \ac{CMS} JetMET \ac{POG}, turning the measured $\ptmiss$ into a better estimate of the ``true'' \ac{MET}.

Measurements show that the \ac{JER} in data is worse than in the simulation.
As discussed in [[#sec:jets_corrections]], jets in simulation should thus be smeared to achieve a better agreement with data.
Given that jets are one of the building blocks of \ac{MET}, their smearing should be propagated to the \ac{MET}.
The corrections replace the vector sum of transverse momenta of particles clustered as jets by the vector sum of the transverse momenta of the jets to which \acp{JEC} are applied.
Corrections are applied to AK4 jets.

It has been observed that uncorrected \ac{MET} features a modulation in the azimuthal \ac{phi} coordinate.
The modulation roughly follows a sinusoidal curve with a $2\pi$ period.
The distribution of true \ac{MET} should instead be independent of \ac{phi} because of the collisions' rotational symmetry along the transverse axis.
The modulation can be due to anisotropic detector responses, to inactive calorimeter cells and/or tracking regions, to the detector misalignment, and even to the displacement of the beam spot. 
The amplitude of the modulation increases roughly linearly with the number of \ac{PU} interactions. 
In this analysis, we reduce the amplitude of the \ac{phi} modulation by shifting the origin of the $x$ and $y$ coordinates in the transverse momentum plane, as a function of the run number and of the number of \acp{PV}.

We also apply \ac{MET} quality filters provided by the JetMET \ac{POG}, in order to improve the quality of the reconstructed \ac{MET}:
+ Events where the \ac{PV} is not of good quality are rejected.
+ A beam halo filter is used, to reduce the non negligible probability of high-energy halo muons to interact in the calorimeters. Such interactions can create clusters of up to several hundreds of \si{\GeV}.
+ Events with problematic dead cell \ac{TP} energy recovery are removed.
+ Events where a large nonphysical \ac{MET} is erroneously reconstructed due to the presence of additional muons are rejected.
+ Additional filters are applied to reject events with high \ac{HCAL} or \ac{ECAL} noise.
